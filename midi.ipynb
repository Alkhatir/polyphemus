{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmanueleCosenza/Polyphemus/blob/main/midi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "50lpUn9bO0ug",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cosenza/thesis/Polyphemus\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -C data -xvzf data/lmd_matched.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "She0QbN5Kopo",
    "outputId": "0f3fb4c7-bd7d-4ee4-b2cd-d567d8e490db",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: muspy in /home/cosenza/penv/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (6.0)\n",
      "Requirement already satisfied: mido>=1.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (1.2.10)\n",
      "Requirement already satisfied: bidict>=0.21 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (0.21.4)\n",
      "Requirement already satisfied: tqdm>=4.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (4.62.3)\n",
      "Requirement already satisfied: pypianoroll>=1.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (1.0.4)\n",
      "Requirement already satisfied: matplotlib>=1.5 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (3.3.4)\n",
      "Requirement already satisfied: requests>=2.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (2.26.0)\n",
      "Requirement already satisfied: pretty-midi>=0.2 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (0.2.9)\n",
      "Requirement already satisfied: joblib>=0.15 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (1.1.0)\n",
      "Requirement already satisfied: music21>=5.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (6.7.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (2.8.2)\n",
      "Requirement already satisfied: chardet in /home/cosenza/penv/lib/python3.6/site-packages (from music21>=5.0->muspy) (4.0.0)\n",
      "Requirement already satisfied: more-itertools in /home/cosenza/penv/lib/python3.6/site-packages (from music21>=5.0->muspy) (8.12.0)\n",
      "Requirement already satisfied: webcolors in /home/cosenza/penv/lib/python3.6/site-packages (from music21>=5.0->muspy) (1.11.1)\n",
      "Requirement already satisfied: six in /home/cosenza/penv/lib/python3.6/site-packages (from pretty-midi>=0.2->muspy) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/cosenza/penv/lib/python3.6/site-packages (from pypianoroll>=1.0->muspy) (1.5.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cosenza/penv/lib/python3.6/site-packages (from requests>=2.0->muspy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/cosenza/penv/lib/python3.6/site-packages (from requests>=2.0->muspy) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/cosenza/penv/lib/python3.6/site-packages (from requests>=2.0->muspy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cosenza/penv/lib/python3.6/site-packages (from requests>=2.0->muspy) (2021.10.8)\n",
      "Requirement already satisfied: pypianoroll in /home/cosenza/penv/lib/python3.6/site-packages (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/cosenza/penv/lib/python3.6/site-packages (from pypianoroll) (1.19.5)\n",
      "Requirement already satisfied: matplotlib>=1.5 in /home/cosenza/penv/lib/python3.6/site-packages (from pypianoroll) (3.3.4)\n",
      "Requirement already satisfied: pretty-midi>=0.2.8 in /home/cosenza/penv/lib/python3.6/site-packages (from pypianoroll) (0.2.9)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/cosenza/penv/lib/python3.6/site-packages (from pypianoroll) (1.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->pypianoroll) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->pypianoroll) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->pypianoroll) (3.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->pypianoroll) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->pypianoroll) (0.11.0)\n",
      "Requirement already satisfied: mido>=1.1.16 in /home/cosenza/penv/lib/python3.6/site-packages (from pretty-midi>=0.2.8->pypianoroll) (1.2.10)\n",
      "Requirement already satisfied: six in /home/cosenza/penv/lib/python3.6/site-packages (from pretty-midi>=0.2.8->pypianoroll) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the required music libraries\n",
    "!pip3 install muspy\n",
    "!pip3 install pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uveQkY7O0CF",
    "outputId": "12e1f638-ee78-4617-844a-10e9a26c298e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu102.html\n",
      "Requirement already satisfied: torch-scatter in /home/cosenza/penv/lib/python3.6/site-packages (2.0.9)\n",
      "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu102.html\n",
      "Requirement already satisfied: torch-sparse in /home/cosenza/penv/lib/python3.6/site-packages (0.6.12)\n",
      "Requirement already satisfied: scipy in /home/cosenza/penv/lib/python3.6/site-packages (from torch-sparse) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/cosenza/penv/lib/python3.6/site-packages (from scipy->torch-sparse) (1.19.5)\n",
      "Requirement already satisfied: torch-geometric in /home/cosenza/penv/lib/python3.6/site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (1.19.5)\n",
      "Requirement already satisfied: tqdm in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (4.62.3)\n",
      "Requirement already satisfied: scipy in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (1.5.4)\n",
      "Requirement already satisfied: networkx in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (0.24.2)\n",
      "Requirement already satisfied: requests in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (2.26.0)\n",
      "Requirement already satisfied: pandas in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (1.1.5)\n",
      "Requirement already satisfied: rdflib in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (5.0.0)\n",
      "Requirement already satisfied: googledrivedownloader in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (0.4)\n",
      "Requirement already satisfied: jinja2 in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (3.0.3)\n",
      "Requirement already satisfied: pyparsing in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (3.0.6)\n",
      "Requirement already satisfied: yacs in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (0.1.8)\n",
      "Requirement already satisfied: PyYAML in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cosenza/penv/lib/python3.6/site-packages (from jinja2->torch-geometric) (2.0.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /home/cosenza/penv/lib/python3.6/site-packages (from networkx->torch-geometric) (4.4.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/cosenza/penv/lib/python3.6/site-packages (from pandas->torch-geometric) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/cosenza/penv/lib/python3.6/site-packages (from pandas->torch-geometric) (2.8.2)\n",
      "Requirement already satisfied: isodate in /home/cosenza/penv/lib/python3.6/site-packages (from rdflib->torch-geometric) (0.6.0)\n",
      "Requirement already satisfied: six in /home/cosenza/penv/lib/python3.6/site-packages (from rdflib->torch-geometric) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cosenza/penv/lib/python3.6/site-packages (from requests->torch-geometric) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/cosenza/penv/lib/python3.6/site-packages (from requests->torch-geometric) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/cosenza/penv/lib/python3.6/site-packages (from requests->torch-geometric) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cosenza/penv/lib/python3.6/site-packages (from requests->torch-geometric) (2021.10.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/cosenza/penv/lib/python3.6/site-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/cosenza/penv/lib/python3.6/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Install torch_geometric\n",
    "!v=$(python -c \"import torch; print(torch.__version__)\"); \\\n",
    "pip install torch-scatter -f https://data.pyg.org/whl/torch-${v}.html; \\\n",
    "pip install torch-sparse -f https://data.pyg.org/whl/torch-${v}.html; \\\n",
    "pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B45l1513wJ1Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import muspy\n",
    "from itertools import product\n",
    "import pypianoroll as pproll\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class MIDIPreprocessor():\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def preprocess_dataset(self, dir, early_exit=None):\n",
    "        pass\n",
    "    \n",
    "    def preprocess_file(self, f):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Todo: to config file (or separate files)\n",
    "MAX_SIMU_NOTES = 16 # 14 + SOS and EOS\n",
    "\n",
    "PITCH_SOS = 128\n",
    "PITCH_EOS = 129\n",
    "PITCH_PAD = 130\n",
    "DUR_PAD_IND = 2\n",
    "MAX_DUR = 511 # equivalent to 16 bars (with RESOLUTION=32)\n",
    "\n",
    "RESOLUTION = 32\n",
    "NUM_BARS = 1\n",
    "\n",
    "\n",
    "def preprocess_file(filepath, dest_dir, num_samples):\n",
    "\n",
    "    saved_samples = 0\n",
    "\n",
    "    print()\n",
    "    print(\"Preprocessing file \" + filepath)\n",
    "\n",
    "    # Load the file both as a pypianoroll song and a muspy song\n",
    "    # (Need to load both since muspy.to_pypianoroll() is expensive)\n",
    "    try:\n",
    "        pproll_song = pproll.read(filepath, resolution=RESOLUTION)\n",
    "        muspy_song = muspy.read(filepath)\n",
    "    except Exception as e:\n",
    "        print(\"Song skipped (Invalid song format)\")\n",
    "        return 0\n",
    "    \n",
    "    # Only accept songs that have a time signature of 4/4 and no time changes\n",
    "    for t in muspy_song.time_signatures:\n",
    "        if t.numerator != 4 or t.denominator != 4:\n",
    "            print(\"Song skipped ({}/{} time signature)\".\n",
    "                            format(t.numerator, t.denominator))\n",
    "            return 0\n",
    "\n",
    "    # Gather tracks of pypianoroll song based on MIDI program number\n",
    "    drum_tracks = []\n",
    "    bass_tracks = []\n",
    "    guitar_tracks = []\n",
    "    strings_tracks = []\n",
    "\n",
    "    for track in pproll_song.tracks:\n",
    "        if track.is_drum:\n",
    "            track.name = 'Drums'\n",
    "            drum_tracks.append(track)\n",
    "        elif 0 <= track.program <= 31:\n",
    "            track.name = 'Guitar'\n",
    "            guitar_tracks.append(track)\n",
    "        elif 32 <= track.program <= 39:\n",
    "            track.name = 'Bass'\n",
    "            bass_tracks.append(track)\n",
    "        else:\n",
    "            # Tracks with program > 39 are all considered as strings tracks\n",
    "            # and will be merged into a single track later on\n",
    "            strings_tracks.append(track)\n",
    "\n",
    "    # Filter song if it does not contain drum, guitar, bass or strings tracks\n",
    "    if not drum_tracks or not guitar_tracks \\\n",
    "       or not bass_tracks or not strings_tracks:\n",
    "        print(\"Song skipped (does not contain drum or \"\n",
    "                \"guitar or bass or strings tracks)\")\n",
    "        return 0\n",
    "    \n",
    "    # Merge strings tracks into a single pypianoroll track\n",
    "    strings = pproll.Multitrack(tracks=strings_tracks)\n",
    "    strings_track = pproll.Track(pianoroll=strings.blend(mode='max'),\n",
    "                                 program=48, name='Strings')\n",
    "\n",
    "    combinations = list(product(drum_tracks, bass_tracks, guitar_tracks))\n",
    "\n",
    "    # Single instruments can have multiple tracks.\n",
    "    # Consider all possible combinations of drum, bass, and guitar tracks\n",
    "    for i, combination in enumerate(combinations):\n",
    "\n",
    "        print(\"Processing combination\", i+1, \"of\", len(combinations))\n",
    "        \n",
    "        # Process combination (called 'subsong' from now on)\n",
    "        drum_track, bass_track, guitar_track = combination\n",
    "        tracks = [drum_track, bass_track, guitar_track, strings_track]\n",
    "        \n",
    "        pproll_subsong = pproll.Multitrack(\n",
    "            tracks=tracks,\n",
    "            tempo=pproll_song.tempo,\n",
    "            resolution=RESOLUTION\n",
    "        )\n",
    "        muspy_subsong = muspy.from_pypianoroll(pproll_subsong)\n",
    "        \n",
    "        tracks_notes = [track.notes for track in muspy_subsong.tracks]\n",
    "        \n",
    "        # Obtain length of subsong (maximum of each track's length)\n",
    "        length = 0\n",
    "        for notes in tracks_notes:\n",
    "            track_length = max(note.end for note in notes)\n",
    "            length = max(length, track_length)\n",
    "        length += 1\n",
    "\n",
    "        # Add timesteps until length is a multiple of RESOLUTION\n",
    "        length = length if length%(RESOLUTION) == 0 \\\n",
    "                                else length + (RESOLUTION-(length%(RESOLUTION)))\n",
    "\n",
    "\n",
    "        tracks_tensors = []\n",
    "        tracks_activations = []\n",
    "\n",
    "        dur_bin_length = int(np.ceil(np.log2(MAX_DUR)))\n",
    "\n",
    "        # Todo: adapt to velocity\n",
    "        for notes in tracks_notes:\n",
    "\n",
    "            # Initialize encoder-ready track tensor\n",
    "            # track_tensor: (length x max_simu_notes x 2 (or 3 if velocity))\n",
    "            # The last dimension contains pitches and durations (and velocities)\n",
    "            # int16 is enough for small to medium duration values\n",
    "            track_tensor = np.zeros((length, MAX_SIMU_NOTES, 2), np.int16)\n",
    "\n",
    "            track_tensor[:, :, 0] = PITCH_PAD\n",
    "            track_tensor[:, 0, 0] = PITCH_SOS\n",
    "\n",
    "            # Keeps track of how many notes have been stored in each timestep\n",
    "            # (int8 imposes that MAX_SIMU_NOTES < 256)\n",
    "            notes_counter = np.ones(length, dtype=np.int8)\n",
    "\n",
    "            # Todo: np.put_along_axis?\n",
    "            for note in notes:\n",
    "                # Insert note in the lowest position available in the timestep\n",
    "                \n",
    "                t = note.time\n",
    "\n",
    "                if notes_counter[t] >= MAX_SIMU_NOTES-1:\n",
    "                    # Skip note if there is no more space\n",
    "                    continue\n",
    "\n",
    "                track_tensor[t, notes_counter[t], 0] = note.pitch\n",
    "                track_tensor[t, notes_counter[t], 1] = note.duration\n",
    "                notes_counter[t] += 1\n",
    "            \n",
    "            # Add end of sequence token\n",
    "            track_tensor[np.arange(0, length), notes_counter, 0] = PITCH_EOS\n",
    "\n",
    "            # Get track activations, a boolean tensor indicating whether notes\n",
    "            # are being played in a timestep (sustain does not count)\n",
    "            # (needed for graph rep.)\n",
    "            activations = np.array(notes_counter-1, dtype=bool)\n",
    "\n",
    "            tracks_tensors.append(track_tensor)\n",
    "            tracks_activations.append(activations)\n",
    "        \n",
    "        # (#tracks x length x max_simu_notes x 2 (or 3))\n",
    "        subsong_tensor = np.stack(tracks_tensors, axis=0)\n",
    "\n",
    "        # (#tracks x length)\n",
    "        subsong_activations = np.stack(tracks_activations, axis=0)\n",
    "\n",
    "\n",
    "        # Slide window over 'subsong_tensor' and 'subsong_activations' along the\n",
    "        # time axis (2nd dimension) with the stride of a bar\n",
    "        # Todo: np.lib.stride_tricks.as_strided(song_proll)\n",
    "        for i in range(0, length-NUM_BARS*RESOLUTION+1, RESOLUTION):\n",
    "            \n",
    "            # Get the sequence and its activations\n",
    "            seq_tensor = subsong_tensor[:, i:i+NUM_BARS*RESOLUTION, :]\n",
    "            seq_acts = subsong_activations[:, i:i+NUM_BARS*RESOLUTION]\n",
    "\n",
    "            # Skip sequence if it contains more than one bar of consecutive\n",
    "            # silence in at least one track\n",
    "            bars = seq_acts.reshape(seq_acts.shape[0], NUM_BARS, -1)\n",
    "            bars_acts = np.any(bars, axis=2)\n",
    "            \n",
    "            if 1 in np.diff(np.where(bars_acts == 0)[1]):\n",
    "                continue\n",
    "\n",
    "            # Randomly transpose the pitches of the sequence (-5 to 6 semitones)\n",
    "            shift = np.random.choice(np.arange(-5, 7), 1)\n",
    "            cond = (seq_tensor[:, :, :, 0] != PITCH_PAD) &                     \\\n",
    "                   (seq_tensor[:, :, :, 0] != PITCH_SOS) &                     \\\n",
    "                   (seq_tensor[:, :, :, 0] != PITCH_EOS)\n",
    "            seq_tensor[cond, 0] += shift\n",
    "\n",
    "            # Save sample (seq_tensor and seq_acts) to file\n",
    "            curr_sample = str(num_samples + saved_samples)\n",
    "            sample_filepath = os.path.join(dest_dir, curr_sample)\n",
    "            np.savez(sample_filepath, seq_tensor=seq_tensor, seq_acts=seq_acts)\n",
    "\n",
    "            saved_samples += 1\n",
    "\n",
    "\n",
    "    print(\"File preprocessing finished. Saved samples:\", saved_samples)\n",
    "    print()\n",
    "\n",
    "    return saved_samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Total number of files: 116189\n",
    "# Number of unique files: 45129\n",
    "def preprocess_dataset(dataset_dir, dest_dir, early_exit=None):\n",
    "\n",
    "    files_dict = {}\n",
    "    seen = 0\n",
    "    tot_samples = 0\n",
    "    finished = False\n",
    "\n",
    "    # Visit recursively the directories inside the dataset directory\n",
    "    for dirpath, dirs, files in os.walk(dataset_dir):\n",
    "\n",
    "        # Sort alphabetically the found directories\n",
    "        # (to help guess the remaining time) \n",
    "        dirs.sort()\n",
    "        \n",
    "        print(\"Current path:\", dirpath)\n",
    "\n",
    "        for f in files:\n",
    "            \n",
    "            seen += 1\n",
    "\n",
    "            if f in files_dict:\n",
    "                # Skip already seen file\n",
    "                files_dict[f] += 1\n",
    "                continue\n",
    "\n",
    "            # File never seen before, add to dictionary of files\n",
    "            # (from filename to # of occurrences)\n",
    "            files_dict[f] = 1\n",
    "\n",
    "            # Preprocess file\n",
    "            filepath = os.path.join(dirpath, f)\n",
    "            saved = preprocess_file(filepath, dest_dir, tot_samples)\n",
    "\n",
    "            tot_samples += saved\n",
    "\n",
    "            # Exit when a maximum number of files has been processed (if set)\n",
    "            if early_exit != None and len(files_dict) >= early_exit:\n",
    "                finished = True\n",
    "                break\n",
    "\n",
    "        # Todo: also print # of processed (not filtered) files\n",
    "        #       and # of produced sequences (samples)\n",
    "        print(\"Total number of seen files:\", seen)\n",
    "        print(\"Number of unique files:\", len(files_dict))\n",
    "        print(\"Total number of saved samples:\", tot_samples)\n",
    "        print()\n",
    "\n",
    "        if finished:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aYc5y-CYyetK"
   },
   "outputs": [],
   "source": [
    "!rm -rf data/preprocessed/\n",
    "!mkdir data/preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqnubg3oP4ES",
    "outputId": "40cc38a2-1f7d-4f6f-e6c9-9e14dfc7f683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current path: data/lmd_matched\n",
      "Total number of seen files: 0\n",
      "Number of unique files: 0\n",
      "Total number of saved samples: 0\n",
      "\n",
      "Current path: data/lmd_matched/A\n",
      "Total number of seen files: 0\n",
      "Number of unique files: 0\n",
      "Total number of saved samples: 0\n",
      "\n",
      "Current path: data/lmd_matched/A/A\n",
      "Total number of seen files: 0\n",
      "Number of unique files: 0\n",
      "Total number of saved samples: 0\n",
      "\n",
      "Current path: data/lmd_matched/A/A/A\n",
      "Total number of seen files: 0\n",
      "Number of unique files: 0\n",
      "Total number of saved samples: 0\n",
      "\n",
      "Current path: data/lmd_matched/A/A/A/TRAAAGR128F425B14B\n",
      "\n",
      "Preprocessing file data/lmd_matched/A/A/A/TRAAAGR128F425B14B/1d9d16a9da90c090809c153754823c2b.mid\n",
      "Processing combination 1 of 7\n",
      "Processing combination 2 of 7\n",
      "Processing combination 3 of 7\n",
      "Processing combination 4 of 7\n",
      "Processing combination 5 of 7\n",
      "Processing combination 6 of 7\n",
      "Processing combination 7 of 7\n",
      "File preprocessing finished. Saved samples: 3059\n",
      "\n",
      "\n",
      "Preprocessing file data/lmd_matched/A/A/A/TRAAAGR128F425B14B/5dd29e99ed7bd3cc0c5177a6e9de22ea.mid\n",
      "Processing combination 1 of 5\n",
      "Processing combination 2 of 5\n",
      "Processing combination 3 of 5\n",
      "Processing combination 4 of 5\n",
      "Processing combination 5 of 5\n",
      "File preprocessing finished. Saved samples: 2130\n",
      "\n",
      "Total number of seen files: 2\n",
      "Number of unique files: 2\n",
      "Total number of saved samples: 5189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset\n",
    "dataset_dir = 'data/lmd_matched'\n",
    "dest_dir = 'data/preprocessed'\n",
    "preprocess_dataset(dataset_dir, dest_dir, early_exit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG88mekfrrcp"
   },
   "source": [
    "Check preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JlP6iUNugNtP"
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(dest_dir, \"5.npz\")\n",
    "data = np.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VUpOEObhwYQ",
    "outputId": "aac6e029-93b1-485f-f13a-2a00abedbc7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 32, 16, 2)\n",
      "(4, 32)\n"
     ]
    }
   ],
   "source": [
    "print(data[\"seq_tensor\"].shape)\n",
    "print(data[\"seq_acts\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6NA5IAAmtK8",
    "outputId": "6e661b3a-05a1-4e2d-9a3d-e1c037b4d04f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128,   0],\n",
       "       [129,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0]], dtype=int16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"seq_tensor\"][0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C19X9m-3iMlm"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zymqD-UqR8wq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cosenza/penv/lib64/python3.6/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import itertools\n",
    "\n",
    "\n",
    "def unpackbits(x, num_bits):\n",
    "\n",
    "    if np.issubdtype(x.dtype, np.floating):\n",
    "        raise ValueError(\"numpy data type needs to be int-like\")\n",
    "\n",
    "    xshape = list(x.shape)\n",
    "    x = x.reshape([-1, 1])\n",
    "    mask = 2**np.arange(num_bits, dtype=x.dtype).reshape([1, num_bits])\n",
    "\n",
    "    return (x & mask).astype(bool).astype(int).reshape(xshape + [num_bits])\n",
    "\n",
    "\n",
    "class MIDIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir):\n",
    "        self.dir = dir\n",
    "\n",
    "    def __len__(self):\n",
    "        _, _, files = next(os.walk(self.dir))\n",
    "        return len(files)\n",
    "\n",
    "    \n",
    "    def __get_track_edges(self, acts, edge_type_ind=0):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "\n",
    "        labels = np.arange(acts.shape[0]*acts.shape[1])\n",
    "        labels = labels.reshape(acts.shape[0], acts.shape[1]).transpose()\n",
    "\n",
    "        track_edges = []\n",
    "\n",
    "        for track in range(a_t.shape[1]):\n",
    "            tr_inds = list(inds[inds[:,1] == track])\n",
    "            e_inds = [(tr_inds[i],\n",
    "                    tr_inds[i+1]) for i in range(len(tr_inds)-1)]\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, e[1][0]-e[0][0]) for e in e_inds]\n",
    "            track_edges.extend(edges)\n",
    "\n",
    "        return np.array(track_edges, dtype='long')\n",
    "\n",
    "    \n",
    "    def __get_onset_edges(self, acts, edge_type_ind=1):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        labels = np.arange(acts.shape[0]*acts.shape[1])\n",
    "        labels = labels.reshape(acts.shape[0], acts.shape[1]).transpose()\n",
    "\n",
    "        onset_edges = []\n",
    "\n",
    "        for i in ts_inds:\n",
    "            ts_acts_inds = list(inds[inds[:,0] == i])\n",
    "            if len(ts_acts_inds) < 2:\n",
    "                continue\n",
    "            e_inds = list(itertools.combinations(ts_acts_inds, 2))\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, 0) for e in e_inds]\n",
    "            inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "            onset_edges.extend(edges)\n",
    "            onset_edges.extend(inv_edges)\n",
    "\n",
    "        return np.array(onset_edges, dtype='long')\n",
    "\n",
    "\n",
    "    def __get_next_edges(self, acts, edge_type_ind=2):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        labels = np.arange(acts.shape[0]*acts.shape[1])\n",
    "        labels = labels.reshape(acts.shape[0], acts.shape[1]).transpose()\n",
    "\n",
    "        next_edges = []\n",
    "\n",
    "        for i in range(len(ts_inds)-1):\n",
    "\n",
    "            ind_s = ts_inds[i]\n",
    "            ind_e = ts_inds[i+1]\n",
    "            s = inds[inds[:,0] == ind_s]\n",
    "            e = inds[inds[:,0] == ind_e]\n",
    "\n",
    "            e_inds = [t for t in list(itertools.product(s, e)) if t[0][1] != t[1][1]]\n",
    "            edges = [(labels[tuple(e[0])],labels[tuple(e[1])], edge_type_ind, ind_e-ind_s) for e in e_inds]\n",
    "\n",
    "            next_edges.extend(edges)\n",
    "\n",
    "        return np.array(next_edges, dtype='long')\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Load tensors\n",
    "        sample_path = os.path.join(self.dir, str(idx) + \".npz\")\n",
    "        data = np.load(sample_path)\n",
    "\n",
    "        seq_tensor = data[\"seq_tensor\"]\n",
    "        seq_acts = data[\"seq_acts\"]\n",
    "\n",
    "        # From decimals to one-hot (pitch)\n",
    "        pitches = seq_tensor[:, :, :, 0]\n",
    "        onehot = np.zeros((pitches.shape[0]*pitches.shape[1]*pitches.shape[2],\n",
    "                            131), dtype=np.float)\n",
    "        onehot[np.arange(0, onehot.shape[0]), pitches.reshape(-1)] = 1.\n",
    "        onehot = onehot.reshape(-1, pitches.shape[1], seq_tensor.shape[2], 131)\n",
    "\n",
    "        # From decimals to binary (pitch)\n",
    "        durs = seq_tensor[:, :, :, 1]\n",
    "        bin_durs = unpackbits(durs, 9)[:, :, :, ::-1]\n",
    "\n",
    "        # Concatenate pitches and durations\n",
    "        new_seq_tensor = np.concatenate((onehot[:, :, :, :], bin_durs),\n",
    "                             axis=-1)\n",
    "        \n",
    "        # Construct graph from boolean activations\n",
    "        track_edges = self.__get_track_edges(seq_acts)\n",
    "        onset_edges = self.__get_onset_edges(seq_acts)\n",
    "        next_edges = self.__get_next_edges(seq_acts)\n",
    "        edges = [track_edges, onset_edges, next_edges]\n",
    "\n",
    "        # Concatenate edge tensors (N x 4) (if any)\n",
    "        no_edges = (len(track_edges) == 0 and \n",
    "                    len(onset_edges) == 0 and len(next_edges) == 0)\n",
    "        if not no_edges:\n",
    "            edge_list = np.concatenate([x for x in edges\n",
    "                                          if x.size > 0])\n",
    "            edge_list = torch.from_numpy(edge_list)\n",
    "        \n",
    "        # Adapt tensor to torch_geometric's Data\n",
    "        edge_index = (torch.LongTensor([[], []]) if no_edges else\n",
    "                               edge_list[:, :2].t().contiguous())\n",
    "        edge_attr = (torch.Tensor([[]]) if no_edges else\n",
    "                                       edge_list[:, 2:])\n",
    "\n",
    "        n = seq_acts.shape[0]*seq_acts.shape[1]\n",
    "        graph = Data(edge_index=edge_index, edge_attr=edge_attr, num_nodes=n)\n",
    "        \n",
    "        # Todo: start with torch at mount\n",
    "        return torch.Tensor(new_seq_tensor), torch.Tensor(seq_acts), graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "hSwcnlq4g50O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Todo: check and think about max_len\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 256):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *                     \\\n",
    "                             (-math.log(10000.0)/d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position*div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, features_dims=[256, 256, 256], num_relations=3):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(features_dims)-1):\n",
    "            self.layers.append(GCNConv(features_dims[i], features_dims[i+1]))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    # 140 = 128+3+9\n",
    "    def __init__(self, d_token=140, d_transf=256, nhead_transf=4, \n",
    "                 num_layers_transf=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Todo: one separate encoder for drums\n",
    "        # Transformer Encoder\n",
    "        self.embedding = nn.Linear(d_token, d_transf)\n",
    "        self.pos_encoder = PositionalEncoding(d_transf, dropout)\n",
    "        transf_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_transf,\n",
    "            nhead=nhead_transf\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            transf_layer,\n",
    "            num_layers=num_layers_transf\n",
    "        )\n",
    "\n",
    "        # Graph encoder\n",
    "        self.graph_encoder = GCN()\n",
    "\n",
    "        # (LSTM)\n",
    "        \n",
    "        # Linear layers that compute the final mu and log_var\n",
    "        # Todo: as parameters\n",
    "        self.linear_mu = nn.Linear(256, 256)\n",
    "        self.linear_log_var = nn.Linear(256, 256)\n",
    "\n",
    "    def forward(self, x_seq, x_acts, x_graph):\n",
    "\n",
    "        # Collapse track (and optionally batch) dimension\n",
    "        print(\"Init input:\", x_seq.size())\n",
    "        x_seq = x_seq.view(-1, x_seq.size(-2), x_seq.size(-1))\n",
    "        print(\"Reshaped input:\", x_seq.size())\n",
    "\n",
    "        # Compute embeddings\n",
    "        embs = self.embedding(x_seq)\n",
    "        print(\"Embs:\", embs.size())\n",
    "\n",
    "        # batch_first = False\n",
    "        embs = torch.permute(embs, (1, 0, 2))\n",
    "        print(\"Seq len first input:\", embs.size())\n",
    "\n",
    "        pos_encs = self.pos_encoder(embs)\n",
    "        print(\"Pos encodings:\", pos_encs.size())\n",
    "\n",
    "        # Todo: src_key_padding_mask = (src != pad).unsqueeze(-2) ?\n",
    "        transformer_encs = self.transformer_encoder(pos_encs)\n",
    "        print(\"Transf encodings:\", transformer_encs.size())\n",
    "\n",
    "        pooled_encs = torch.mean(transformer_encs, 0)\n",
    "        print(\"Pooled encodings:\", pooled_encs.size())\n",
    "\n",
    "        # Compute node encodings\n",
    "        x_graph.x = pooled_encs\n",
    "        node_encs = self.graph_encoder(x_graph)\n",
    "        print(\"Node encodings:\", node_encs.size())\n",
    "        \n",
    "        # Compute final graph latent vector(s)\n",
    "        # (taking into account the batch size)\n",
    "        num_nodes = x_graph[0].num_nodes\n",
    "        batch_sz = node_encs.size(0) // num_nodes\n",
    "        node_encs = node_encs.view(batch_sz, num_nodes, -1)\n",
    "        encoding = torch.mean(node_encs, 1)\n",
    "\n",
    "        # Compute mu and log(std^2)\n",
    "        mu = self.linear_mu(encoding)\n",
    "        log_var = self.linear_log_var(encoding)\n",
    "        \n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_z=256, n_tracks=4, resolution=32, d_token=140, d_model=256,\n",
    "                 d_transf=256, nhead_transf=4, num_layers_transf=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # (LSTM)\n",
    "\n",
    "        # Boolean activations decoder (CNN/MLP)\n",
    "        self.acts_decoder = nn.Linear(d_z, n_tracks*resolution)\n",
    "\n",
    "        # GNN\n",
    "        self.graph_decoder = GCN()\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        self.embedding = nn.Linear(d_token, d_transf)\n",
    "        self.pos_encoder = PositionalEncoding(d_transf,dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead_transf\n",
    "        )\n",
    "        self.transf_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_layers_transf\n",
    "        )\n",
    "        \n",
    "        # Last linear layer\n",
    "        self.lin = nn.Linear(d_model, 140)\n",
    "\n",
    "\n",
    "    # Todo: batches!\n",
    "    def forward(self, z, x_seq, x_acts, x_graph):\n",
    "\n",
    "        # Compute activations from z\n",
    "        acts_out = self.acts_decoder(z)\n",
    "        acts_out = acts_out.view(x_acts.size())\n",
    "        print(\"Acts out:\", acts_out.size())\n",
    "\n",
    "        # Initialize node features with z and propagate with GNN\n",
    "        node_features = torch.repeat_interleave(\n",
    "                            z, x_acts.size(-1)*x_acts.size(-2), axis=0)\n",
    "        print(\"Node features:\", node_features.size())\n",
    "\n",
    "        # Todo: use also edge info\n",
    "        x_graph.x = node_features\n",
    "        node_decs = self.graph_decoder(x_graph)\n",
    "        print(\"Node decodings:\", node_decs.size())\n",
    "        \n",
    "        node_decs = node_decs.tile((16, 1, 1))\n",
    "        print(\"Tiled node decodings:\", node_decs.size())\n",
    "\n",
    "        # Decode features with transformer decoder\n",
    "        # forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
    "        \n",
    "        # Todo: same embeddings as encoder?\n",
    "        seq = x_seq.view(-1, x_seq.size(-2), x_seq.size(-1))\n",
    "        embs = self.embedding(seq)\n",
    "        embs = torch.permute(embs, (1, 0, 2))\n",
    "        pos_encs = self.pos_encoder(embs)\n",
    "\n",
    "        seq_out = self.transf_decoder(pos_encs, node_decs)\n",
    "        print(\"Seq out:\", seq_out.size())\n",
    "        \n",
    "        seq_out = self.lin(seq_out)\n",
    "        print(\"Seq out after lin:\", seq_out.size())\n",
    "        \n",
    "        # Softmax on first 131 values (pitch), sigmoid on last 9 (dur)\n",
    "        #seq_out[:, :, :131] = F.log_softmax(seq_out[:, :, :131], dim=-1)\n",
    "        #seq_out[:, :, 131:] = torch.sigmoid(seq_out[:, :, 131:])\n",
    "        seq_out = torch.permute(seq_out, (1, 0, 2))\n",
    "        seq_out = seq_out.view(x_seq.size())\n",
    "        print(\"Seq out after reshape\", seq_out.size())\n",
    "        \n",
    "\n",
    "        return seq_out, acts_out\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x_seq, x_acts, x_graph):\n",
    "        \n",
    "        mu, log_var = self.encoder(x_seq, x_acts, x_graph)\n",
    "        print(\"Mu:\", mu.size())\n",
    "        print(\"log_var:\", log_var.size())\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        sigma = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(sigma)\n",
    "        print(\"eps:\", eps.size())\n",
    "        z = mu + eps*sigma\n",
    "        \n",
    "        out = self.decoder(z, x_seq, x_acts, x_graph)\n",
    "        \n",
    "        return out, mu, log_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class VAETrainer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def train(self, model, trainloader):\n",
    "        \n",
    "        ce = nn.CrossEntropyLoss()\n",
    "        bce = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        # Todo: as parameter\n",
    "        for epoch in range(1):\n",
    "            \n",
    "            losses = []\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(trainloader):\n",
    "                \n",
    "                # Get the inputs\n",
    "                x_seq, x_acts, x_graph = data\n",
    "                \n",
    "                # Zero out the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Todo: remove [0] when batched\n",
    "                # Forward pass\n",
    "                out, mu, log_var = model(x_seq.float(), x_acts, x_graph)\n",
    "                seq_rec, acts_rec = out\n",
    "                \n",
    "                # Compute loss\n",
    "                acts_loss = bce(acts_rec.view(-1), x_acts.view(-1).float())\n",
    "                pitches_loss = ce(seq_rec.reshape(-1, seq_rec.size(-1))[:, :131],\n",
    "                                  x_seq.reshape(-1, x_seq.size(-1))[:, :131])\n",
    "                dur_loss = bce(seq_rec[..., 131:].reshape(-1), \n",
    "                               x_seq[..., 131:].reshape(-1))\n",
    "                kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "                loss = pitches_loss + dur_loss + acts_loss + kld_loss\n",
    "                \n",
    "                # Compute gradients and update weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % 3 == 2:\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 3))\n",
    "                    losses.append(loss.item())\n",
    "                    running_loss = 0.0\n",
    "                    \n",
    "                if i > 30:\n",
    "                    break\n",
    "            \n",
    "            plt.plot(range(1, len(losses)+1), losses)\n",
    "\n",
    "        print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5189"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dir = \"data/preprocessed\"\n",
    "dataset = MIDIDataset(ds_dir)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[7][0][0,0,:,-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 6060300\n",
      "\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "[1,     3] loss: 26.343\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "[1,     6] loss: 4.199\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "[1,     9] loss: 3.321\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "[1,    12] loss: 3.349\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "[1,    15] loss: 2.877\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "[1,    18] loss: 2.147\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "[1,    21] loss: 1.658\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "[1,    24] loss: 1.380\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "[1,    27] loss: 1.225\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "[1,    30] loss: 1.185\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Init input: torch.Size([2, 4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([256, 16, 140])\n",
      "Embs: torch.Size([256, 16, 256])\n",
      "Seq len first input: torch.Size([16, 256, 256])\n",
      "Pos encodings: torch.Size([16, 256, 256])\n",
      "Transf encodings: torch.Size([16, 256, 256])\n",
      "Pooled encodings: torch.Size([256, 256])\n",
      "Node encodings: torch.Size([256, 256])\n",
      "Mu: torch.Size([2, 256])\n",
      "log_var: torch.Size([2, 256])\n",
      "eps: torch.Size([2, 256])\n",
      "Acts out: torch.Size([2, 4, 32])\n",
      "Node features: torch.Size([256, 256])\n",
      "Node decodings: torch.Size([256, 256])\n",
      "Tiled node decodings: torch.Size([16, 256, 256])\n",
      "Seq out: torch.Size([16, 256, 256])\n",
      "Seq out after lin: torch.Size([16, 256, 140])\n",
      "Seq out after reshape torch.Size([2, 4, 32, 16, 140])\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 362
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae = VAE()\n",
    "\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in vae.parameters()))\n",
    "print()\n",
    "\n",
    "trainer = VAETrainer()\n",
    "trainer.train(vae, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTFN-DCJjZWM"
   },
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sSvVK7CxjV8"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter as Param\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter\n",
    "from torch_sparse import SparseTensor, matmul, masked_select_nnz\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (Tensor, Tensor) -> Tensor\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (SparseTensor, Tensor) -> SparseTensor\n",
    "    pass\n",
    "\n",
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return masked_select_nnz(edge_index, edge_mask, layout='coo')\n",
    "\n",
    "\n",
    "class RGCNConv(MessagePassing):\n",
    "    r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "    Relational Data with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}_{\\textrm{root}} \\cdot\n",
    "        \\mathbf{x}_i + \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
    "        \\frac{1}{|\\mathcal{N}_r(i)|} \\mathbf{\\Theta}_r \\cdot \\mathbf{x}_j,\n",
    "\n",
    "    where :math:`\\mathcal{R}` denotes the set of relations, *i.e.* edge types.\n",
    "    Edge type needs to be a one-dimensional :obj:`torch.long` tensor which\n",
    "    stores a relation identifier\n",
    "    :math:`\\in \\{ 0, \\ldots, |\\mathcal{R}| - 1\\}` for each edge.\n",
    "\n",
    "    .. note::\n",
    "        This implementation is as memory-efficient as possible by iterating\n",
    "        over each individual relation type.\n",
    "        Therefore, it may result in low GPU utilization in case the graph has a\n",
    "        large number of relations.\n",
    "        As an alternative approach, :class:`FastRGCNConv` does not iterate over\n",
    "        each individual type, but may consume a large amount of memory to\n",
    "        compensate.\n",
    "        We advise to check out both implementations to see which one fits your\n",
    "        needs.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\n",
    "            corresponds to the sizes of source and target dimensionalities.\n",
    "            In case no input features are given, this argument should\n",
    "            correspond to the number of nodes in your graph.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_relations (int): Number of relations.\n",
    "        num_bases (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the basis-decomposition regularization scheme where\n",
    "            :obj:`num_bases` denotes the number of bases to use.\n",
    "            (default: :obj:`None`)\n",
    "        num_blocks (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the block-diagonal-decomposition regularization scheme where\n",
    "            :obj:`num_blocks` denotes the number of blocks to use.\n",
    "            (default: :obj:`None`)\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
    "            (default: :obj:`\"mean\"`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        num_relations: int,\n",
    "        num_bases: Optional[int] = None,\n",
    "        num_blocks: Optional[int] = None,\n",
    "        aggr: str = 'mean',\n",
    "        root_weight: bool = True,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(aggr=aggr, node_dim=0, **kwargs)\n",
    "\n",
    "        if num_bases is not None and num_blocks is not None:\n",
    "            raise ValueError('Can not apply both basis-decomposition and '\n",
    "                             'block-diagonal-decomposition at the same time.')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_relations = num_relations\n",
    "        self.num_bases = num_bases\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "        self.in_channels_l = in_channels[0]\n",
    "\n",
    "        if num_bases is not None:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_bases, in_channels[0], out_channels))\n",
    "            self.comp = Parameter(torch.Tensor(num_relations, num_bases))\n",
    "\n",
    "        elif num_blocks is not None:\n",
    "            assert (in_channels[0] % num_blocks == 0\n",
    "                    and out_channels % num_blocks == 0)\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, num_blocks,\n",
    "                             in_channels[0] // num_blocks,\n",
    "                             out_channels // num_blocks))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        else:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, in_channels[0], out_channels))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        if root_weight:\n",
    "            self.root = Param(torch.Tensor(in_channels[1], out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Param(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        glorot(self.comp)\n",
    "        glorot(self.root)\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
    "                edge_index: Adj, edge_type: OptTensor = None):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x: The input node features. Can be either a :obj:`[num_nodes,\n",
    "                in_channels]` node feature matrix, or an optional\n",
    "                one-dimensional node index tensor (in which case input features\n",
    "                are treated as trainable node embeddings).\n",
    "                Furthermore, :obj:`x` can be of type :obj:`tuple` denoting\n",
    "                source and destination node features.\n",
    "            edge_type: The one-dimensional relation type/index for each edge in\n",
    "                :obj:`edge_index`.\n",
    "                Should be only :obj:`None` in case :obj:`edge_index` is of type\n",
    "                :class:`torch_sparse.tensor.SparseTensor`.\n",
    "                (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert input features to a pair of node features or node indices.\n",
    "        x_l: OptTensor = None\n",
    "        if isinstance(x, tuple):\n",
    "            x_l = x[0]\n",
    "        else:\n",
    "            x_l = x\n",
    "        if x_l is None:\n",
    "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
    "\n",
    "        x_r: Tensor = x_l\n",
    "        if isinstance(x, tuple):\n",
    "            x_r = x[1]\n",
    "\n",
    "        size = (x_l.size(0), x_r.size(0))\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_type = edge_index.storage.value()\n",
    "        assert edge_type is not None\n",
    "\n",
    "        # propagate_type: (x: Tensor)\n",
    "        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)\n",
    "\n",
    "        weight = self.weight\n",
    "        if self.num_bases is not None:  # Basis-decomposition =================\n",
    "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
    "                self.num_relations, self.in_channels_l, self.out_channels)\n",
    "\n",
    "        if self.num_blocks is not None:  # Block-diagonal-decomposition =====\n",
    "\n",
    "            if x_l.dtype == torch.long and self.num_blocks is not None:\n",
    "                raise ValueError('Block-diagonal decomposition not supported '\n",
    "                                 'for non-continuous input features.')\n",
    "\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                h = self.propagate(tmp, x=x_l, size=size)\n",
    "                h = h.view(-1, weight.size(1), weight.size(2))\n",
    "                h = torch.einsum('abc,bcd->abd', h, weight[i])\n",
    "                out += h.contiguous().view(-1, self.out_channels)\n",
    "\n",
    "        else:  # No regularization/Basis-decomposition ========================\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "\n",
    "                if x_l.dtype == torch.long:\n",
    "                    out += self.propagate(tmp, x=weight[i, x_l], size=size)\n",
    "                else:\n",
    "                    h = self.propagate(tmp, x=x_l, size=size)\n",
    "                    out = out + (h @ weight[i])\n",
    "\n",
    "        root = self.root\n",
    "        if root is not None:\n",
    "            out += root[x_r] if x_r.dtype == torch.long else x_r @ root\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor) -> Tensor:\n",
    "        return x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        adj_t = adj_t.set_value(None, layout=None)\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, num_relations={self.num_relations})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfsNpMLrEXLk"
   },
   "source": [
    "next edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DaVTonr_XB8",
    "outputId": "9e5fa8f9-604e-4273-a34d-fad73ad9ab7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 0 0]\n",
      " [1 1 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 0]\n",
      " [1 1 1 0]\n",
      " [0 0 0 0]]\n",
      "[[ 0  8 16 24]\n",
      " [ 1  9 17 25]\n",
      " [ 2 10 18 26]\n",
      " [ 3 11 19 27]\n",
      " [ 4 12 20 28]\n",
      " [ 5 13 21 29]\n",
      " [ 6 14 22 30]\n",
      " [ 7 15 23 31]]\n",
      "[(8, 1, 1), (8, 17, 1), (8, 25, 1), (16, 1, 1), (16, 9, 1), (16, 25, 1), (24, 1, 1), (24, 9, 1), (24, 17, 1), (1, 10, 1), (9, 2, 1), (17, 2, 1), (17, 10, 1), (25, 2, 1), (25, 10, 1), (2, 11, 1), (2, 19, 1), (2, 27, 1), (10, 3, 1), (10, 19, 1), (10, 27, 1), (3, 12, 1), (3, 28, 1), (11, 28, 1), (19, 12, 1), (19, 28, 1), (27, 12, 1), (12, 5, 1), (28, 5, 1), (5, 14, 1), (5, 22, 1)]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "a = np.random.randint(2, size=(4,8))\n",
    "a_t = a.transpose()\n",
    "print(a_t)\n",
    "inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "ts_acts = np.any(a_t, axis=1)\n",
    "ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "labels = np.arange(32).reshape(4, 8).transpose()\n",
    "print(labels)\n",
    "\n",
    "next_edges = []\n",
    "for i in range(len(ts_inds)-1):\n",
    "    ind_s = ts_inds[i]\n",
    "    ind_e = ts_inds[i+1]\n",
    "    s = inds[inds[:,0] == ind_s]\n",
    "    e = inds[inds[:,0] == ind_e]\n",
    "    e_inds = [t for t in list(itertools.product(s, e)) if t[0][1] != t[1][1]]\n",
    "    edges = [(labels[tuple(e[0])],labels[tuple(e[1])], ind_e-ind_s) for e in e_inds]\n",
    "    next_edges.extend(edges)\n",
    "\n",
    "print(next_edges)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ5JQm1aEbmb"
   },
   "source": [
    "onset edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DISmsJB3EatR",
    "outputId": "fc864608-63a6-4ad0-84d9-1478001ce60e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 0 0]\n",
      " [1 1 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 0]\n",
      " [1 1 1 0]\n",
      " [0 0 0 0]]\n",
      "[[ 0  8 16 24]\n",
      " [ 1  9 17 25]\n",
      " [ 2 10 18 26]\n",
      " [ 3 11 19 27]\n",
      " [ 4 12 20 28]\n",
      " [ 5 13 21 29]\n",
      " [ 6 14 22 30]\n",
      " [ 7 15 23 31]]\n",
      "[(8, 16, 0), (8, 24, 0), (16, 24, 0), (16, 8, 0), (24, 8, 0), (24, 16, 0), (1, 9, 0), (1, 17, 0), (1, 25, 0), (9, 17, 0), (9, 25, 0), (17, 25, 0), (9, 1, 0), (17, 1, 0), (25, 1, 0), (17, 9, 0), (25, 9, 0), (25, 17, 0), (2, 10, 0), (10, 2, 0), (3, 11, 0), (3, 19, 0), (3, 27, 0), (11, 19, 0), (11, 27, 0), (19, 27, 0), (11, 3, 0), (19, 3, 0), (27, 3, 0), (19, 11, 0), (27, 11, 0), (27, 19, 0), (12, 28, 0), (28, 12, 0), (6, 14, 0), (6, 22, 0), (14, 22, 0), (14, 6, 0), (22, 6, 0), (22, 14, 0)]\n"
     ]
    }
   ],
   "source": [
    "onset_edges = []\n",
    "print(a_t)\n",
    "print(labels)\n",
    "\n",
    "for i in ts_inds:\n",
    "    ts_acts_inds = list(inds[inds[:,0] == i])\n",
    "    if len(ts_acts_inds) < 2:\n",
    "        continue\n",
    "    e_inds = list(itertools.combinations(ts_acts_inds, 2))\n",
    "    edges = [(labels[tuple(e[0])], labels[tuple(e[1])], 0) for e in e_inds]\n",
    "    inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "    onset_edges.extend(edges)\n",
    "    onset_edges.extend(inv_edges)\n",
    "\n",
    "print(onset_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujitZCKaa7nu"
   },
   "source": [
    "track edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbVG1vdFa-7e",
    "outputId": "c042449b-eef2-4707-a524-5f66f3ec07c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]\n",
      " [1 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [1 1 0 0]\n",
      " [0 0 1 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 1]]\n",
      "[[ 0  8 16 24]\n",
      " [ 1  9 17 25]\n",
      " [ 2 10 18 26]\n",
      " [ 3 11 19 27]\n",
      " [ 4 12 20 28]\n",
      " [ 5 13 21 29]\n",
      " [ 6 14 22 30]\n",
      " [ 7 15 23 31]]\n",
      "[(array([0, 0]), array([1, 0])), (array([1, 0]), array([4, 0]))]\n",
      "[(array([0, 1]), array([2, 1])), (array([2, 1]), array([4, 1]))]\n",
      "[(array([0, 2]), array([5, 2])), (array([5, 2]), array([6, 2])), (array([6, 2]), array([7, 2]))]\n",
      "[(array([0, 3]), array([1, 3])), (array([1, 3]), array([5, 3])), (array([5, 3]), array([7, 3]))]\n",
      "[(0, 1, 1), (1, 4, 3), (8, 10, 2), (10, 12, 2), (16, 21, 5), (21, 22, 1), (22, 23, 1), (24, 25, 1), (25, 29, 4), (29, 31, 2)]\n"
     ]
    }
   ],
   "source": [
    "print(a_t)\n",
    "print(labels)\n",
    "track_edges = []\n",
    "\n",
    "for track in range(a_t.shape[1]):\n",
    "    tr_inds = list(inds[inds[:,1] == track])\n",
    "    e_inds = [(tr_inds[i],\n",
    "               tr_inds[i+1]) for i in range(len(tr_inds)-1)]\n",
    "    print(e_inds)\n",
    "    edges = [(labels[tuple(e[0])], labels[tuple(e[1])], e[1][0]-e[0][0]) for e in e_inds]\n",
    "    track_edges.extend(edges)\n",
    "\n",
    "print(track_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DzouJ5NqALB",
    "outputId": "20a76e82-6305-4154-d894-6d69a64435a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_edges = np.array(track_edges)\n",
    "onset_edges = np.array(onset_edges)\n",
    "np.concatenate((track_edges, onset_edges)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihIYkPWPzyGX"
   },
   "outputs": [],
   "source": [
    "pip install pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie0pU8NWAUNM"
   },
   "outputs": [],
   "source": [
    "import pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTbGBSrdAZGH"
   },
   "outputs": [],
   "source": [
    "multitrack = pypianoroll.read(\"tests_fur-elise.mid\")\n",
    "print(multitrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eVo_BKzAmz4"
   },
   "outputs": [],
   "source": [
    "multitrack.tracks[0].pianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPpWw-rLA7CI"
   },
   "outputs": [],
   "source": [
    "multitrack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-PYbS7FA-Gg"
   },
   "outputs": [],
   "source": [
    "multitrack.trim(0, 12 * multitrack.resolution)\n",
    "multitrack.binarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psxuoTsZBFXY"
   },
   "outputs": [],
   "source": [
    "multitrack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovyixmSvBG3w"
   },
   "outputs": [],
   "source": [
    "multitrack.tracks[0].pianoroll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHlKNufuBzLn"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOhjCOJb34P4bTid7qFDg58",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1NeVldMsPVJd6pXbxZDmuiUP-QJBRhYtj",
   "name": "midi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
