{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmanueleCosenza/Polyphemus/blob/main/midi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "50lpUn9bO0ug",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cosenza/thesis/Polyphemus\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -C data -xvzf data/lmd_matched.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "She0QbN5Kopo",
    "outputId": "0f3fb4c7-bd7d-4ee4-b2cd-d567d8e490db",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: muspy in /home/cosenza/penv/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (6.0)\n",
      "Requirement already satisfied: mido>=1.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (1.2.10)\n",
      "Requirement already satisfied: bidict>=0.21 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (0.21.4)\n",
      "Requirement already satisfied: tqdm>=4.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (4.62.3)\n",
      "Requirement already satisfied: pypianoroll>=1.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (1.0.4)\n",
      "Requirement already satisfied: matplotlib>=1.5 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (3.3.4)\n",
      "Requirement already satisfied: requests>=2.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (2.26.0)\n",
      "Requirement already satisfied: pretty-midi>=0.2 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (0.2.9)\n",
      "Requirement already satisfied: joblib>=0.15 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (1.1.0)\n",
      "Requirement already satisfied: music21>=5.0 in /home/cosenza/penv/lib/python3.6/site-packages (from muspy) (6.7.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->muspy) (2.8.2)\n",
      "Requirement already satisfied: chardet in /home/cosenza/penv/lib/python3.6/site-packages (from music21>=5.0->muspy) (4.0.0)\n",
      "Requirement already satisfied: more-itertools in /home/cosenza/penv/lib/python3.6/site-packages (from music21>=5.0->muspy) (8.12.0)\n",
      "Requirement already satisfied: webcolors in /home/cosenza/penv/lib/python3.6/site-packages (from music21>=5.0->muspy) (1.11.1)\n",
      "Requirement already satisfied: six in /home/cosenza/penv/lib/python3.6/site-packages (from pretty-midi>=0.2->muspy) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/cosenza/penv/lib/python3.6/site-packages (from pypianoroll>=1.0->muspy) (1.5.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cosenza/penv/lib/python3.6/site-packages (from requests>=2.0->muspy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/cosenza/penv/lib/python3.6/site-packages (from requests>=2.0->muspy) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/cosenza/penv/lib/python3.6/site-packages (from requests>=2.0->muspy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cosenza/penv/lib/python3.6/site-packages (from requests>=2.0->muspy) (2021.10.8)\n",
      "Requirement already satisfied: pypianoroll in /home/cosenza/penv/lib/python3.6/site-packages (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/cosenza/penv/lib/python3.6/site-packages (from pypianoroll) (1.19.5)\n",
      "Requirement already satisfied: matplotlib>=1.5 in /home/cosenza/penv/lib/python3.6/site-packages (from pypianoroll) (3.3.4)\n",
      "Requirement already satisfied: pretty-midi>=0.2.8 in /home/cosenza/penv/lib/python3.6/site-packages (from pypianoroll) (0.2.9)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/cosenza/penv/lib/python3.6/site-packages (from pypianoroll) (1.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->pypianoroll) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->pypianoroll) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->pypianoroll) (3.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->pypianoroll) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/cosenza/penv/lib/python3.6/site-packages (from matplotlib>=1.5->pypianoroll) (0.11.0)\n",
      "Requirement already satisfied: mido>=1.1.16 in /home/cosenza/penv/lib/python3.6/site-packages (from pretty-midi>=0.2.8->pypianoroll) (1.2.10)\n",
      "Requirement already satisfied: six in /home/cosenza/penv/lib/python3.6/site-packages (from pretty-midi>=0.2.8->pypianoroll) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the required music libraries\n",
    "!pip3 install muspy\n",
    "!pip3 install pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uveQkY7O0CF",
    "outputId": "12e1f638-ee78-4617-844a-10e9a26c298e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu102.html\n",
      "Requirement already satisfied: torch-scatter in /home/cosenza/penv/lib/python3.6/site-packages (2.0.9)\n",
      "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu102.html\n",
      "Requirement already satisfied: torch-sparse in /home/cosenza/penv/lib/python3.6/site-packages (0.6.12)\n",
      "Requirement already satisfied: scipy in /home/cosenza/penv/lib/python3.6/site-packages (from torch-sparse) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/cosenza/penv/lib/python3.6/site-packages (from scipy->torch-sparse) (1.19.5)\n",
      "Requirement already satisfied: torch-geometric in /home/cosenza/penv/lib/python3.6/site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (1.19.5)\n",
      "Requirement already satisfied: tqdm in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (4.62.3)\n",
      "Requirement already satisfied: scipy in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (1.5.4)\n",
      "Requirement already satisfied: networkx in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (0.24.2)\n",
      "Requirement already satisfied: requests in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (2.26.0)\n",
      "Requirement already satisfied: pandas in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (1.1.5)\n",
      "Requirement already satisfied: rdflib in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (5.0.0)\n",
      "Requirement already satisfied: googledrivedownloader in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (0.4)\n",
      "Requirement already satisfied: jinja2 in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (3.0.3)\n",
      "Requirement already satisfied: pyparsing in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (3.0.6)\n",
      "Requirement already satisfied: yacs in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (0.1.8)\n",
      "Requirement already satisfied: PyYAML in /home/cosenza/penv/lib/python3.6/site-packages (from torch-geometric) (6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cosenza/penv/lib/python3.6/site-packages (from jinja2->torch-geometric) (2.0.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /home/cosenza/penv/lib/python3.6/site-packages (from networkx->torch-geometric) (4.4.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/cosenza/penv/lib/python3.6/site-packages (from pandas->torch-geometric) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/cosenza/penv/lib/python3.6/site-packages (from pandas->torch-geometric) (2.8.2)\n",
      "Requirement already satisfied: isodate in /home/cosenza/penv/lib/python3.6/site-packages (from rdflib->torch-geometric) (0.6.0)\n",
      "Requirement already satisfied: six in /home/cosenza/penv/lib/python3.6/site-packages (from rdflib->torch-geometric) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cosenza/penv/lib/python3.6/site-packages (from requests->torch-geometric) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/cosenza/penv/lib/python3.6/site-packages (from requests->torch-geometric) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/cosenza/penv/lib/python3.6/site-packages (from requests->torch-geometric) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cosenza/penv/lib/python3.6/site-packages (from requests->torch-geometric) (2021.10.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/cosenza/penv/lib/python3.6/site-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/cosenza/penv/lib/python3.6/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Install torch_geometric\n",
    "!v=$(python -c \"import torch; print(torch.__version__)\"); \\\n",
    "pip install torch-scatter -f https://data.pyg.org/whl/torch-${v}.html; \\\n",
    "pip install torch-sparse -f https://data.pyg.org/whl/torch-${v}.html; \\\n",
    "pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B45l1513wJ1Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import muspy\n",
    "from itertools import product\n",
    "import pypianoroll as pproll\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class MIDIPreprocessor():\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def preprocess_dataset(self, dir, early_exit=None):\n",
    "        pass\n",
    "    \n",
    "    def preprocess_file(self, f):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Todo: to config file (or separate files)\n",
    "MAX_SIMU_NOTES = 16 # 14 + SOS and EOS\n",
    "\n",
    "PITCH_SOS = 128\n",
    "PITCH_EOS = 129\n",
    "PITCH_PAD = 130\n",
    "DUR_PAD_IND = 2\n",
    "MAX_DUR = 511 # equivalent to 16 bars (with RESOLUTION=32)\n",
    "\n",
    "RESOLUTION = 32\n",
    "NUM_BARS = 1\n",
    "\n",
    "\n",
    "def preprocess_file(filepath, dest_dir, num_samples):\n",
    "\n",
    "    saved_samples = 0\n",
    "\n",
    "    print()\n",
    "    print(\"Preprocessing file \" + filepath)\n",
    "\n",
    "    # Load the file both as a pypianoroll song and a muspy song\n",
    "    # (Need to load both since muspy.to_pypianoroll() is expensive)\n",
    "    try:\n",
    "        pproll_song = pproll.read(filepath, resolution=RESOLUTION)\n",
    "        muspy_song = muspy.read(filepath)\n",
    "    except Exception as e:\n",
    "        print(\"Song skipped (Invalid song format)\")\n",
    "        return 0\n",
    "    \n",
    "    # Only accept songs that have a time signature of 4/4 and no time changes\n",
    "    for t in muspy_song.time_signatures:\n",
    "        if t.numerator != 4 or t.denominator != 4:\n",
    "            print(\"Song skipped ({}/{} time signature)\".\n",
    "                            format(t.numerator, t.denominator))\n",
    "            return 0\n",
    "\n",
    "    # Gather tracks of pypianoroll song based on MIDI program number\n",
    "    drum_tracks = []\n",
    "    bass_tracks = []\n",
    "    guitar_tracks = []\n",
    "    strings_tracks = []\n",
    "\n",
    "    for track in pproll_song.tracks:\n",
    "        if track.is_drum:\n",
    "            track.name = 'Drums'\n",
    "            drum_tracks.append(track)\n",
    "        elif 0 <= track.program <= 31:\n",
    "            track.name = 'Guitar'\n",
    "            guitar_tracks.append(track)\n",
    "        elif 32 <= track.program <= 39:\n",
    "            track.name = 'Bass'\n",
    "            bass_tracks.append(track)\n",
    "        else:\n",
    "            # Tracks with program > 39 are all considered as strings tracks\n",
    "            # and will be merged into a single track later on\n",
    "            strings_tracks.append(track)\n",
    "\n",
    "    # Filter song if it does not contain drum, guitar, bass or strings tracks\n",
    "    if not drum_tracks or not guitar_tracks \\\n",
    "       or not bass_tracks or not strings_tracks:\n",
    "        print(\"Song skipped (does not contain drum or \"\n",
    "                \"guitar or bass or strings tracks)\")\n",
    "        return 0\n",
    "    \n",
    "    # Merge strings tracks into a single pypianoroll track\n",
    "    strings = pproll.Multitrack(tracks=strings_tracks)\n",
    "    strings_track = pproll.Track(pianoroll=strings.blend(mode='max'),\n",
    "                                 program=48, name='Strings')\n",
    "\n",
    "    combinations = list(product(drum_tracks, bass_tracks, guitar_tracks))\n",
    "\n",
    "    # Single instruments can have multiple tracks.\n",
    "    # Consider all possible combinations of drum, bass, and guitar tracks\n",
    "    for i, combination in enumerate(combinations):\n",
    "\n",
    "        print(\"Processing combination\", i+1, \"of\", len(combinations))\n",
    "        \n",
    "        # Process combination (called 'subsong' from now on)\n",
    "        drum_track, bass_track, guitar_track = combination\n",
    "        tracks = [drum_track, bass_track, guitar_track, strings_track]\n",
    "        \n",
    "        pproll_subsong = pproll.Multitrack(\n",
    "            tracks=tracks,\n",
    "            tempo=pproll_song.tempo,\n",
    "            resolution=RESOLUTION\n",
    "        )\n",
    "        muspy_subsong = muspy.from_pypianoroll(pproll_subsong)\n",
    "        \n",
    "        tracks_notes = [track.notes for track in muspy_subsong.tracks]\n",
    "        \n",
    "        # Obtain length of subsong (maximum of each track's length)\n",
    "        length = 0\n",
    "        for notes in tracks_notes:\n",
    "            track_length = max(note.end for note in notes)\n",
    "            length = max(length, track_length)\n",
    "        length += 1\n",
    "\n",
    "        # Add timesteps until length is a multiple of RESOLUTION\n",
    "        length = length if length%(RESOLUTION) == 0 \\\n",
    "                                else length + (RESOLUTION-(length%(RESOLUTION)))\n",
    "\n",
    "\n",
    "        tracks_tensors = []\n",
    "        tracks_activations = []\n",
    "\n",
    "        dur_bin_length = int(np.ceil(np.log2(MAX_DUR)))\n",
    "\n",
    "        # Todo: adapt to velocity\n",
    "        for notes in tracks_notes:\n",
    "\n",
    "            # Initialize encoder-ready track tensor\n",
    "            # track_tensor: (length x max_simu_notes x 2 (or 3 if velocity))\n",
    "            # The last dimension contains pitches and durations (and velocities)\n",
    "            # int16 is enough for small to medium duration values\n",
    "            track_tensor = np.zeros((length, MAX_SIMU_NOTES, 2), np.int16)\n",
    "\n",
    "            track_tensor[:, :, 0] = PITCH_PAD\n",
    "            track_tensor[:, 0, 0] = PITCH_SOS\n",
    "\n",
    "            # Keeps track of how many notes have been stored in each timestep\n",
    "            # (int8 imposes that MAX_SIMU_NOTES < 256)\n",
    "            notes_counter = np.ones(length, dtype=np.int8)\n",
    "\n",
    "            # Todo: np.put_along_axis?\n",
    "            for note in notes:\n",
    "                # Insert note in the lowest position available in the timestep\n",
    "                \n",
    "                t = note.time\n",
    "\n",
    "                if notes_counter[t] >= MAX_SIMU_NOTES-1:\n",
    "                    # Skip note if there is no more space\n",
    "                    continue\n",
    "\n",
    "                track_tensor[t, notes_counter[t], 0] = note.pitch\n",
    "                track_tensor[t, notes_counter[t], 1] = note.duration\n",
    "                notes_counter[t] += 1\n",
    "            \n",
    "            # Add end of sequence token\n",
    "            track_tensor[np.arange(0, length), notes_counter, 0] = PITCH_EOS\n",
    "\n",
    "            # Get track activations, a boolean tensor indicating whether notes\n",
    "            # are being played in a timestep (sustain does not count)\n",
    "            # (needed for graph rep.)\n",
    "            activations = np.array(notes_counter-1, dtype=bool)\n",
    "\n",
    "            tracks_tensors.append(track_tensor)\n",
    "            tracks_activations.append(activations)\n",
    "        \n",
    "        # (#tracks x length x max_simu_notes x 2 (or 3))\n",
    "        subsong_tensor = np.stack(tracks_tensors, axis=0)\n",
    "\n",
    "        # (#tracks x length)\n",
    "        subsong_activations = np.stack(tracks_activations, axis=0)\n",
    "\n",
    "\n",
    "        # Slide window over 'subsong_tensor' and 'subsong_activations' along the\n",
    "        # time axis (2nd dimension) with the stride of a bar\n",
    "        # Todo: np.lib.stride_tricks.as_strided(song_proll)\n",
    "        for i in range(0, length-NUM_BARS*RESOLUTION+1, RESOLUTION):\n",
    "            \n",
    "            # Get the sequence and its activations\n",
    "            seq_tensor = subsong_tensor[:, i:i+NUM_BARS*RESOLUTION, :]\n",
    "            seq_acts = subsong_activations[:, i:i+NUM_BARS*RESOLUTION]\n",
    "\n",
    "            # Skip sequence if it contains more than one bar of consecutive\n",
    "            # silence in at least one track\n",
    "            bars = seq_acts.reshape(seq_acts.shape[0], NUM_BARS, -1)\n",
    "            bars_acts = np.any(bars, axis=2)\n",
    "            \n",
    "            if 1 in np.diff(np.where(bars_acts == 0)[1]):\n",
    "                continue\n",
    "\n",
    "            # Randomly transpose the pitches of the sequence (-5 to 6 semitones)\n",
    "            shift = np.random.choice(np.arange(-5, 7), 1)\n",
    "            cond = (seq_tensor[:, :, :, 0] != PITCH_PAD) &                     \\\n",
    "                   (seq_tensor[:, :, :, 0] != PITCH_SOS) &                     \\\n",
    "                   (seq_tensor[:, :, :, 0] != PITCH_EOS)\n",
    "            seq_tensor[cond, 0] += shift\n",
    "\n",
    "            # Save sample (seq_tensor and seq_acts) to file\n",
    "            curr_sample = str(num_samples + saved_samples)\n",
    "            sample_filepath = os.path.join(dest_dir, curr_sample)\n",
    "            np.savez(sample_filepath, seq_tensor=seq_tensor, seq_acts=seq_acts)\n",
    "\n",
    "            saved_samples += 1\n",
    "\n",
    "\n",
    "    print(\"File preprocessing finished. Saved samples:\", saved_samples)\n",
    "    print()\n",
    "\n",
    "    return saved_samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Total number of files: 116189\n",
    "# Number of unique files: 45129\n",
    "def preprocess_dataset(dataset_dir, dest_dir, early_exit=None):\n",
    "\n",
    "    files_dict = {}\n",
    "    seen = 0\n",
    "    tot_samples = 0\n",
    "    finished = False\n",
    "\n",
    "    # Visit recursively the directories inside the dataset directory\n",
    "    for dirpath, dirs, files in os.walk(dataset_dir):\n",
    "\n",
    "        # Sort alphabetically the found directories\n",
    "        # (to help guess the remaining time) \n",
    "        dirs.sort()\n",
    "        \n",
    "        print(\"Current path:\", dirpath)\n",
    "\n",
    "        for f in files:\n",
    "            \n",
    "            seen += 1\n",
    "\n",
    "            if f in files_dict:\n",
    "                # Skip already seen file\n",
    "                files_dict[f] += 1\n",
    "                continue\n",
    "\n",
    "            # File never seen before, add to dictionary of files\n",
    "            # (from filename to # of occurrences)\n",
    "            files_dict[f] = 1\n",
    "\n",
    "            # Preprocess file\n",
    "            filepath = os.path.join(dirpath, f)\n",
    "            saved = preprocess_file(filepath, dest_dir, tot_samples)\n",
    "\n",
    "            tot_samples += saved\n",
    "\n",
    "            # Exit when a maximum number of files has been processed (if set)\n",
    "            if early_exit != None and len(files_dict) >= early_exit:\n",
    "                finished = True\n",
    "                break\n",
    "\n",
    "        # Todo: also print # of processed (not filtered) files\n",
    "        #       and # of produced sequences (samples)\n",
    "        print(\"Total number of seen files:\", seen)\n",
    "        print(\"Number of unique files:\", len(files_dict))\n",
    "        print(\"Total number of saved samples:\", tot_samples)\n",
    "        print()\n",
    "\n",
    "        if finished:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aYc5y-CYyetK"
   },
   "outputs": [],
   "source": [
    "!rm -rf data/preprocessed/\n",
    "!mkdir data/preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqnubg3oP4ES",
    "outputId": "40cc38a2-1f7d-4f6f-e6c9-9e14dfc7f683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current path: data/lmd_matched\n",
      "Total number of seen files: 0\n",
      "Number of unique files: 0\n",
      "Total number of saved samples: 0\n",
      "\n",
      "Current path: data/lmd_matched/A\n",
      "Total number of seen files: 0\n",
      "Number of unique files: 0\n",
      "Total number of saved samples: 0\n",
      "\n",
      "Current path: data/lmd_matched/A/A\n",
      "Total number of seen files: 0\n",
      "Number of unique files: 0\n",
      "Total number of saved samples: 0\n",
      "\n",
      "Current path: data/lmd_matched/A/A/A\n",
      "Total number of seen files: 0\n",
      "Number of unique files: 0\n",
      "Total number of saved samples: 0\n",
      "\n",
      "Current path: data/lmd_matched/A/A/A/TRAAAGR128F425B14B\n",
      "\n",
      "Preprocessing file data/lmd_matched/A/A/A/TRAAAGR128F425B14B/1d9d16a9da90c090809c153754823c2b.mid\n",
      "Processing combination 1 of 7\n",
      "Processing combination 2 of 7\n",
      "Processing combination 3 of 7\n",
      "Processing combination 4 of 7\n",
      "Processing combination 5 of 7\n",
      "Processing combination 6 of 7\n",
      "Processing combination 7 of 7\n",
      "File preprocessing finished. Saved samples: 3059\n",
      "\n",
      "\n",
      "Preprocessing file data/lmd_matched/A/A/A/TRAAAGR128F425B14B/5dd29e99ed7bd3cc0c5177a6e9de22ea.mid\n",
      "Processing combination 1 of 5\n",
      "Processing combination 2 of 5\n",
      "Processing combination 3 of 5\n",
      "Processing combination 4 of 5\n",
      "Processing combination 5 of 5\n",
      "File preprocessing finished. Saved samples: 2130\n",
      "\n",
      "Total number of seen files: 2\n",
      "Number of unique files: 2\n",
      "Total number of saved samples: 5189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset\n",
    "dataset_dir = 'data/lmd_matched'\n",
    "dest_dir = 'data/preprocessed'\n",
    "preprocess_dataset(dataset_dir, dest_dir, early_exit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG88mekfrrcp"
   },
   "source": [
    "Check preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JlP6iUNugNtP"
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(dest_dir, \"5.npz\")\n",
    "data = np.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VUpOEObhwYQ",
    "outputId": "aac6e029-93b1-485f-f13a-2a00abedbc7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 32, 16, 2)\n",
      "(4, 32)\n"
     ]
    }
   ],
   "source": [
    "print(data[\"seq_tensor\"].shape)\n",
    "print(data[\"seq_acts\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6NA5IAAmtK8",
    "outputId": "6e661b3a-05a1-4e2d-9a3d-e1c037b4d04f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128,   0],\n",
       "       [129,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0]], dtype=int16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"seq_tensor\"][0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C19X9m-3iMlm"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zymqD-UqR8wq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import itertools\n",
    "\n",
    "\n",
    "def unpackbits(x, num_bits):\n",
    "\n",
    "    if np.issubdtype(x.dtype, np.floating):\n",
    "        raise ValueError(\"numpy data type needs to be int-like\")\n",
    "\n",
    "    xshape = list(x.shape)\n",
    "    x = x.reshape([-1, 1])\n",
    "    mask = 2**np.arange(num_bits, dtype=x.dtype).reshape([1, num_bits])\n",
    "\n",
    "    return (x & mask).astype(bool).astype(int).reshape(xshape + [num_bits])\n",
    "\n",
    "\n",
    "class MIDIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir):\n",
    "        self.dir = dir\n",
    "\n",
    "    def __len__(self):\n",
    "        _, _, files = next(os.walk(self.dir))\n",
    "        return len(files)\n",
    "\n",
    "    \n",
    "    def __get_track_edges(self, acts, edge_type_ind=0):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "\n",
    "        labels = np.arange(acts.shape[0]*acts.shape[1])\n",
    "        labels = labels.reshape(acts.shape[0], acts.shape[1]).transpose()\n",
    "\n",
    "        track_edges = []\n",
    "\n",
    "        for track in range(a_t.shape[1]):\n",
    "            tr_inds = list(inds[inds[:,1] == track])\n",
    "            e_inds = [(tr_inds[i],\n",
    "                    tr_inds[i+1]) for i in range(len(tr_inds)-1)]\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, e[1][0]-e[0][0]) for e in e_inds]\n",
    "            track_edges.extend(edges)\n",
    "\n",
    "        return np.array(track_edges, dtype='long')\n",
    "\n",
    "    \n",
    "    def __get_onset_edges(self, acts, edge_type_ind=1):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        labels = np.arange(acts.shape[0]*acts.shape[1])\n",
    "        labels = labels.reshape(acts.shape[0], acts.shape[1]).transpose()\n",
    "\n",
    "        onset_edges = []\n",
    "\n",
    "        for i in ts_inds:\n",
    "            ts_acts_inds = list(inds[inds[:,0] == i])\n",
    "            if len(ts_acts_inds) < 2:\n",
    "                continue\n",
    "            e_inds = list(itertools.combinations(ts_acts_inds, 2))\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, 0) for e in e_inds]\n",
    "            inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "            onset_edges.extend(edges)\n",
    "            onset_edges.extend(inv_edges)\n",
    "\n",
    "        return np.array(onset_edges, dtype='long')\n",
    "\n",
    "\n",
    "    def __get_next_edges(self, acts, edge_type_ind=2):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        labels = np.arange(acts.shape[0]*acts.shape[1])\n",
    "        labels = labels.reshape(acts.shape[0], acts.shape[1]).transpose()\n",
    "\n",
    "        next_edges = []\n",
    "\n",
    "        for i in range(len(ts_inds)-1):\n",
    "\n",
    "            ind_s = ts_inds[i]\n",
    "            ind_e = ts_inds[i+1]\n",
    "            s = inds[inds[:,0] == ind_s]\n",
    "            e = inds[inds[:,0] == ind_e]\n",
    "\n",
    "            e_inds = [t for t in list(itertools.product(s, e)) if t[0][1] != t[1][1]]\n",
    "            edges = [(labels[tuple(e[0])],labels[tuple(e[1])], edge_type_ind, ind_e-ind_s) for e in e_inds]\n",
    "\n",
    "            next_edges.extend(edges)\n",
    "\n",
    "        return np.array(next_edges, dtype='long')\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Load tensors\n",
    "        sample_path = os.path.join(self.dir, str(idx) + \".npz\")\n",
    "        data = np.load(sample_path)\n",
    "\n",
    "        seq_tensor = data[\"seq_tensor\"]\n",
    "        seq_acts = data[\"seq_acts\"]\n",
    "\n",
    "        # From decimals to one-hot (pitch)\n",
    "        pitches = seq_tensor[:, :, :, 0]\n",
    "        onehot = np.zeros((pitches.shape[0]*pitches.shape[1]*pitches.shape[2],\n",
    "                            131), dtype=np.float)\n",
    "        onehot[np.arange(0, onehot.shape[0]), pitches.reshape(-1)] = 1.\n",
    "        onehot = onehot.reshape(-1, pitches.shape[1], seq_tensor.shape[2], 131)\n",
    "\n",
    "        # From decimals to binary (pitch)\n",
    "        durs = seq_tensor[:, :, :, 1]\n",
    "        bin_durs = unpackbits(durs, 9)[:, :, :, ::-1]\n",
    "\n",
    "        # Concatenate pitches and durations\n",
    "        new_seq_tensor = np.concatenate((onehot[:, :, :, :], bin_durs),\n",
    "                             axis=-1)\n",
    "        \n",
    "        # Construct graph from boolean activations\n",
    "        track_edges = self.__get_track_edges(seq_acts)\n",
    "        onset_edges = self.__get_onset_edges(seq_acts)\n",
    "        next_edges = self.__get_next_edges(seq_acts)\n",
    "        edges = [track_edges, onset_edges, next_edges]\n",
    "        \n",
    "        print(len(track_edges), len(onset_edges), len(next_edges))\n",
    "\n",
    "        if len(track_edges) == 0 and len(onset_edges) == 0 and len(next_edges) == 0:\n",
    "            edge_list = np.array([])\n",
    "        else:\n",
    "            edge_list = np.concatenate([x for x in edges if x.size > 0])\n",
    "        \n",
    "        return new_seq_tensor, data[\"seq_acts\"], edge_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mqNwlt-hVKbX"
   },
   "outputs": [],
   "source": [
    "ds_dir = \"data/preprocessed\"\n",
    "dataset = MIDIDataset(ds_dir)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7n61CIb3bB5",
    "outputId": "faf4099c-b444-4230-9ffc-576985cfc99c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[7][0][0,0,:,-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hSwcnlq4g50O"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cosenza/penv/lib64/python3.6/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Todo: check and think about max_len\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 256):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *                     \\\n",
    "                             (-math.log(10000.0)/d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position*div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, features_dims=[256, 256, 256], num_relations=3):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(features_dims)-1):\n",
    "            self.layers.append(GCNConv(features_dims[i], features_dims[i+1]))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    # 140 = 128+3+9\n",
    "    def __init__(self, d_token=140, d_transf=256, nhead_transf=4, \n",
    "                 num_layers_transf=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Todo: one separate encoder for drums\n",
    "        # Transformer Encoder\n",
    "        self.embedding = nn.Linear(d_token, d_transf)\n",
    "        self.pos_encoder = PositionalEncoding(d_transf, dropout)\n",
    "        transf_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_transf,\n",
    "            nhead=nhead_transf\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            transf_layer,\n",
    "            num_layers=num_layers_transf\n",
    "        )\n",
    "\n",
    "        # Graph encoder\n",
    "        self.graph_encoder = GCN()\n",
    "\n",
    "        # (LSTM)\n",
    "        \n",
    "        # Linear layers that compute the final mu and log_var\n",
    "        # Todo: as parameters\n",
    "        self.linear_mu = nn.Linear(256, 256)\n",
    "        self.linear_log_var = nn.Linear(256, 256)\n",
    "\n",
    "    def forward(self, x_seq, x_acts, x_edges):\n",
    "\n",
    "        # Move to training loop!\n",
    "        #x_seq, x_acts = torch.Tensor(x_seq), torch.Tensor(x_acts)\n",
    "\n",
    "        # Collapse track dimension\n",
    "        print(\"Init input:\", x_seq.size())\n",
    "        x_seq = x_seq.view(x_seq.size(0)*x_seq.size(1), x_seq.size(2), -1)\n",
    "        print(\"Reshaped input:\", x_seq.size())\n",
    "\n",
    "        # Compute embeddings\n",
    "        embs = self.embedding(x_seq)\n",
    "        print(\"Embs:\", embs.size())\n",
    "\n",
    "        # batch_first = False\n",
    "        embs = torch.permute(embs, (1, 0, 2))\n",
    "        print(\"Seq len first input:\", embs.size())\n",
    "\n",
    "        pos_encs = self.pos_encoder(embs)\n",
    "        print(\"Pos encodings:\", pos_encs.size())\n",
    "\n",
    "        # src_key_padding_mask = (src != pad).unsqueeze(-2) ?\n",
    "        transformer_encs = self.transformer_encoder(pos_encs)\n",
    "        print(\"Transf encodings:\", transformer_encs.size())\n",
    "\n",
    "        pooled_encs = torch.mean(transformer_encs, 0)\n",
    "        print(\"Pooled encodings:\", pooled_encs.size())\n",
    "\n",
    "        print(pooled_encs.view(4, 32, -1).size())\n",
    "\n",
    "        # Compute graph embedding (Do not consider type and dist for now)\n",
    "        edge_index = x_edges[:,:2].t().contiguous() if len(x_edges.size()) == 0 else torch.LongTensor([[], []])\n",
    "        graph = Data(x=pooled_encs, edge_index=edge_index)\n",
    "        node_encs = self.graph_encoder(graph)\n",
    "\n",
    "        print(\"Node encodings:\", node_encs.size())\n",
    "\n",
    "        # Final encoding, mu and log(std^2)\n",
    "        encoding = torch.mean(node_encs, 0)\n",
    "        mu = self.linear_mu(encoding)\n",
    "        log_var = self.linear_log_var(encoding)\n",
    "        \n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_z=256, n_tracks=4, resolution=32, d_token=140, d_model=256,\n",
    "                 d_transf=256, nhead_transf=4, num_layers_transf=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # (LSTM)\n",
    "\n",
    "        # Boolean activations decoder (CNN/MLP)\n",
    "        self.acts_decoder = nn.Linear(d_z, n_tracks*resolution)\n",
    "\n",
    "        # GNN\n",
    "        self.graph_decoder = GCN()\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        self.embedding = nn.Linear(d_token, d_transf)\n",
    "        self.pos_encoder = PositionalEncoding(d_transf, dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead_transf)\n",
    "        self.transf_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers_transf)\n",
    "        \n",
    "        # Last linear layer\n",
    "        self.lin = nn.Linear(d_model, 140)\n",
    "\n",
    "\n",
    "    # Todo: batches!\n",
    "    def forward(self, z, x_seq, x_acts, x_edges):\n",
    "\n",
    "        # Compute activations from z\n",
    "        acts_out = self.acts_decoder(z)\n",
    "        #acts_out = torch.sigmoid(acts_out)\n",
    "        acts_out = acts_out.view(x_acts.size())\n",
    "        print(\"Acts out:\", acts_out.size())\n",
    "\n",
    "        # Initialize node features with z and propagate with GNN\n",
    "        node_features = z.tile((x_seq.size(0)*x_seq.size(1), 1))\n",
    "        print(\"Node features:\", node_features.size())\n",
    "\n",
    "        # Todo: use also edge info\n",
    "        edge_index = x_edges[:,:2].t().contiguous() if len(x_edges.size()) == 0 else torch.LongTensor([[], []])\n",
    "        graph = Data(x=node_features, edge_index=edge_index)\n",
    "        node_decs = self.graph_decoder(graph)\n",
    "        print(\"Node decodings:\", node_decs.size())\n",
    "        \n",
    "        node_decs = node_decs.tile((16, 1, 1))\n",
    "        print(\"Tiled node decodings:\", node_decs.size())\n",
    "\n",
    "        # Decode features with transformer decoder\n",
    "        # forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
    "        \n",
    "        # Todo: same embeddings as encoder?\n",
    "        seq = x_seq.view(x_seq.size(0)*x_seq.size(1), x_seq.size(2), -1)\n",
    "        embs = self.embedding(seq)\n",
    "        embs = torch.permute(embs, (1, 0, 2))\n",
    "        pos_encs = self.pos_encoder(embs)\n",
    "\n",
    "        seq_out = self.transf_decoder(pos_encs, node_decs)\n",
    "        print(\"Seq out:\", seq_out.size())\n",
    "        \n",
    "        seq_out = self.lin(seq_out)\n",
    "        print(\"Seq out after lin:\", seq_out.size())\n",
    "        \n",
    "        # Softmax on first 131 values (pitch), sigmoid on last 9 (dur)\n",
    "        #seq_out[:, :, :131] = F.log_softmax(seq_out[:, :, :131], dim=-1)\n",
    "        #seq_out[:, :, 131:] = torch.sigmoid(seq_out[:, :, 131:])\n",
    "        seq_out = torch.permute(seq_out, (1, 0, 2))\n",
    "        seq_out = seq_out.view(x_seq.size())\n",
    "        print(\"Seq out after reshape\", seq_out.size())\n",
    "        \n",
    "\n",
    "        return seq_out, acts_out\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x_seq, x_acts, x_edges):\n",
    "        \n",
    "        mu, log_var = self.encoder(x_seq, x_acts, x_edges)\n",
    "        print(\"Mu:\", mu.size())\n",
    "        print(\"log_var:\", log_var.size())\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        sigma = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(sigma)\n",
    "        print(\"eps:\", eps.size())\n",
    "        z = mu + eps*sigma\n",
    "        \n",
    "        out = self.decoder(z, x_seq, x_acts, x_edges)\n",
    "        \n",
    "        return out, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYr5VVlk8OtQ",
    "outputId": "a3132dad-2ab7-46c2-e2a0-2146fd34c2f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0 0\n",
      "2 0 0\n",
      "2 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([[[[ 5.8178e-01, -8.1274e-01, -6.0987e-01,  ..., -4.2947e-01,\n",
       "             -2.6958e-01, -1.0131e-01],\n",
       "            [ 6.2090e-02, -8.7876e-01, -3.2557e-01,  ..., -6.1638e-01,\n",
       "              1.3293e-01, -2.9136e-01],\n",
       "            [ 3.5527e-01, -5.4562e-01, -4.7331e-01,  ..., -8.0257e-01,\n",
       "              9.8798e-02, -1.8894e-01],\n",
       "            ...,\n",
       "            [-1.6079e-01, -2.7815e-01, -4.7734e-01,  ..., -5.3943e-01,\n",
       "              5.1004e-01, -4.2630e-01],\n",
       "            [-1.9691e-01, -5.1504e-01, -4.0255e-01,  ..., -5.9239e-01,\n",
       "              6.1500e-01, -4.5866e-01],\n",
       "            [-1.1322e-03, -3.1152e-01, -4.9656e-01,  ..., -3.5086e-01,\n",
       "              6.2072e-01, -6.0967e-01]],\n",
       "  \n",
       "           [[-6.9046e-02,  7.6266e-01, -3.3946e-01,  ...,  2.9051e-01,\n",
       "             -7.7294e-01,  8.1180e-02],\n",
       "            [-2.1154e-01,  3.9877e-01, -7.1932e-01,  ..., -2.6378e-03,\n",
       "             -1.1565e+00,  9.4193e-02],\n",
       "            [-3.5359e-01,  5.7683e-01, -6.1277e-01,  ...,  3.8320e-01,\n",
       "             -8.2399e-01,  3.3788e-01],\n",
       "            ...,\n",
       "            [-2.7462e-01,  1.8083e-01, -2.8693e-01,  ...,  2.1770e-01,\n",
       "             -2.9475e-01, -3.0708e-03],\n",
       "            [-4.9999e-01,  2.4617e-01, -8.0431e-01,  ...,  1.1199e-01,\n",
       "              1.7094e-01, -8.7328e-02],\n",
       "            [-2.4058e-01,  1.6843e-01, -3.7020e-01,  ...,  1.4087e-01,\n",
       "              6.7829e-02, -4.9300e-01]],\n",
       "  \n",
       "           [[ 2.3882e-01, -2.6851e-01, -9.6948e-01,  ...,  4.3898e-01,\n",
       "             -6.3011e-01,  4.8039e-02],\n",
       "            [ 2.1421e-01,  7.7301e-02, -5.6477e-01,  ...,  3.3940e-01,\n",
       "              7.4019e-02,  2.3356e-01],\n",
       "            [-3.0060e-01,  2.6933e-01, -1.1169e+00,  ...,  2.8079e-01,\n",
       "             -2.5322e-01,  9.3581e-02],\n",
       "            ...,\n",
       "            [-7.2141e-02, -1.4576e-01, -8.9954e-01,  ...,  4.9058e-01,\n",
       "              2.5124e-01,  1.3883e-01],\n",
       "            [-4.0652e-01, -2.4570e-01, -8.8556e-01,  ...,  1.6388e-01,\n",
       "              6.7454e-01,  5.9647e-02],\n",
       "            [-3.3634e-01,  2.8394e-01, -1.1964e+00,  ...,  3.5108e-01,\n",
       "              3.0663e-01, -2.5360e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 7.0402e-01, -2.2258e-01, -4.5764e-01,  ..., -3.5740e-01,\n",
       "             -4.5961e-01,  1.2004e-01],\n",
       "            [ 7.8225e-01, -1.2040e-01, -7.6829e-01,  ..., -4.5885e-01,\n",
       "             -6.0237e-01,  2.6846e-01],\n",
       "            [ 3.7728e-01,  2.0702e-01, -6.6449e-01,  ..., -2.9382e-01,\n",
       "             -3.2150e-01,  4.2443e-02],\n",
       "            ...,\n",
       "            [ 5.4634e-01,  2.7223e-01, -9.7473e-01,  ..., -4.0296e-01,\n",
       "             -2.7166e-01, -4.7217e-02],\n",
       "            [ 2.3241e-01, -1.1354e-01, -5.5180e-01,  ..., -2.3531e-01,\n",
       "              2.8094e-01, -4.4155e-01],\n",
       "            [ 3.3880e-01, -2.4588e-02, -8.4650e-01,  ..., -5.6448e-01,\n",
       "              2.3322e-01, -4.2599e-01]],\n",
       "  \n",
       "           [[ 4.1106e-01,  9.6489e-01, -4.0652e-01,  ...,  7.0671e-01,\n",
       "             -2.2186e-01, -5.5857e-01],\n",
       "            [-3.2772e-03,  1.0809e+00, -9.1753e-01,  ...,  3.9897e-01,\n",
       "              2.4641e-01, -8.0662e-01],\n",
       "            [ 1.0411e-01,  1.1780e+00, -8.4884e-01,  ...,  2.2413e-01,\n",
       "              3.8315e-01, -5.9052e-01],\n",
       "            ...,\n",
       "            [ 1.5779e-01,  8.1932e-01, -5.6339e-01,  ...,  6.9202e-01,\n",
       "              9.5454e-01, -7.4957e-01],\n",
       "            [ 1.4108e-01,  9.5722e-01, -8.7151e-01,  ...,  4.6209e-01,\n",
       "              6.4201e-01, -6.8401e-01],\n",
       "            [-1.6589e-01,  1.3663e+00, -8.6805e-01,  ...,  3.1852e-01,\n",
       "              5.2058e-01, -6.5621e-01]],\n",
       "  \n",
       "           [[ 2.6975e-01,  3.5386e-01, -1.2102e+00,  ...,  5.1227e-01,\n",
       "             -3.9065e-01, -1.0060e-01],\n",
       "            [-6.0963e-02,  7.2211e-01, -1.2211e+00,  ...,  7.8515e-01,\n",
       "             -3.3146e-01, -4.4469e-01],\n",
       "            [ 2.0363e-01,  6.3569e-01, -1.1389e+00,  ...,  2.3031e-01,\n",
       "             -1.1234e-01,  1.5896e-01],\n",
       "            ...,\n",
       "            [-2.5766e-01,  6.2883e-03, -1.0243e+00,  ...,  2.8268e-01,\n",
       "              3.8650e-01, -2.8542e-01],\n",
       "            [-2.4702e-01,  2.5770e-01, -8.5902e-01,  ...,  6.0156e-01,\n",
       "              2.4819e-01, -2.1497e-01],\n",
       "            [ 1.1046e-01,  4.6967e-01, -9.8364e-01,  ...,  5.0272e-01,\n",
       "              4.5111e-01, -6.8737e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[ 7.3990e-01,  5.0885e-01, -7.3085e-01,  ...,  1.3630e-02,\n",
       "              3.4443e-02, -6.4684e-01],\n",
       "            [ 3.8050e-01,  4.8328e-01, -9.9915e-01,  ..., -2.7060e-01,\n",
       "              6.6570e-02, -5.6956e-01],\n",
       "            [-3.0812e-01,  5.4460e-01, -3.0514e-01,  ..., -6.9653e-02,\n",
       "              1.5881e-01, -1.2316e-01],\n",
       "            ...,\n",
       "            [ 2.3756e-02,  5.7148e-01, -6.7354e-01,  ...,  1.5779e-01,\n",
       "              5.7204e-01, -3.4706e-01],\n",
       "            [ 2.4087e-01,  3.8628e-01, -8.4298e-01,  ..., -2.2022e-01,\n",
       "              9.9137e-01, -2.9266e-01],\n",
       "            [ 6.5321e-02,  5.7096e-01, -5.3919e-01,  ..., -1.2025e-01,\n",
       "              8.0614e-01, -3.3089e-01]],\n",
       "  \n",
       "           [[ 4.7262e-01,  5.0284e-01, -7.7200e-01,  ...,  2.6905e-01,\n",
       "             -3.0086e-01, -2.1397e-01],\n",
       "            [ 1.6958e-01,  4.3461e-01, -4.2111e-01,  ...,  2.5085e-01,\n",
       "             -6.9600e-02, -2.2549e-01],\n",
       "            [ 3.6638e-02,  4.9736e-01, -8.8056e-01,  ...,  2.5983e-01,\n",
       "             -1.1623e-01, -3.0218e-01],\n",
       "            ...,\n",
       "            [ 1.4985e-01,  3.5919e-01, -3.0107e-01,  ...,  3.1767e-01,\n",
       "              4.9847e-01, -3.9427e-01],\n",
       "            [-3.5935e-02,  3.0104e-01, -5.3956e-01,  ...,  3.3839e-01,\n",
       "              1.8946e-01, -4.0502e-01],\n",
       "            [ 2.7088e-01,  8.0084e-01, -1.1382e+00,  ...,  2.1874e-01,\n",
       "              6.2858e-01, -2.7578e-01]],\n",
       "  \n",
       "           [[ 5.0103e-01, -4.5843e-01, -3.2732e-01,  ...,  1.9973e-01,\n",
       "             -3.4336e-01,  2.0100e-01],\n",
       "            [ 3.0585e-01, -3.9070e-01, -4.0782e-01,  ..., -1.8644e-01,\n",
       "             -1.7927e-02,  4.0846e-01],\n",
       "            [-5.6811e-02, -2.0877e-01, -4.3326e-01,  ...,  8.3260e-02,\n",
       "             -3.4366e-01,  2.4817e-01],\n",
       "            ...,\n",
       "            [-1.6797e-02, -4.3630e-01, -3.2358e-01,  ...,  2.4141e-01,\n",
       "              2.1025e-01,  1.5286e-01],\n",
       "            [-1.5103e-01, -5.4890e-01, -5.3871e-01,  ...,  2.1742e-02,\n",
       "              2.6133e-01, -1.4910e-01],\n",
       "            [ 2.0257e-01, -2.1050e-01, -5.5312e-01,  ...,  3.4823e-01,\n",
       "              5.0162e-01, -1.4754e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 6.9418e-01,  2.0136e-01, -2.2772e-01,  ...,  9.4013e-01,\n",
       "             -3.7021e-01, -2.5461e-01],\n",
       "            [ 5.9474e-02,  1.7683e-01, -4.1793e-01,  ...,  5.4446e-01,\n",
       "             -4.7012e-01, -1.8229e-02],\n",
       "            [-3.1700e-02,  3.8729e-01, -6.3842e-01,  ...,  6.8787e-01,\n",
       "             -4.7940e-01, -9.3251e-05],\n",
       "            ...,\n",
       "            [-1.7357e-01,  2.9628e-01, -1.8163e-01,  ...,  2.9996e-01,\n",
       "              1.4995e-01, -1.4497e-01],\n",
       "            [ 4.1357e-01,  4.3104e-01, -7.5177e-01,  ...,  1.5915e-01,\n",
       "              3.4407e-02, -4.7089e-01],\n",
       "            [-3.8734e-02,  2.5418e-01, -6.5848e-01,  ...,  5.0230e-01,\n",
       "              1.8700e-01, -4.3541e-01]],\n",
       "  \n",
       "           [[ 2.6146e-01, -2.1936e-01, -6.0393e-01,  ...,  6.1642e-01,\n",
       "             -2.4183e-01, -2.9633e-01],\n",
       "            [-5.8563e-02,  1.2251e-01, -7.6206e-01,  ..., -3.1248e-01,\n",
       "             -7.2700e-01, -5.5923e-01],\n",
       "            [-2.2934e-01,  1.8455e-01, -9.4190e-01,  ...,  4.3889e-01,\n",
       "             -3.8613e-01, -4.2415e-01],\n",
       "            ...,\n",
       "            [-1.8815e-01,  2.9836e-01, -4.7509e-01,  ...,  2.7193e-01,\n",
       "              1.1069e-01, -4.3313e-01],\n",
       "            [-2.7796e-01, -2.7943e-01, -7.3372e-01,  ...,  5.0064e-01,\n",
       "              1.1155e-01, -7.3684e-01],\n",
       "            [-1.6078e-01,  3.6234e-01, -7.4364e-01,  ...,  3.4108e-01,\n",
       "             -3.0598e-01, -3.1170e-01]],\n",
       "  \n",
       "           [[ 6.4182e-01,  5.4956e-01, -7.4502e-01,  ...,  3.9986e-01,\n",
       "             -4.9301e-01,  4.4653e-02],\n",
       "            [ 4.9805e-01,  7.1651e-01, -7.1464e-01,  ...,  4.8418e-01,\n",
       "             -2.6045e-01,  7.5274e-01],\n",
       "            [-3.7736e-01,  9.8271e-01, -3.9664e-01,  ...,  2.8331e-01,\n",
       "             -5.7273e-01,  5.3152e-01],\n",
       "            ...,\n",
       "            [ 1.5222e-01,  2.8615e-01, -7.4011e-01,  ...,  4.3433e-01,\n",
       "             -5.4469e-02,  5.4618e-01],\n",
       "            [ 2.3740e-01,  5.7265e-01, -8.9839e-01,  ...,  2.7835e-01,\n",
       "              1.9199e-01,  1.3966e-01],\n",
       "            [ 2.5534e-02,  6.0584e-01, -7.4537e-01,  ...,  5.9182e-01,\n",
       "              1.0099e-01,  2.5606e-01]]],\n",
       "  \n",
       "  \n",
       "          [[[ 5.0996e-03,  4.4728e-01, -1.7875e-01,  ...,  2.7478e-01,\n",
       "              1.1935e-01,  2.4755e-01],\n",
       "            [ 2.8008e-01,  7.5198e-02, -2.1762e-01,  ...,  3.5773e-01,\n",
       "              3.9227e-01,  2.9542e-01],\n",
       "            [-5.2561e-01,  6.3803e-01, -1.1395e+00,  ...,  2.6642e-01,\n",
       "             -2.5566e-01, -5.6213e-02],\n",
       "            ...,\n",
       "            [-4.1298e-01,  4.3495e-01, -6.9380e-01,  ...,  4.2981e-01,\n",
       "              8.1471e-01, -1.0459e-01],\n",
       "            [-8.4633e-02,  2.3937e-01, -9.9934e-01,  ..., -1.0780e-01,\n",
       "              5.1892e-01, -1.9318e-01],\n",
       "            [-1.3548e-01,  6.6135e-01, -6.9843e-01,  ...,  2.0718e-01,\n",
       "              4.7309e-01,  4.1949e-02]],\n",
       "  \n",
       "           [[ 4.6662e-02,  3.0205e-01, -9.3431e-01,  ...,  6.8664e-01,\n",
       "             -8.9993e-01,  2.9674e-01],\n",
       "            [-5.4767e-01,  3.5648e-01, -9.0635e-01,  ...,  6.9930e-01,\n",
       "             -8.6495e-01, -3.8262e-01],\n",
       "            [-1.4030e-01,  8.6387e-01, -8.8275e-01,  ...,  6.2948e-01,\n",
       "             -8.3499e-01,  9.6958e-02],\n",
       "            ...,\n",
       "            [-7.2267e-01,  4.2105e-01, -7.6079e-01,  ...,  4.1845e-01,\n",
       "             -1.5094e-01, -1.7873e-01],\n",
       "            [-3.0438e-01,  1.9255e-01, -7.6831e-01,  ...,  4.0332e-01,\n",
       "             -7.4070e-02, -2.7107e-01],\n",
       "            [-5.9957e-01,  5.2629e-01, -8.9888e-01,  ...,  8.5803e-01,\n",
       "              7.4399e-02, -2.3382e-01]],\n",
       "  \n",
       "           [[ 3.0691e-01,  2.0247e-01,  1.9854e-01,  ...,  3.0967e-01,\n",
       "              3.0223e-01,  1.7914e-01],\n",
       "            [-4.2168e-01,  8.6331e-01, -4.4607e-01,  ...,  6.8216e-01,\n",
       "              1.2214e-01,  2.8521e-01],\n",
       "            [-8.7892e-01,  7.9381e-01, -1.2480e-01,  ...,  2.9400e-01,\n",
       "              4.8381e-01,  3.9857e-01],\n",
       "            ...,\n",
       "            [-2.7930e-01,  5.3052e-01, -8.5308e-02,  ...,  1.4124e-01,\n",
       "              7.7940e-01,  1.1641e-02],\n",
       "            [-2.6588e-01,  7.2132e-01, -6.9257e-02,  ...,  1.0222e-01,\n",
       "              1.0066e+00, -5.9449e-02],\n",
       "            [-2.4812e-01,  5.0699e-01,  1.4462e-01,  ...,  3.0230e-01,\n",
       "              8.3567e-01,  1.9750e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 1.5151e-01,  4.6210e-01,  3.8379e-02,  ...,  1.9666e-01,\n",
       "              2.0677e-01,  2.6065e-01],\n",
       "            [ 8.9978e-02,  2.3674e-01, -3.7762e-02,  ..., -5.4943e-02,\n",
       "             -6.9398e-02,  2.6738e-01],\n",
       "            [-3.9673e-01,  5.8664e-01, -2.6012e-01,  ..., -3.3552e-01,\n",
       "              1.8056e-01,  5.5016e-01],\n",
       "            ...,\n",
       "            [-1.4422e-01,  3.2417e-01, -3.2322e-01,  ...,  4.4056e-01,\n",
       "              7.4292e-01,  4.3997e-02],\n",
       "            [-5.2555e-01,  4.1583e-01,  4.9211e-02,  ..., -2.9038e-01,\n",
       "              8.6413e-01, -2.0468e-01],\n",
       "            [-5.1757e-02,  4.9525e-01, -2.0127e-01,  ..., -3.9170e-03,\n",
       "              9.9479e-01,  6.5006e-02]],\n",
       "  \n",
       "           [[ 7.6819e-01, -3.9659e-01, -8.7082e-01,  ...,  2.7745e-01,\n",
       "             -7.9897e-01,  8.3531e-03],\n",
       "            [ 7.2121e-01, -3.5862e-01, -4.8930e-01,  ...,  1.0265e-01,\n",
       "             -3.6038e-01, -6.7638e-02],\n",
       "            [ 2.6672e-01, -2.9253e-01, -8.5388e-01,  ..., -7.9804e-02,\n",
       "             -7.6798e-01,  4.8888e-01],\n",
       "            ...,\n",
       "            [ 2.6796e-01, -5.1564e-01, -8.3284e-01,  ...,  3.8040e-02,\n",
       "             -9.3102e-03, -1.1046e-01],\n",
       "            [ 5.7676e-01, -2.2151e-02, -6.8141e-01,  ...,  5.6339e-01,\n",
       "              1.3877e-01, -6.6265e-02],\n",
       "            [ 4.3100e-02, -6.6265e-02, -7.2782e-01,  ...,  5.8514e-01,\n",
       "              1.8692e-01, -2.0135e-01]],\n",
       "  \n",
       "           [[ 1.3790e-01,  9.4833e-02, -1.6099e-01,  ..., -2.4025e-01,\n",
       "             -2.6163e-01,  4.4224e-01],\n",
       "            [-2.7047e-01, -2.4653e-02, -3.7312e-01,  ...,  9.3477e-02,\n",
       "             -1.8199e-01,  3.9544e-01],\n",
       "            [-4.9465e-01,  4.6174e-01, -7.3378e-01,  ..., -3.2117e-01,\n",
       "             -2.4314e-01,  6.7894e-01],\n",
       "            ...,\n",
       "            [-4.4590e-01,  8.3305e-02, -7.2375e-01,  ..., -2.5953e-01,\n",
       "              7.2795e-01,  6.0034e-02],\n",
       "            [-6.2428e-01,  1.4679e-01, -3.5993e-01,  ..., -5.4790e-01,\n",
       "              4.1937e-01,  2.3766e-01],\n",
       "            [-8.3134e-01, -3.0018e-01, -4.5811e-01,  ..., -1.4658e-01,\n",
       "              4.0128e-01,  1.4996e-01]]],\n",
       "  \n",
       "  \n",
       "          [[[ 5.0835e-01,  2.4045e-01, -1.3637e-01,  ...,  7.2699e-01,\n",
       "              2.7498e-01, -3.7126e-02],\n",
       "            [ 1.7464e-01,  2.5855e-01, -3.2468e-01,  ...,  7.5971e-01,\n",
       "             -2.1250e-01,  6.3259e-02],\n",
       "            [-1.9527e-01,  4.4463e-01, -3.9048e-01,  ...,  3.3756e-01,\n",
       "              4.3356e-01, -1.5712e-01],\n",
       "            ...,\n",
       "            [-6.2963e-02,  4.5958e-01,  1.8201e-02,  ...,  8.3837e-01,\n",
       "              5.6724e-01, -2.8178e-01],\n",
       "            [-2.1321e-01,  4.2845e-01, -2.1097e-02,  ...,  1.8644e-01,\n",
       "              3.8655e-01, -5.2320e-01],\n",
       "            [-6.1123e-01,  2.5441e-01, -1.0610e-01,  ...,  2.4730e-01,\n",
       "              3.8604e-01, -3.5090e-01]],\n",
       "  \n",
       "           [[ 5.7681e-01,  2.9612e-01, -5.6699e-01,  ...,  4.0428e-01,\n",
       "             -6.0324e-01, -1.2772e-01],\n",
       "            [ 8.1981e-01,  4.8191e-01, -1.0107e+00,  ...,  7.9544e-01,\n",
       "             -7.0718e-01, -5.6410e-01],\n",
       "            [ 2.1117e-01,  5.9722e-01, -1.2052e+00,  ...,  4.3103e-01,\n",
       "             -2.3457e-01, -1.0373e-01],\n",
       "            ...,\n",
       "            [-3.1202e-01,  5.1414e-01, -8.3877e-01,  ...,  4.8422e-02,\n",
       "             -1.2628e-01, -4.1566e-01],\n",
       "            [ 5.6770e-02,  6.6689e-01, -9.4818e-01,  ...,  4.2519e-01,\n",
       "              9.5251e-02, -4.8801e-01],\n",
       "            [-5.5356e-02,  6.4428e-01, -9.8645e-01,  ...,  6.8595e-01,\n",
       "              1.7600e-01, -3.7852e-01]],\n",
       "  \n",
       "           [[ 4.3760e-01, -1.8324e-01, -6.2142e-03,  ..., -1.6988e-01,\n",
       "             -6.5711e-01,  5.0402e-01],\n",
       "            [ 1.4575e-01,  2.5439e-01, -5.8092e-01,  ...,  1.3423e-01,\n",
       "             -5.1195e-01,  1.0609e-01],\n",
       "            [ 2.7240e-02,  2.3773e-01, -2.8204e-01,  ..., -1.9898e-01,\n",
       "             -1.0059e-01,  5.5120e-01],\n",
       "            ...,\n",
       "            [-2.7605e-01, -3.1621e-01, -2.2287e-01,  ..., -9.6012e-02,\n",
       "              1.7556e-01, -8.7051e-02],\n",
       "            [ 1.7968e-01,  1.1353e-01, -5.0063e-01,  ...,  2.0008e-01,\n",
       "             -7.9592e-02,  2.0113e-01],\n",
       "            [-2.0165e-01, -8.2497e-02, -3.0194e-01,  ...,  2.0301e-01,\n",
       "              2.8418e-01, -3.0039e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 4.9169e-01,  1.7614e-01, -5.7150e-01,  ...,  1.9641e-01,\n",
       "             -3.0816e-01, -4.0356e-01],\n",
       "            [ 4.2838e-01,  3.4230e-01, -6.7518e-01,  ...,  2.5153e-01,\n",
       "             -4.4347e-01,  1.5008e-01],\n",
       "            [-1.3132e-01,  3.3337e-01, -9.9002e-01,  ...,  2.2062e-01,\n",
       "             -2.1829e-01,  9.3056e-02],\n",
       "            ...,\n",
       "            [ 7.5087e-02,  2.1281e-01, -5.8845e-01,  ...,  2.7877e-01,\n",
       "              8.4742e-01, -2.3127e-01],\n",
       "            [ 1.4231e-01,  2.1770e-01, -4.1678e-01,  ...,  2.0235e-01,\n",
       "              3.3983e-01, -4.2097e-01],\n",
       "            [-7.5259e-02,  5.9907e-01, -4.9100e-01,  ...,  3.1836e-01,\n",
       "              4.4390e-01, -1.5313e-01]],\n",
       "  \n",
       "           [[ 7.3395e-01, -1.4586e-01,  1.1255e-01,  ...,  4.3247e-01,\n",
       "             -4.5165e-01,  8.6930e-02],\n",
       "            [ 6.2501e-01,  4.5925e-01, -1.1908e-01,  ...,  1.3441e-01,\n",
       "             -3.0151e-01, -3.5419e-01],\n",
       "            [ 1.0931e-01,  2.2937e-01, -4.8969e-01,  ...,  4.1241e-01,\n",
       "             -5.3137e-01,  8.8778e-02],\n",
       "            ...,\n",
       "            [ 2.8723e-01,  2.4464e-01, -2.1139e-01,  ...,  1.8401e-01,\n",
       "              3.1389e-01,  5.5907e-02],\n",
       "            [ 3.3152e-01,  6.8766e-01, -2.7887e-01,  ...,  2.3040e-01,\n",
       "              3.1034e-01, -3.8691e-01],\n",
       "            [ 1.9683e-01,  5.2623e-01, -3.6676e-01,  ...,  3.6636e-01,\n",
       "              2.4845e-01, -1.0116e-01]],\n",
       "  \n",
       "           [[-1.8897e-01,  7.3131e-01, -1.9905e-01,  ...,  6.0052e-01,\n",
       "             -4.3170e-01,  2.0850e-02],\n",
       "            [-5.3633e-01,  1.9108e-01, -1.8913e-01,  ...,  2.5866e-01,\n",
       "             -3.4665e-01,  4.6080e-01],\n",
       "            [-5.7898e-01,  3.5354e-01, -3.1520e-01,  ...,  1.7100e-01,\n",
       "             -6.8827e-02,  3.3814e-01],\n",
       "            ...,\n",
       "            [-5.1024e-01,  5.5498e-01, -6.7248e-01,  ...,  1.8003e-01,\n",
       "              5.0224e-01,  3.3788e-01],\n",
       "            [-3.0035e-01,  4.8014e-01, -2.0439e-01,  ...,  1.4715e-01,\n",
       "              1.1224e-01,  3.9163e-02],\n",
       "            [-5.5085e-01,  1.2278e-01, -6.2317e-01,  ..., -7.8368e-02,\n",
       "             -1.3260e-02,  1.3766e-01]]]], grad_fn=<ViewBackward0>),\n",
       "  tensor([[ 0.7074, -1.0842, -0.2849, -0.6762,  0.2706, -0.8602,  0.9903,  0.5509,\n",
       "           -0.4617, -1.5335, -0.6854,  0.1323,  0.2295,  0.1889,  0.2887, -0.0599,\n",
       "           -0.9114, -0.8160,  0.7814, -0.5278,  1.1897,  0.2983, -0.3183, -0.1584,\n",
       "            0.2311, -0.6965,  0.3085, -1.1432, -0.2153,  0.7249, -0.5398,  0.0439],\n",
       "          [-0.0644,  0.1699, -0.1282,  0.3689,  1.2545,  0.3864,  0.7751, -0.3501,\n",
       "           -0.4132, -0.7469, -0.8060,  0.0622,  0.7124,  0.3816,  0.1023, -0.3598,\n",
       "            0.3401,  0.1608, -0.4222, -1.0859,  0.8943, -1.0139, -0.3648, -0.0398,\n",
       "           -0.0574, -0.4522, -0.4240, -0.9946,  0.2048,  0.0526,  0.3205, -0.4355],\n",
       "          [ 1.1608, -0.5255, -1.1017, -0.4188, -0.8317,  0.3010,  0.3099,  0.5599,\n",
       "           -0.4096, -0.0527, -1.0429, -0.3765, -0.0775, -0.0725, -0.2246, -0.3993,\n",
       "           -0.1920,  0.4669,  0.0609,  0.3700, -0.7667,  0.5572,  0.0671,  0.7896,\n",
       "            0.6946, -0.4934,  0.3931,  0.8661, -1.1221, -0.1904,  0.5890,  0.1050],\n",
       "          [ 0.9441, -0.3560, -0.2887,  0.8512, -0.5047, -0.2174, -0.0336, -0.4191,\n",
       "            0.0181, -0.4443,  0.0467,  1.0250,  0.3169, -0.3840, -0.1203, -0.3405,\n",
       "           -0.0716, -0.8222,  0.0556,  0.9823,  0.9433, -0.0780, -0.6744, -0.6497,\n",
       "            0.2613, -0.9507, -0.0826,  0.5788, -0.4363, -0.4960, -0.3043, -0.3256]],\n",
       "         grad_fn=<ViewBackward0>)),\n",
       " tensor([-9.2839e-02, -1.1179e-01,  4.3906e-01, -3.4375e-01,  1.5959e-01,\n",
       "          4.2069e-02,  3.4915e-01, -2.4363e-01, -1.4524e-01,  2.4457e-01,\n",
       "          1.6332e-01, -5.1413e-02,  2.2970e-01, -8.1832e-02,  4.6551e-01,\n",
       "          1.0367e-01,  1.8622e-01,  9.2271e-03,  8.8502e-02,  5.3975e-01,\n",
       "         -7.6069e-01,  7.7788e-02,  2.6279e-01,  2.6913e-02,  7.3866e-02,\n",
       "         -6.1747e-01,  3.3944e-01,  2.0610e-02,  2.3188e-01, -6.7242e-02,\n",
       "         -6.1762e-01, -2.7703e-01, -4.8376e-01,  1.7505e-02,  4.1612e-01,\n",
       "          3.6576e-02,  1.2953e-02, -2.4545e-04, -3.0891e-01,  1.1579e-01,\n",
       "         -9.8014e-02, -1.8353e-01,  2.6733e-01,  3.8923e-01,  6.3096e-02,\n",
       "         -1.0164e-01,  1.7164e-01, -3.3321e-01, -5.5656e-02,  8.3253e-02,\n",
       "          1.7286e-01,  1.9056e-01, -9.4842e-02, -5.5308e-02, -5.6834e-02,\n",
       "         -8.4831e-02,  7.3297e-03,  8.9426e-02, -1.2657e-01, -7.7212e-02,\n",
       "         -1.9456e-01,  3.1903e-01, -2.6884e-01,  2.0574e-01,  2.2342e-01,\n",
       "          1.6218e-01,  1.4012e-01, -4.5819e-01, -1.6805e-01, -5.1326e-01,\n",
       "          4.8943e-02,  2.7748e-01, -5.8459e-03, -3.3723e-01, -9.0411e-02,\n",
       "          5.4404e-02, -2.6317e-01,  1.1287e-01, -4.5227e-01,  7.8222e-02,\n",
       "          7.7883e-02, -2.9305e-01,  2.5124e-01,  2.2758e-01,  3.3051e-01,\n",
       "         -7.3506e-01, -4.7596e-02, -6.6399e-02, -1.7018e-01, -5.6495e-02,\n",
       "         -1.8168e-01,  1.3194e-01, -2.6940e-01,  1.4901e-01,  6.3200e-01,\n",
       "         -2.4949e-01,  4.8305e-01,  5.2000e-01,  1.2701e-01,  4.4537e-02,\n",
       "          1.1692e-01, -6.8620e-02, -7.8121e-02, -4.2514e-01, -2.1370e-01,\n",
       "         -4.8996e-01,  4.8126e-01,  2.5525e-01,  1.9123e-01, -1.1854e+00,\n",
       "         -2.6741e-01,  2.4420e-01, -2.1205e-01,  1.9152e-01, -3.6675e-01,\n",
       "          4.7261e-01,  1.2009e-01, -2.0942e-01, -1.9467e-01, -6.4163e-02,\n",
       "         -1.3260e-01, -5.3032e-01, -2.3793e-02, -3.3269e-01, -5.7275e-01,\n",
       "          2.8553e-02,  3.8211e-01,  3.4990e-01, -1.7815e-01, -1.6180e-03,\n",
       "         -1.6658e-01, -6.3297e-02, -2.9405e-01,  4.1135e-01, -1.9559e-02,\n",
       "         -5.2286e-01,  8.7091e-01,  2.5218e-01,  4.8594e-01,  7.0543e-03,\n",
       "         -3.1305e-01,  3.2131e-02, -3.7525e-01, -3.6073e-01,  5.1155e-02,\n",
       "          1.7218e-01, -7.4031e-01, -4.5253e-02,  1.8145e-01,  2.7053e-02,\n",
       "         -5.0580e-01, -5.4675e-01,  3.6133e-02, -1.5761e-01,  5.1562e-01,\n",
       "         -2.9360e-01,  1.4945e-01, -2.9545e-01,  1.3969e-01, -7.0963e-01,\n",
       "          1.9326e-01, -9.5342e-02, -6.0796e-02, -1.1250e-01,  9.1892e-02,\n",
       "          3.1779e-01, -2.8735e-01, -3.5767e-01, -2.8394e-01,  3.9259e-02,\n",
       "          6.2172e-01, -4.6384e-01,  1.3211e-01,  1.4084e-01,  3.3749e-01,\n",
       "         -1.6337e-01, -2.5769e-02, -2.2206e-01, -3.6526e-01, -2.3903e-01,\n",
       "         -5.0700e-02,  3.8567e-01,  4.8592e-01,  1.6843e-01,  1.4724e-01,\n",
       "          2.4526e-02,  1.6880e-01, -1.3507e-01,  3.2656e-01, -2.0285e-01,\n",
       "         -3.5018e-02, -5.3739e-01, -1.5607e-01,  5.6469e-01,  2.7622e-01,\n",
       "         -7.5129e-01, -2.5217e-01,  6.3203e-02, -1.1142e-01, -2.5872e-01,\n",
       "         -2.1477e-01, -6.0563e-01,  5.6629e-01, -7.6243e-02, -5.3406e-01,\n",
       "         -7.4576e-02, -8.7763e-01, -5.9868e-02, -1.8601e-01, -1.3026e-01,\n",
       "          4.7587e-01, -1.2205e-01, -5.1941e-02, -3.9214e-01,  3.7294e-01,\n",
       "         -8.8472e-02, -7.1084e-01, -4.2990e-01, -7.6510e-01, -2.4043e-01,\n",
       "         -7.9166e-02, -1.0380e-01,  3.6577e-01,  1.6336e-01,  1.3022e-01,\n",
       "         -2.7318e-01,  4.7005e-02, -1.2893e-01,  1.0160e-01,  2.3227e-01,\n",
       "         -4.9893e-01, -3.0156e-01,  1.7448e-01,  5.1205e-02, -2.2389e-01,\n",
       "          2.7348e-01,  4.7868e-01,  4.5747e-01,  5.4862e-01, -2.3565e-01,\n",
       "          8.2894e-02, -1.3646e-01, -1.1631e-01,  1.5013e-01,  8.9244e-01,\n",
       "         -3.2278e-01,  4.6949e-01, -1.8745e-01,  3.1522e-01, -1.2300e-01,\n",
       "          3.5951e-01,  7.9796e-02, -1.6932e-01, -5.6852e-01,  7.2160e-02,\n",
       "         -5.0354e-01], grad_fn=<AddBackward0>),\n",
       " tensor([ 0.0981, -0.2849,  0.5270,  0.8591, -0.2083,  0.3267,  0.2834,  0.3039,\n",
       "          0.2282, -0.0613, -0.0798, -0.0436, -0.4505,  0.5000,  0.0999,  0.0867,\n",
       "         -0.4689,  0.2554, -0.3487,  0.5419,  0.3349, -0.5015,  0.2789, -0.0963,\n",
       "          0.5846,  0.4939,  0.4484, -0.2766,  0.0571, -0.1115, -0.0634,  0.3438,\n",
       "         -0.1172, -0.2248,  0.4184, -0.3928, -0.0688, -0.2320,  0.3504, -0.6346,\n",
       "          0.2472,  0.0392, -0.3331, -0.5242, -0.1573, -0.2244, -0.1329,  0.5450,\n",
       "          0.0506, -0.2657, -1.0600,  0.6102, -0.1111, -0.1444, -0.5238, -0.2118,\n",
       "          0.7231, -0.0174,  0.2238,  0.0405, -0.2259,  0.0814,  0.0250,  0.4950,\n",
       "          0.4599, -0.2299,  0.2937, -0.3759, -0.0060, -0.5330,  0.5402, -0.0270,\n",
       "         -0.0182,  0.2006,  0.0659, -0.1326, -0.0413,  0.3344,  0.3669,  0.4293,\n",
       "          0.5652,  0.0680, -0.4042,  0.4850,  0.2670,  0.4866, -0.0616, -0.2924,\n",
       "         -0.2805, -0.1812, -0.5547,  0.4586, -0.0851, -0.0936, -0.3465, -0.0266,\n",
       "          0.0723, -0.0506,  0.6425, -0.3982,  0.4072, -0.3116,  0.1136, -0.0594,\n",
       "          0.5113, -0.3774,  0.1048, -0.1616,  0.2333,  0.9583, -0.0028, -0.0569,\n",
       "          0.3660, -0.3449, -0.6951,  0.3129,  0.1860,  0.3394, -0.4115, -0.5570,\n",
       "          0.0887, -0.0526,  0.2889,  0.2978,  0.4801, -0.1361,  0.1301, -0.7282,\n",
       "          0.4028,  0.2265,  0.1057,  0.3678, -0.1872, -0.2340, -0.2134, -0.5073,\n",
       "          0.2087, -0.0072,  0.4391, -0.0844,  0.3241,  0.1783,  0.1007,  0.1305,\n",
       "         -0.4354,  0.4977, -0.4761, -0.0427, -0.0612,  0.4431, -0.0479,  0.1903,\n",
       "          0.1903, -0.0462,  0.1170,  0.3802,  0.3544,  0.0911,  0.4789, -0.3662,\n",
       "          0.1209,  0.1236,  0.6406,  0.6513, -0.3184, -0.1722,  0.4167, -0.0852,\n",
       "          0.2170,  0.2574,  0.8175, -0.3081,  0.3437, -0.4739, -0.3567,  0.1691,\n",
       "         -0.2883, -0.3266, -0.3386,  0.4639,  0.0086, -0.2598, -0.0619,  0.2131,\n",
       "         -0.4829, -0.6633, -0.4621,  0.2839,  0.4131,  0.3336, -0.3583, -0.8430,\n",
       "          0.2618,  0.1637, -0.2912,  0.0360, -0.2202,  0.1660, -0.2088,  0.0440,\n",
       "          0.4388,  0.0391,  0.1760, -0.0794, -0.3011,  0.4653, -0.8672,  0.5338,\n",
       "         -0.0689,  0.3307,  0.3863, -0.3092, -0.0946, -0.5853, -0.4585,  0.4064,\n",
       "         -0.2978,  0.2201,  0.2894,  0.1191,  0.4175,  0.6753,  0.1920, -0.1364,\n",
       "         -0.5111, -0.4416, -0.1210,  0.0574, -0.3655, -0.4036,  0.2929, -0.4661,\n",
       "         -0.0916,  0.0897,  0.0229,  0.2466,  0.2404,  0.4116, -0.0600, -0.5264,\n",
       "          0.0013,  0.1415, -0.2883, -0.0238, -0.2375,  0.3754,  0.0726,  0.5549,\n",
       "         -0.5723, -0.1627, -0.4379, -0.3646,  0.1167, -0.1201,  0.3877,  0.3042],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE()\n",
    "vae(torch.Tensor(dataset[0][0]), torch.Tensor(dataset[0][1]), torch.LongTensor(dataset[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VAETrainer():\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def train(self, model, trainloader):\n",
    "        \n",
    "        ce = nn.CrossEntropyLoss()\n",
    "        bce = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        \n",
    "        # Todo: as parameter\n",
    "        for epoch in range(1):\n",
    "            \n",
    "            losses = []\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(trainloader):\n",
    "                \n",
    "                # Get the inputs\n",
    "                x_seq, x_acts, x_edges = data\n",
    "\n",
    "                # Zero out the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Todo: remove [0] when batched\n",
    "                # Forward pass\n",
    "                out, mu, log_var = model(x_seq.float()[0], x_acts[0], x_edges[0])\n",
    "                seq_rec, acts_rec = out\n",
    "                \n",
    "                # Compute the loss\n",
    "                acts_loss = bce(acts_rec.view(-1), x_acts.view(-1).float())\n",
    "                pitches_loss = ce(seq_rec.reshape(-1, seq_rec.size(-1))[:, :131],\n",
    "                                  x_seq.reshape(-1, x_seq.size(-1))[:, :131])\n",
    "                dur_loss = bce(seq_rec[..., 131:].reshape(-1), \n",
    "                               x_seq[..., 131:].reshape(-1))\n",
    "                kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "                loss = pitches_loss + dur_loss + acts_loss + kld_loss\n",
    "                \n",
    "                # Compute gradients and update weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % 3 == 2:\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 3))\n",
    "                    losses.append(loss.item())\n",
    "                    running_loss = 0.0\n",
    "                    \n",
    "                if i > 100:\n",
    "                    break\n",
    "            \n",
    "            plt.plot(range(1, len(losses)+1), losses)\n",
    "\n",
    "        print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 6060300\n",
      "8 0 9\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 2 2\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,     3] loss: 19.887\n",
      "7 14 6\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "8 0 9\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,     6] loss: 8.729\n",
      "9 24 18\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "5 6 8\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "4 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,     9] loss: 5.424\n",
      "6 20 8\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "7 2 9\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    12] loss: 4.159\n",
      "6 8 7\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "2 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 8 8\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    15] loss: 3.423\n",
      "5 2 6\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 14 7\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 4 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    18] loss: 2.951\n",
      "3 0 3\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 6 6\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 6 6\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    21] loss: 2.588\n",
      "6 8 8\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 8 8\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 6 7\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    24] loss: 2.357\n",
      "3 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "7 8 8\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "5 6 7\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    27] loss: 2.159\n",
      "5 0 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 2 2\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "7 16 12\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    30] loss: 2.030\n",
      "8 12 10\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "5 6 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "7 14 6\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    33] loss: 1.895\n",
      "2 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 0 3\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    36] loss: 1.693\n",
      "6 2 7\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 4 8\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "4 2 4\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    39] loss: 1.614\n",
      "4 2 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "8 0 9\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 8 8\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    42] loss: 1.553\n",
      "4 6 7\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "0 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "7 12 13\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    45] loss: 1.476\n",
      "8 12 10\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "2 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    48] loss: 1.367\n",
      "7 2 9\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "4 2 6\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "5 10 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    51] loss: 1.313\n",
      "6 4 4\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "5 4 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 16 12\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    54] loss: 1.295\n",
      "8 28 14\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 8 4\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "4 4 3\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    57] loss: 1.282\n",
      "7 12 11\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "4 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 8 7\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    60] loss: 1.175\n",
      "3 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 2 3\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 2 7\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    63] loss: 1.155\n",
      "8 0 9\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 6 4\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 6 6\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    66] loss: 1.140\n",
      "2 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 14 8\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 8 8\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    69] loss: 1.131\n",
      "3 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "7 16 13\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 8 4\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    72] loss: 1.071\n",
      "6 4 8\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "2 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "9 24 18\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    75] loss: 1.083\n",
      "6 4 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 8 9\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "4 8 7\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    78] loss: 1.068\n",
      "5 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 2 4\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "7 2 9\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    81] loss: 1.040\n",
      "7 26 10\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 12 10\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    84] loss: 1.031\n",
      "4 2 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 0 2\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "5 6 4\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    87] loss: 1.010\n",
      "6 8 6\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "8 16 12\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "6 8 10\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    90] loss: 0.995\n",
      "4 4 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 0 2\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 0 0\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    93] loss: 0.967\n",
      "3 2 3\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "8 0 9\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 8 6\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    96] loss: 0.979\n",
      "4 2 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "3 2 3\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "5 0 5\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,    99] loss: 0.957\n",
      "4 8 7\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "8 0 9\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "7 12 13\n",
      "Init input: torch.Size([4, 32, 16, 140])\n",
      "Reshaped input: torch.Size([128, 16, 140])\n",
      "Embs: torch.Size([128, 16, 256])\n",
      "Seq len first input: torch.Size([16, 128, 256])\n",
      "Pos encodings: torch.Size([16, 128, 256])\n",
      "Transf encodings: torch.Size([16, 128, 256])\n",
      "Pooled encodings: torch.Size([128, 256])\n",
      "torch.Size([4, 32, 256])\n",
      "Node encodings: torch.Size([128, 256])\n",
      "Mu: torch.Size([256])\n",
      "log_var: torch.Size([256])\n",
      "eps: torch.Size([256])\n",
      "Acts out: torch.Size([4, 32])\n",
      "Node features: torch.Size([128, 256])\n",
      "Node decodings: torch.Size([128, 256])\n",
      "Tiled node decodings: torch.Size([16, 128, 256])\n",
      "Seq out: torch.Size([16, 128, 256])\n",
      "Seq out after lin: torch.Size([16, 128, 140])\n",
      "Seq out after reshape torch.Size([4, 32, 16, 140])\n",
      "[1,   102] loss: 0.982\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuIAAAHwCAYAAADjFQoyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAABYlAAAWJQFJUiTwAAA/FUlEQVR4nO3deZzddX3v8ffnbLPPJJnJQhYCZCEQIJAgQljVK6LIVoFyW73UVlu9rVaqrd62trTWXq2tBaUXWzdsacUqRVulgAtI2DUhEEggC1nInpnJ7NtZvveP35mTcyYzyQw553zP8no+HufxO+f7O+fMJz9+j/Ceb76LOecEAAAAoLhCvgsAAAAAqhFBHAAAAPCAIA4AAAB4QBAHAAAAPCCIAwAAAB4QxAEAAAAPCOIAAACABwRxAAAAwAOCOAAAAOABQRwAAADwgCAOAAAAeEAQBwAAADyI+C6gEMxsu6RmSTs8lwIAAIDKdoqkHufcqVP9YEUGcUnNdXV1M84444wZvgsBAABA5dq0aZMGBwff0GcrNYjvOOOMM2asXbvWdx0AAACoYKtWrdK6det2vJHPMkYcAAAA8IAgDgAAAHhAEAcAAAA8IIgDAAAAHhDEAQAAAA8I4gAAAIAHBHEAAADAA4I4AAAA4AFBHAAAAPCAIA4AAAB4kJcgbmY3mtmXzWyNmfWYmTOze6fw+a+lP+PMbHE+agIAAABKWSRP3/OnklZI6pO0W9KyyX7QzK6R9FvpzzbmqR4AAACgpOVraMptkpZKapb04cl+yMxmSvqqpO9IWpunWgAAAICSl5cg7px71Dm3xTnnpvjRf0offzcfdQAAAADlIl9DU6bMzH5D0vWSrnfOdZiZr1IAAACAovMSxM1soaQ7Jd3rnPvBCXzPRMNZJj1GPd+GE0n1DiXU1ljjqwQAAACUgaIHcTMLSfqWgsmZHy32zy+UvV2Descdj6t3KKF50+r05Kfe6rskAAAAlDAfPeK3Sbpc0tXOucMn8kXOuVXjtad7yleeyHdPVUtdVL1DCUlSe9+wnHNiuA0AAAAmUtQNfcxsqaTPSvqmc+7BYv7sQmuoiaguGpYkDSdS6htOeK4IAAAApazYO2ueKalG0vuzNvBxZuYU9JJL0pZ02/VFru2EtTbGMs87+kY8VgIAAIBSV+yhKTskfX2Cc1dLmiPpu5J60u8tK62NNdp9eFBSMDzllLYGzxUBAACgVBU1iDvn1kv6wHjnzOwxBUH8j51zW4tYVt7MzOoRb6dHHAAAAMeQlyCeHkZyffrlnPTxIjO7J/283Tn3iXz8rFLW2nBkycL2vmGPlQAAAKDU5atH/FxJt45pOy39kKSdkio+iLc1MUYcAAAAk5OvLe5vd87ZMR6nTOI7rki/tyyHpUi5PeId/fSIAwAAYGLFXjWlorU1MTQFAAAAk0MQz6O2BiZrAgAAYHII4nnU2kiPOAAAACaHIJ5HbWzoAwAAgEkiiOfRtPqYQhY87x6MaySR8lsQAAAAShZBPI/CIdOMrJVTOvvpFQcAAMD4COJ51pazuybjxAEAADA+gnietRLEAQAAMAkE8Txry1o5hQmbAAAAmAhBPM+yd9ekRxwAAAATIYjnWVtT1hKGTNYEAADABAjiedZGjzgAAAAmgSCeZ9k94mxzDwAAgIkQxPMse4x4Bz3iAAAAmABBPM9YvhAAAACTQRDPs7HLFzrnPFYDAACAUkUQz7PaaFiNNRFJUiLl1D0Y91wRAAAAShFBvAByt7lnwiYAAACORhAvgNZGJmwCAADg2AjiBUCPOAAAAI6HIF4AOT3i/fSIAwAA4GgE8QJoa8jqEe8liAMAAOBoBPECaGvK2ua+n6EpAAAAOBpBvACyd9ekRxwAAADjIYgXQPZkzQ56xAEAADAOgngBsHwhAAAAjocgXgAsXwgAAIDjIYgXQEtdVJGQSZL6hhMaiic9VwQAAIBSQxAvADNTa06vOMNTAAAAkIsgXiBtOePEGZ4CAACAXATxAmF3TQAAABwLQbxAciZs9tIjDgAAgFwE8QLJHprSTo84AAAAxiCIF0hrAz3iAAAAmBhBvEDaGCMOAACAYyCIFwjLFwIAAOBYCOIFwvKFAAAAOBaCeIHkTNYkiAMAAGAMgniBzMiarNnZP6xkynmsBgAAAKWGIF4gsUhILXVRSVLKSV0D9IoDAADgCIJ4AeVO2CSIAwAA4AiCeAHlTthk5RQAAAAcQRAvoOxt7g8RxAEAAJCFIF5ALGEIAACAiRDEC6i1gd01AQAAMD6CeAG1NWVN1uylRxwAAABH5CWIm9mNZvZlM1tjZj1m5szs3gneu8TMPmlmPzOz181sxMwOmNkPzOwt+ainVNAjDgAAgIlE8vQ9fypphaQ+SbslLTvGez8j6VclbZT0oKROSadLulbStWb2+865L+WpLq9yJ2vSIw4AAIAj8hXEb1MQwLdKulzSo8d470OSPu+cez670cwul/RjSV8ws+865/blqTZvWL4QAAAAE8nL0BTn3KPOuS3OuePu4+6cu2dsCE+3/1zSY5Jiklbnoy7fcjf0GdYkLg8AAACqRKlN1oynjwmvVeRJY01ENZHgEg/FUxoYSXquCAAAAKUiX0NTTpiZLZT0NkkDkh6f5GfWTnDqWGPUi8bM1NZYoz1dg5KCtcQbakrmkgMAAMCjkugRN7MaSf8qqUbS7c65w55LyptWdtcEAADAOLx3z5pZWNK/SLpY0nck/e1kP+ucWzXBd66VtDIvBZ4gJmwCAABgPF57xNMh/F5JN0n6d0nvncyEz3LS2pA9YZMlDAEAABDwFsTNLCrp25JukfRvkn7NOVcRkzSztTXRIw4AAICjeRmaYmYxBT3g10n6Z0nvd86lfNRSaNk94h399IgDAAAgUPQe8fTEzAcUhPCvq4JDuCTNzOoRZ7ImAAAARuWlR9zMrpd0ffrlnPTxIjO7J/283Tn3ifTzr0h6l6R2SXsk/ZmZjf3Kx5xzj+WjNt9aGxiaAgAAgKPla2jKuZJuHdN2WvohSTsljQbxU9PHNkl/dozvfCxPtXmVu7smQ1MAAAAQyEsQd87dLun2Sb73inz8zHLB8oUAAAAYT0ls6FPJptdHNTry5vBAXPFkxQ6HBwAAwBQQxAssEg5pRv2R4SmHWTkFAAAAIogXBePEAQAAMBZBvAiyx4m3M04cAAAAIogXRWv2hM1+gjgAAAAI4kWRvbtmey9DUwAAAEAQL4rs3TXb6REHAACACOJFQY84AAAAxiKIF0EbY8QBAAAwBkG8CLKXL+xg+UIAAACIIF4ULF8IAACAsQjiRTC2R9w557EaAAAAlAKCeBHUxyKqj4UlSSPJlHqGEp4rAgAAgG8E8SLJmbDJ8BQAAICqRxAvkpzhKf1M2AQAAKh2BPEiyZmw2UuPOAAAQLUjiBdJW1aPeDs94gAAAFWPIF4krQ30iAMAAOAIgniRtOWMESeIAwAAVDuCeJG05owRZ2gKAABAtSOIF0nO8oX0iAMAAFQ9gniRtI3ZXRMAAADVjSBeJNk94ofY0AcAAKDqEcSLpKUuqnDIJEm9QwkNJ5KeKwIAAIBPBPEiCYVMMxoYngIAAIAAQbyIciZsEsQBAACqGkG8iHJ212ScOAAAQFUjiBdRdo84QRwAAKC6EcSLqDV7jHg/Q1MAAACqGUG8iNqasnfXpEccAACgmhHEi4gecQAAAIwiiBcRY8QBAAAwiiBeRLlBnB5xAACAakYQL6JWli8EAABAGkG8iLKDeGf/iFIp57EaAAAA+EQQL6KaSFhNtRFJUjLl1D0Y91wRAAAAfCGIF9lMJmwCAABABPGiyx0nzoRNAACAakUQL7LWBnrEAQAAQBAvuramrE19COIAAABViyBeZNk94uyuCQAAUL0I4kXW1sTQFAAAABDEi66tgcmaAAAAIIgXXSvLFwIAAEAE8aJra8yerEmPOAAAQLUiiBcZPeIAAACQ8hTEzexGM/uyma0xsx4zc2Z273E+s9rMHjSzTjMbNLMXzexjZhbOR02lqrk2olg4uOwDI0kNjCQ8VwQAAAAf8tUj/qeSfk/SuZL2HO/NZnadpMclXSbpAUl3SYpJ+ntJ9+WpppJkZjm7azI8BQAAoDrlK4jfJmmppGZJHz7WG82sWdJXJSUlXeGc+y3n3B8qCPFPS7rRzG7JU10lqY3hKQAAAFUvL0HcOfeoc26Lc85N4u03Spop6T7n3C+zvmNIQc+6dJwwX+7oEQcAAICPyZpvTR8fGufc45IGJK02s5pxzleE7N016REHAACoThEPP/P09HHz2BPOuYSZbZe0XNJpkjYd64vMbO0Ep5adUIUF1taU1SPONvcAAABVyUePeEv62D3B+dH2aYUvxY+2rB7xQ730iAMAAFQjHz3ieeOcWzVee7qnfGWRy5k0esQBAADgo0d8tMe7ZYLzo+1dhS/Fj+wx4h2MEQcAAKhKPoL4q+nj0rEnzCwi6VRJCUmvFbOoYmL5QgAAAPgI4j9LH68a59xlkuolPeWcq9iE2sbyhQAAAFXPRxD/nqR2SbeY2fmjjWZWK+mv0i/v9lBX0UxvOBLEOwdGlEimPFYDAAAAH/IyWdPMrpd0ffrlnPTxIjO7J/283Tn3CUlyzvWY2QcVBPLHzOw+SZ2SrlWwtOH3JH0nH3WVqmg4pOn1UR0eiMs56fBAXDObKnbZdAAAAIwjX6umnCvp1jFtp6UfkrRT0idGTzjnvm9ml0v6E0nvkVQraaukP5D0pUnu0FnWWhtrdHggLikYJ04QBwAAqC55CeLOudsl3T7Fzzwp6V35+PnlqK0xpq0Hg+eMEwcAAKg+PsaIQ0GP+KiO/oqdlwoAAIAJEMQ9mdnI7poAAADVjCDuSWsDu2sCAABUM4K4J9lDU9rpEQcAAKg6BHFPcjb1oUccAACg6hDEPcmZrMk29wAAAFWHIO5J9mTNdpYvBAAAqDoEcU9as4amtPcNqwr2MAIAAEAWgrgn9bGwaqPB5R9OpNQ3nPBcEQAAAIqJIO6JmaktZ5w4w1MAAACqCUHco5wlDJmwCQAAUFUI4h7NzBknTo84AABANSGIe9TakDU0pZ8ecQAAgGpCEPeorSmrR7yXHnEAAIBqQhD3iB5xAACA6kUQ92jsWuIAAACoHgRxj9hdEwAAoHoRxD1i+UIAAIDqRRD3qC1raAob+gAAAFQXgrhH0+pjClnwvHswrpFEym9BAAAAKBqCuEfhkGlG1sopnf30igMAAFQLgrhnbaycAgAAUJUI4p6xhCEAAEB1Ioh71pa1cgoTNgEAAKoHQdyz7N016REHAACoHgRxz9qaspYwZLImAABA1SCIe9ZGjzgAAEBVIoh7lt0jzjb3AAAA1YMg7ln2GPEOesQBAACqBkHcM5YvBAAAqE4Ecc/GLl/onPNYDQAAAIqFIO5ZbTSsxpqIJCmRcuoZTHiuCAAAAMVAEC8B2dvcH2J4CgAAQFUgiJeA1kYmbAIAAFQbgngJaG1gCUMAAIBqQxAvAW1NWT3i/fSIAwAAVAOCeAloy+4R7yWIAwAAVAOCeAnI7hFv72doCgAAQDUgiJcAdtcEAACoPgTxEtDWyGRNAACAakMQLwEsXwgAAFB9COIlgB5xAACA6kMQLwEtdVFFQiZJ6htOaCie9FwRAAAACo0gXgLMTK05veIMTwEAAKh0BPES0ZYzTpzhKQAAAJWOIF4iciZssrsmAABAxfMaxM3sajN7xMx2m9mgmb1mZt81s4t81uVDzoTNXnrEAQAAKp23IG5mn5f0Q0krJT0k6U5J6yRdJ+lJM3uvr9p8yB6a0k6POAAAQMWL+PihZjZH0ickHZB0jnPuYNa5t0j6maS/lHSvj/p8aG2gRxwAAKCa+OoRX5j+2c9mh3BJcs49KqlX0kwfhfnSxhhxAACAquIriG+RNCLpAjNryz5hZpdJapL0Ex+F+ZK9fCGrpgAAAFQ+L0NTnHOdZvZJSV+UtNHMvi+pQ9IiSddK+rGk3zne95jZ2glOLctTqUWTM0acdcQBAAAqnpcgLknOuTvMbIekb0j6YNaprZLuGTtkpdLlBnF6xAEAACqdz1VT/kjS9yTdo6AnvEHSKkmvSfpXM/ub432Hc27VeA9JrxSw9IKYkTVZs7N/WMmU81gNAAAACs1LEDezKyR9XtJ/Ouf+wDn3mnNuwDm3TtINkvZI+riZneajPh9ikZBa6qKSpJSTugboFQcAAKhkvnrE350+Pjr2hHNuQNJzCmo7r5hF+ZY9YZPhKQAAAJXNVxAfHRA90RKFo+1VlUZzljBkwiYAAEBF8xXE16SPv21m87JPmNk7JV0saUjSU8UuzKecbe77q+p3EAAAgKrja9WU7ylYJ/x/SNpkZg9I2i/pDAXDVkzSp5xzHZ7q8yJn5ZReesQBAAAqma91xFNm9i5JvyvpFgUTNOsldUp6UNKXnHOP+KjNp9YGdtcEAACoFj7XEY9LuiP9gMZM1uxlaAoAAEAl87aOOI6WM1mTHnEAAICKRhAvIdmTNQ+xfCEAAEBFI4iXEJYvBAAAqB4E8RKSPUa8gx5xAACAikYQLyGNNRHVRIL/JIPxpPqHE54rAgAAQKEQxEuImY0ZnkKvOAAAQKUiiJeY1pwJm4wTBwAAqFQE8RLDhE0AAIDqQBAvMa0NWZv6MDQFAACgYhHES0xbEz3iAAAA1YAgXmKye8Q7+ukRBwAAqFQE8RIzM6tHnMmaAAAAlYsgXmJaGxiaAgAAUA0I4iUme/lCJmsCAABULoJ4iWH5QgAAgOpAEC8x0+ujMgueHx6IK5FM+S0IAAAABUEQLzGRcEgz6o8MT+lk5RQAAICKRBAvQYwTBwAAqHwE8RKUvXJKO+PEAQAAKhJBvARl765JEAcAAKhMBPESNH96Xeb5i7u7PVYCAACAQiGIl6DVi1ozz5/Y2u6xEgAAABQKQbwEvemUGYpFgv80Ww/2aV/3oOeKAAAAkG8E8RJUGw3rTadMz7x+Ygu94gAAAJWGIF6iLlk8M/Oc4SkAAACVhyBeoi5d0pZ5/uTWdjnnPFYDAACAfCOIl6gzT2rW9PqopGBTn1f293quCAAAAPlEEC9RoZBp9eIjveKMEwcAAKgsBPESdmlWEF/DOHEAAICKQhAvYZdkjRN/bnuHhhNJj9UAAAAgnwjiJWz+9Hqd2tYgSRqKp7R252HPFQEAACBfCOIl7uLFWbtsMk4cAACgYhDESxzriQMAAFQmgniJu2hRq0IWPN+wp1uH+0f8FgQAAIC8IIiXuJa6qFYsmCZJck56+rUOvwUBAAAgLwjiZeCS7GUMGScOAABQEQjiZSA7iD+x9ZDHSgAAAJAvBPEycN7J01UfC0uSXu8c1M6Ofs8VAQAA4EQRxMtALBLShadlLWPI6ikAAABljyBeJi7OHp7COHEAAICyRxAvE5dmbXf/1LYOJVPOYzUAAAA4UQTxMrFkVqNmN9dIkroH49qwp9tzRQAAADgRBPEyYWY5w1OeZJw4AABAWSOIl5Hc9cRZxhAAAKCcEcTLSHYQX7vzsAZGEh6rAQAAwIkgiJeRWc21On12kyQpnnR6dnun54oAAADwRnkP4mb2NjN7wMz2m9mwme01s4fN7F2+aytFl2StnvIkyxgCAACULa9B3Mz+RtJPJJ0v6T8l/Z2kH0maKekKf5WVrtzt7gniAAAA5Sri6web2Qcl/aGkb0n6befcyJjzUS+Flbg3nzZD0bApnnR6ZX+vDvYOaVZTre+yAAAAMEVeesTNrEbSZyXt0jghXJKcc/GiF1YG6mMRrTx5euY1yxgCAACUJ1894m9XMPzkDkkpM7ta0lmShiQ955x7ejJfYmZrJzi1LB9FlqpLl7RlJmqu2dKuG86b77kiAAAATJWvIP6m9HFI0vMKQniGmT0u6UbnHItlj+OSJTP1t49slhT0iDvnZGaeqwIAAMBU+JqsOSt9/ENJTtKlkpoknSPpEUmXSfru8b7EObdqvIekVwpUd0k4e16LmmuD36EO9Axr68E+zxUBAABgqnwF8dGfm5B0rXPuCedcn3Nug6QbJO2WdLmZXeSpvpIWDplWL8reZZNx4gAAAOXGVxDvSh+fd87tyD7hnBuQ9HD65QVFrKmsZK8nzjKGAAAA5cdXEH81feya4Pzh9LGu8KWUp0uzgvgzr3Uonkx5rAYAAABT5SuI/1TB2PAzzWy8GkYnb24vXknl5eQZ9Zo/Pfg9ZWAkqed3dfktCAAAAFPiJYg753ZK+i9JJ0v6/exzZnalpHco6C1/qOjFlQkzy+kVf2ILC8wAAACUE59b3P+upNclfdHMfmJmXzCz70l6UFJS0gecc90e6yt5lyyemXm+hnHiAAAAZcVbEHfO7Za0StJdkpYo6Bm/QkFP+cXOuft91VYuVi9q1ejy4S+83qWeITYjBQAAKBc+e8TlnDvknPuIc26hcy7mnGtzzt3gnHvOZ13lYnpDTGfNbZEkpZz09LYOzxUBAABgsrwGcZy4nGUMWU8cAACgbBDEy9yli1lPHAAAoBwRxMvcyoXTVRsN/jNub+/X7sMDnisCAADAZBDEy1xtNKw3nTIj8/pJesUBAADKAkG8AmSvJ76GceIAAABlgSBeAbLXE39qW4dSKeexGgAAAEwGQbwCLJvTpLbGmCSps39EG/f1eK4IAAAAx0MQrwChkGn1IlZPAQAAKCcE8QrBeuIAAADlhSBeIbInbD63o1ND8aTHagAAAHA8BPEKcVJLnRbNbJAkjSRS+sWOTs8VAQAA4FgI4hXk0iVHVk9hnDgAAEBpI4hXkIsXM04cAACgXBDEK8iFp81QOGSSpJf39qijb9hzRQAAAJgIQbyCNNVGdd6CaZnXT27r8FcMAAAAjokgXmGylzF8kuEpAAAAJYsgXmEuWZy7sY9zbHcPAABQigjiFWbFgmlqrIlIkvZ0DWp7e7/nigAAADAegniFiYZDuvC01sxrljEEAAAoTQTxCpS9y+YaxokDAACUJIJ4BcpeT/yZbR1KJFMeqwEAAMB4COIVaNHMBp3UUitJ6h1O6IXd3Z4rAgAAwFgE8QpkZrmrpzA8BQAAoOQQxCtU9nriT2w95LESAAAAjIcgXqGyx4k/v6tLfcMJj9UAAABgLIJ4hWprrNEZJzVLkhIpp2dfY7t7AACAUkIQr2AsYwgAAFC6COIVLHvC5n++sFftfcMeqwEAAEA2gngFu+DUGZrTHCxj2Nk/ok/d/6Kcc56rAgAAgEQQr2i10bD+5sZzMq9/sumg7vvF6x4rAgAAwCiCeIW7bOlM/cbqUzKv//K/Nmp7e7+/ggAAACCJIF4VPvXOZVo8q1GSNBhP6rbvrGfbewAAAM8I4lWgNhrWHb96rqJhkyStf71Ldz261XNVAAAA1Y0gXiXOmtei296+NPP6yz/bqud3HfZYEQAAQHUjiFeR37lskS44ZYYkKZly+oN/f0EDI+y4CQAA4ANBvIqEQ6a/u3mFGmsikqTt7f36qx9t8lwVAABAdSKIV5kFM+p1+7XLM6//7dld+ummAx4rAgAAqE4E8Sr0npXz9M6z5mRef/L+F9l1EwAAoMgI4lXIzPTXN5ytWU01kqT2PnbdBAAAKDaCeJWa3hDTF25akXnNrpsAAADFRRCvYpeP2XXzMz/cqB3sugkAAFAUBPEql73r5sBIUh9j100AAICiIIhXudFdNyOhI7tu/sOj2zxXBQAAUPkI4jhq180v/WyL1r/e5a8gAACAKkAQhyTpQ5cv0ptOmS4p2HXztu+sZ9dNAACAAiqZIG5m7zUzl358wHc91SYcMn3x5nPZdRMAAKBISiKIm9kCSXdJ6vNdSzVbMKNef37NmZnX7LoJAABQON6DuJmZpG9K6pD0Fc/lVL0bV81n100AAIAi8B7EJX1U0lslvV8Si1h7Nv6umxvYdRMAACDPvAZxMztD0uck3emce9xnLTji6F03D+g77LoJAACQV96CuJlFJP2LpF2S/vgNfsfa8R6SluWz1mp0+dKZuvWihZnXf8mumwAAAHnls0f8zySdJ+k3nHODHuvABD71zjO0aGaDJHbdBAAAyDcvQdzM3qygF/zvnHNPv9Hvcc6tGu8h6ZW8FVvF6mJh3XnLeey6CQAAUABFD+LpISn/LGmzpE8X++djasbbdfNHL+7zWBEAAEBl8NEj3ihpqaQzJA1lbeLjJP15+j1fTbfd4aE+jDF2182PfHud7l+723NVAAAA5S3i4WcOS/r6BOdWKhg3/oSkVyW94WEryJ9wyPQPv7ZSv/a1Z7X1YJ9STvr4d1/QUCKpX3/zwuN/AQAAAI5S9CCenpg57hb2Zna7giD+Lefc14pZF45tVnOt7vvtC/W+rz+nTft6JEl/8sBLGoqn9FuXnOq5OgAAgPJTChv6oEy0Ndbo2x98s1bMb8m0feaHG3XXz7Z4rAoAAKA8EcQxJdPqY7r3A2/OjBmXpL99ZLO+8PAr7L4JAAAwBSUVxJ1ztzvnjGEppa2pNqpv/eYFumRxW6btHx7dps/8cBNhHAAAYJJKKoijfNTHIvrarefrrctmZdq+8eR2/cn3X1IqRRgHAAA4HoI43rDaaFhfee8qvevsOZm2f3t2lz7xvRfYgRMAAOA4COI4IbFISF+65TzdcN68TNt/rNuj379vveKEcQAAgAkRxHHCIuGQ/u6mFfqfFyzItP1owz59+N61GoonPVYGAABQugjiyItQyPTXN5yt31h9SqbtJ5sO6oP//EsNjhDGAQAAxiKII2/MTH9+zZn631csyrSt2dKuW7/5nPqGEx4rAwAAKD0EceSVmemPrlqmj799aabtue2d+vWvPavugbjHygAAAEoLQRwF8ZG3LdGfXn1G5vULr3fpf371GXX0DXusCgAAoHQQxFEwH7j0NH3m+rMyrzfu69Et//SMDvYMeawKAACgNBDEUVDvu3ChvnDjOQpZ8HrLwT7d/I9Pa0/XoN/CAAAAPCOIo+BuOn+B7rzlPIXTaXxHx4Bu/srT2naoz3NlAAAA/hDEURTXrJiru399pWLh4Jbb0zWoa778hO5fu9tzZQAAAH4QxFE0Vy6fo6/eer5qo8FtNzCS1Me/+4Ju+856ljcEAABVhyCOorp86Uzd/+HVOm1mQ6btgef36OovrdGLu7v8FQYAAFBkBHEU3fK5LfrhRy7RTavmZ9p2dgzoPXc/pa8+/ppSKeexOgAAgOIgiMOL+lhEX7hphe685Vw11kQkSfGk02cf3KT33/MLtbPeOAAAqHAEcXh13bnz9KOPXqIV81sybT/ffEjvvHONntjS7rEyAACAwiKIw7uFrQ367odW63cuOy3Tdqh3WO/7xrP6/EOvKJ5MeawOAACgMAjiKAmxSEj/511n6Fu/eYHaGmOSJOekux/bppv/8Wm93jnguUIAAID8IoijpFy+dKYe/P1LdemStkzb87u69K471+iHL+71WBkAAEB+EcRRcmY11epb779An3rnMkXSu3H2Dif0e//2vD51/4saHEl6rhAAAODEEcRRkkIh04cuX6TvfugiLZhRl2m/7xev65q7ntAr+3s8VgcAAHDiCOIoaeedPF0/+uilumbF3Ezb1oN9uvauJ/UvT++Qc6w5DgAAyhNBHCWvuTaqL91yrv7mPeeoLhqWJI0kUvr0D17Wh+5dq66BEc8VAgAATB1BHGXBzHTzmxbovz5ysZbNacq0P/zyAV11xxr9fPMhj9UBAABMHUEcZWXxrCZ9/3cv1q0XLcy07e8Z0q3feE7/5z82qG844bE6AACAySOIo+zURsP6i+vO0j+9b5VmNMQy7d9+bpeuuuNxPbWNHTkBAEDpI4ijbF25fI4eue0yXbV8TqZt9+FB/dpXn9Xt//myBkboHQcAAKWLII6y1tZYo7vfu1J33nKuWuqimfZ7ntqhd925Rr/c0emxOgAAgIkRxFH2zEzXnTtPj9x2md66bFamfUfHgG76x6f11w9u0lCcTYAAAEBpIYijYsxurtXXbz1ff3PjOWqqiUiSnJP+6fHXdPWX1mj9611+CwQAAMhCEEdFMTPdfP4CPXTbZbp0SVumfduhfr3n7qf0hYdf0XCC3nEAAOAfQRwVad60Ov3zb16gv7r+LNXHgk2Akimnf3h0m66760m9vLfbc4UAAKDaEcRRscxM771woR76/cv05lNnZNpf2d+r6+56Ul/66RbFkymPFQIAgGpGEEfFO7m1Xt/+4IX6s3efqdpocMsnUk5f/PFm/cr/e0qbD/R6rhAAAFQjgjiqQihk+s1LTtWDH71UK0+elmnfsKdb7/7SE/rKz7cpmXL+CgQAAFWHII6qctrMRn33Q6v1qXcuUywc3P4jyZQ+99+v6D13P6X/3rCPpQ4BAEBRRHwXABRbOGT60OWL9NZls/Txf39BG/YEEzfXv96lD//rOjXVRnT12Sfp+vPm6YJTZigUMs8VAwCASkSPOKrW0tlN+o//vVp/8PalimSF7d6hhO77xeu65Z+e0SWf/5k+/9Ar2sI4cgAAkGf0iKOqRcMhffRtS3TduXN1/9rdemD9Hr3eOZg5v7d7SHc/tk13P7ZNy+c264bz5unaFXM1q7nWY9UAAKASmHOVN0HNzNauXLly5dq1a32XgjLjnNO6XYf1wPN79MMX96lrIH7Ue0ImXby4TdefO0/vOGuOGmv4fRYAgGq1atUqrVu3bp1zbtVUP0sQByYwkkjp55sP6fvP79GPNx3QSOLoNcdroyG9Y/kcXX/ePF26uE2RMKO9AACoJicSxOnKAyYQi4T09jNn6+1nzlbPUFwPbdivB57fo2e2d2j099eheEo/WL9XP1i/V22NMb37nLm64bx5Omd+i8yY5AkAACZGEAcmobk2qpvftEA3v2mB9nYN6gfr9+qB53dr84G+zHva+0Z0z1M7dM9TO9TaENPKhdO18uTpWrVwus6Z36LaaNjjnwAAAJQagjgwRXOn1enDVyzShy4/TZv29er76/foB+v36EDPcOY9Hf0j+vHGA/rxxgOSpEjItHxuc044nzutztcfAQAAlACCOPAGmZnOnNusM+c265NXLdMzr3Xogef36McbD6h7MHeSZyLl9MLubr2wu1vffHKHJGlOc61WLZyu806eplULp2v53BbFIowxBwCgWngJ4mbWKukGSVdLOlvSPEkjkjZI+qakbzrnjp4ZB5SocMh08eI2Xby4TamU02vtfVq787DW7jysdbu6tPVg31Gf2d8zpB9t2KcfbdgnKRiTfs68lnQ4n66VC6dpVhPLJAIAUKl89YjfJOluSfskPSppl6TZkn5F0tckvdPMbnKVuKQLKl4oZFo8q0mLZzXpV990siSpa2BEz7/epXU7D2vdrsNav6tL/SPJnM+NJFL65c7D+uXOw5m2U1rrdcmSNl2yeKZWL25Vc220qH8WAABQOL6C+GZJ10r6UXbPt5n9saTnJL1HQSi/3095QH5Nq4/pLafP0ltOnyVJSiRTevVAr9btOhLOd3YMHPW5HR0D2tGxS/c+s0vhkOncBdN06ZI2XbpkplbMb2G5RAAAypiXIO6c+9kE7fvN7CuSPivpChHEUaEi4ZCWz23R8rktet+FCyVJh3qH9fyuw1q767DW7TysF3d3azhr7fJkymWGu9zxky1qqo3o4kVtumRJmy5bMlMnt9b7+uMAAIA3oBQna47Ockt4rQIosplNNbpy+RxduXyOJGk4kdS6nV1as+WQ1mxp10t7u5U9WKt3KKGHXt6vh17eL0la2Fqf6S2/aBHDWAAAKHUlFcTNLCLpf6VfPjSJ90+0deayvBUFeFITCeuiRa26aFGr/ugqqbN/RE9ubc8E833dQznv39kxoJ0MYwEAoGyUVBCX9DlJZ0l60Dn3sO9igFIyoyGma1bM1TUr5so5p22H+vT45iCYP/NapwbjRyZ/jjeM5ex5LTrzpObMkouLZjYqSjgHAMCbkgniZvZRSR+X9Iqk903mM865VRN811pJK/NXHVBazI6szPKbl5yq4URSa3ce1hNb2rVmS7s27OnOeX/vUEJPbevQU9s6Mm2xcEhLZjdmwvkZJwWPljqGtAAAUAwlEcTN7Pck3Slpo6S3Oec6PZcElJWaSFirF7Vp9aI2/dFVUkffsJ7c1qE1m4NhLPt7ho76zEgypZf39ujlvT1S1iCv+dPrdGY6lJ85t1lnntSs+dPrZGZF/BMBAFD5vAdxM/uYpL+X9JKCEH7Qb0VA+WttrNG1K+bq2vQwltc7B7VxX7c27uvVxr092rSvR3u6Bsf97O7Dg9p9eFCPbDyQaWuqjQTB/KRmrVw4XVecPpPJoAAAnCCvQdzMPqlgXPh6SW93zrX7rAeoRGamk1vrdXJrva4666RMe9fAiDbt69XGfT2ZcL7lYK/iyaP30eodSui57Z16bnun7nlqh6Jh00WL2nTlmbN15ZmzNauZHUABAJgqb0HczD4t6S8V/KP4lQxHAYprWn0ssyrLqJFESlsP9mnjviCYb9zbo437etQ9GM/5bDzp9PjmQ3p88yF9+gcv6bwF03Tl8jl6x/I5OrWtodh/FAAAypKXIG5mtyoI4UlJayR9dJzxpzucc/cUuTSgqsUiocyqKqOcc9rXPaSNe3v04p5uPfrKwZzJoM4p2CF0V5c+99+vaOnsRl15ZhDKz5rXzNhyAAAm4KtH/NT0MSzpYxO85+eS7ilGMQAmZmaaO61Oc6fV6X+cOVt/8Pal2tM1qB+/vF8Pv3xAz+3oVDJ1ZDjL5gN92nxgq+56dKvmttSmNymarQtOmcFa5gAAZDHnjh4PWu7MbO3KlStXrl070X4/APLlcP+IfvrKQT388n49vvmQhhOpcd83rT6qty2brXcsn63Lls5UbTRc5EoBAMi/VatWad26desmWlb7WLyvmgKgvE1viOnGVfN146r5GhhJ6PHN7Xrk5f36yaYD6hlKZN7XNRDX/et26/51u1UXDevixa2aN61OLXVRNddF1ZJ+TKuPZZ631EVVGw0xvAUAUJEI4gDypj4W0VVnzdFVZ81RPJnSc9s79fDL+/XIywdy1jIfjCf1k02TW6k0Fg6ppT6aE87HPuZNr9OyOU1aML1eoRChHQBQHgjiAAoiGg7p4sVtunhxm26/Zrk27OnWwy/v18Mv79e2Q/2T/p6RZEqHeod1qHf4uO+ti4a1dHajls5u0ulz0o/ZTZrZVEOvOgCg5BDEARRcKGRasWCaViyYpj+6apm2HuzTi7u71DUQV/fg+I+ugbh6BuMaSY4/5nw8g/GkXtjdrRd2d+e0T6+PaunsJi2b06Sl6XC+dE4TmxIBALwiiAMousWzGrV4VuNx3+ec01A8lRXOR3LCes9gXJ0DI9re3q9X9/eqvW9k3O85PBDXs9s79ez23O0K5rbU6vR0OF82p0mLZjZqen1MzbVRNdZGFGaYCwCggAjiAEqWmakuFlZdLKw5LcffvbO9b1ibD/Tq1f292nygV6/s79Xm/b3qH0mO+/693UPa2z2kR189NO75ppqImuuiaqqNZCaVNtdG1VwXSR+DMerNtZHcc3VRNdVEGA4DADgmgjiAitHWWKO2xhqtXtSWaXPOaU/XoF7d36tX0yH91f292naoT/HksZdv7R1OqHc4ccz3TKS1Iabl81p09rxmnT2vRcvntmj+9DrCOQAggyAOoKKZmeZPr9f86fV62xmzM+3xZEo72vtzwvnrhwfVMxhXz1BcvUNvLICP6ugf0eObD+nxzUd626fXR3VWOpSfPS94LJhBOAeAakUQB1CVouGQlsxu0pLZTXr3OUefT6ac+oYS6hlKj0cfiqtnMJEJ6sExeJ1zfiiuzv6RcTc2OjwQ15ot7VqzpT3T1lwb0VnpUL48fVw4g2UYAaAaEMQBYBzhkAXrl9dHtWCKn02lnHZ09OulvT16aU+3Nuzu1kt7u8ftZe8ZSuipbR16altHpq2pJqIz5wZDWpbMblRDTUR10WCsfH0sovpYOOt1WLWRMMEdAMoQQRwA8iwUMp02s1GnzWzUtSvmSgrGqu/qHNBLe3q0YU93END3dKt7MH7U53uHE+Ou8nIstdGQ6mNBYK9PT3AdfV4fi6g2GlZjTVgNNRE11ETUWBME+sb069G2hpojbdFwKG/XBABwNII4ABSBmWlha4MWtjbo6nNOkhSE892HB/XSnqDHfMOeoAe9s3/8ZRiPZSie0lB86p87llgklAnnDbFITkBPOadkymWOOc9d8K8CuW0uaHNOqVQw9Cdk0qJZjTp7XovOmd+is+dP09yWWsbMA6gaBHEA8MTMtGBGvRbMqNc7zz4Szvd1D2V6zfd2DWkwntDASFIDI0kNxYPj4EhSAyNB+3jj0fNhJJFSZ2JEnZPfCHXK9nYP5YyZb2uMBRNZ50/TOemAPqv5+EtXAkA5IogDQAkxM82dVqe50+r0juVzJvWZVMppMDugxxMazIT1pAbiSQ2OJNQ/nFT/cEJ9Iwn1Dwev+4ZHnyfUP5I+n36dOvbqjgXR3jeiR189lLO2++zmGp09b1q61zyY0NrWWFP84gAgzwjiAFDmQiHLjPPOl9FdTUdDed9w0PveP5zQSDKlsJnCIVMoZAqbKRTSUW3hkCmUPoZDyjwfPQ7Fk9q4r0cbdnfrxd3BvwCMt277gZ5hHeg5oJ9sOpBpmzetLt1zHvSaz51WJ5f5xcGl/wyjz5Q55+SOPB+nLRoOacGMOtXH+N8jgMLjbxoAwFGydzWd2VS43ufTZjbq3ecEE1pHV5vZsCcI5qOrzQyMszPqnq5B7eka1EMv7y9IXfOm1WnJ7EYtntmoxbOOPKbVxwry8wBUJ4I4AKAkZK82c9258yQFkzpfO9QXBPM93Xpxd5de3ttTsHHxo0aD/mNZQ2SkYPfWxbMagmA+s1GLZzVp8axGzW6uOeFJps45DSdSmQ2letPr1MeTKTXXRdWS9aiNhk/oZ01GIpnS4YG4Dg+MqLP/yONw/4gG48nMEpq10SPLadZF069Hl9iMhlUbC2XaWYkHyEUQBwCUrHDIMhsvvWfVfElBQNxysC8Y0rKnSxv29Kh3dBlIO3IYDcaj8dhMsvSr7Myc/b7BeFK7OgeUnGCAfHvfsNr7hvXMa7lLSzbVRLQoq+d88cxG1URD6UAdBOue7OeD6bA9fCR09w7FFU9ObmB+TSSUCeXT6oPjaFifVhdTS11ELfXB89H25rqIBkeSQZgeGFFH30g6ZMfV2T+szv7c0D3e0ponKhKydDg/srxmW2ON5rTUam5Lrea01OmkllrNaanVSS21aqmLsooOKhpBHABQViLhkM44qVlnnNSsm9801e2Wjm8kkdKOjn5tPdiXeWw52KfXDvVN2BPfO5zQ+te7tP71rrzXM57hREoHe4d1sHe4KD8vXxIpp97hxJi5AL0Tvr8uGs4E89FwflJOWK/T9HrCOsoXQRwAgCyxSEhLZzdp6eymnPZkymnP4UFtPdQbhPMDfdp6KAjq4+2a+oZ+djikptpI+hFVU21EsUhIPYNxdWc9JttzfiLMpGl1Uc1oiGlGQ0zT62NqbQyOddGwhhMpDcaTGownNTSSzDwfTC+zeeR1Kr3s5tRX4hmMJ/Vae79ea594Dc2aSEgntdRqekNMNZGQYpGwYuFQ+nlIsXD6GMlqS7fnvg5nXkfDIcUipmg4lHnEwiFFs9pi4ZCi4WDiMb8IlIbhRFKJpMvrxPVCK59KAQDwKBwyndxar5Nb6/XWZbMz7c45Heod1pasHvTX2vuUSiknUDfXRtRcF81pO3IuOE5m7LdzwXKV3YNxdQ1kBfSB3LDelfV8NMjXRcNBqG6IqXVMuJ7RENWMhhrNaIhqen1M0+pjCofyFzCdc4ong9qH0oG9fyShgz3D2tc9pP3dg9rXPZR+BM/Hm6g71nAipR0dA9rRMZC3WqfCTDnBPBPU08G/PhbsaFsXTR9jYTXEwqqLBbvbjj5vSE+Ozn5v9i65IbP0kCtVZPBPpZz6R44M4eoZTA/hGg6e9w7Fc8+Ned07FNdwIqVfWTlPX7z5XN9/nEkjiAMAcALMTLOaazWruVYXL24rys+rj0VUH4vopJa6gv+8fDEzxSKmWHp8+6jlc8d/v3PBMJZ9XUEw358O6fu7h7Q3/Xp/99C4S14Wk3PBcKaRAk8gnkgw9yG4vpmgnjVZIrvNLL3MaNgUCQW9+ZFQSKGQFAmF0q9tzDHdnu79z16aNHvH3OyddFMp5bQF7craXffIZ5Ipp/70cCWXh3/o6Rn0ez9MFUEcAACUHDNTc21UzXOiOn1O04Tv6x2Ka3/3kLoH4xpJpDScTGWC8UgipZFk7vPhePK47xlJpBRPphRPOsWTQVs8mVI8kft6JJHysvFVtsx6+Tkp1nNRnkRCpnL7sxPEAQBA2QqG90SP/8YCSaaywnniSHgfTqQyY+UHRpIaSG+KNTCS3hxrJNjxdmB0B9ys5/3DiZzPDcaTclJeeoxLWX0sHPzyVRcM22pOD9868jo9lKvuyLmWrHO10VDZDdshiAMAALxBwc6x4aKs7T7KudzdY51zmaDusnaWVVbb6HtTzimZdEqkh4UkUqn0MRgycqTdKZlKKZHMfn3kmHLuqJ1zx+6eO96OujnP07vyNsSCCcqRKlxnniAOAABQRjJr32c6f8urFxhHVN+vHgAAAEAJIIgDAAAAHhDEAQAAAA8I4gAAAIAHBHEAAADAA4I4AAAA4AFBHAAAAPCAIA4AAAB4QBAHAAAAPCCIAwAAAB4QxAEAAAAPCOIAAACABwRxAAAAwAOCOAAAAOABQRwAAADwgCAOAAAAeGDOOd815J2ZddTV1c0444wzfJcCAACACrZp0yYNDg52Oudap/rZSg3i2yU1S9oxyY8sSx9fKUhBGIvrXVxc7+LjmhcX17u4uN7FxfUuvqle81Mk9TjnTp3qD6rIID5VZrZWkpxzq3zXUg243sXF9S4+rnlxcb2Li+tdXFzv4ivmNWeMOAAAAOABQRwAAADwgCAOAAAAeEAQBwAAADwgiAMAAAAesGoKAAAA4AE94gAAAIAHBHEAAADAA4I4AAAA4AFBHAAAAPCAIA4AAAB4QBAHAAAAPCCIAwAAAB5UdRA3s/lm9g0z22tmw2a2w8zuMLPpvmurNOlr6yZ47PddX7kysxvN7MtmtsbMetLX897jfGa1mT1oZp1mNmhmL5rZx8wsXKy6y9VUrreZnXKMe96Z2X3Frr/cmFmrmX3AzB4ws63p+7XbzJ4ws98ys3H/H8Y9/sZM9Xpzj584M/u8mf3UzF5PX+9OM3vezP7czFon+Az39xs0letdrPu7ajf0MbNFkp6SNEvSDyS9IukCSW+R9Kqki51zHf4qrCxmtkPSNEl3jHO6zzn3t8Wsp1KY2XpJKyT1SdotaZmkf3XOvXeC918n6X5JQ5K+I6lT0jWSTpf0PefcTUUou2xN5Xqb2SmStkt6QdL3x/m6l5xz3ytUrZXAzD4k6W5J+yQ9KmmXpNmSfkVSi4J7+SaX9T8y7vE3bqrXm3v8xJnZiKR1kjZKOiipQdKFks6XtFfShc6517Pez/19AqZyvYt2fzvnqvIh6WFJTtJHxrR/Md3+Fd81VtJD0g5JO3zXUWkPBb84LpFkkq5I37v3TvDe5vRfPMOSzs9qr1XwS6mTdIvvP1MpP6Z4vU9Jn7/Hd93l+pD0VgUhIzSmfY6CkOgkvSernXu8uNebe/zEr3ntBO2fTV/b/5fVxv1d3OtdlPu7KoempHvDr1QQDv9hzOk/l9Qv6X1m1lDk0oApcc496pzb4tJ/axzHjZJmSrrPOffLrO8YkvSn6ZcfLkCZFWOK1xsnyDn3M+fcfznnUmPa90v6SvrlFVmnuMdPwBu43jhB6XtzPP+ePi7JauP+PkFTvN5FESn2DywRb0kfHxnnL5xeM3tSQVC/UNJPi11cBasxs/dKOlnBLzsvSnrcOZf0W1bVeGv6+NA45x6XNCBptZnVOOeGi1dWxZtrZr8jqVVSh6SnnXMveq6pEsTTx0RWG/d44Yx3vUdxj+ffNelj9nXk/i6c8a73qILe39UaxE9PHzdPcH6LgiC+VATxfJoj6V/GtG03s/c7537uo6AqM+F975xLmNl2ScslnSZpUzELq3BvTz8yzOwxSbc653Z5qajMmVlE0v9Kv8wOJdzjBXCM6z2Ke/wEmdknJDUqGIt/vqRLFITCz2W9jfs7TyZ5vUcV9P6uyqEpCi68JHVPcH60fVrhS6ka35T0NgVhvEHS2ZL+UcEYrP82sxX+Sqsa3PfFNSDpM5JWSZqeflyuYBLcFZJ+yvC3N+xzks6S9KBz7uGsdu7xwpjoenOP588nFAyN/ZiCUPiQpCudc4ey3sP9nT+Tud5Fub+rNYijyJxzf5Eef3jAOTfgnHvJOfchBZNj6yTd7rdCIL+ccwedc3/mnFvnnOtKPx5X8K9tz0paLOkDfqssP2b2UUkfV7DS1fs8l1PxjnW9ucfzxzk3xzlnCjqrfkVBr/bzZrbSb2WVaTLXu1j3d7UG8dHfGlsmOD/a3lX4Uqre6ASgy7xWUR2470uAcy4h6Wvpl9z3U2BmvyfpTgVLj73FOdc55i3c43k0ies9Lu7xNy7dWfWAgrDXKumfs05zf+fZca73RJ/J6/1drUH81fRx6QTnR2fNTjSGHPkz+s9A/PNl4U1436fHgJ6qYCLWa8Usqkpx30+RmX1M0pclvaQgFI63ERj3eJ5M8nofC/f4CXDO7VTwC9ByM2tLN3N/F8gE1/tY8nZ/V2sQfzR9vHKcncKaJF2sYGzQM8UurApdmD7yF0fh/Sx9vGqcc5dJqpf0FLPti4L7fgrM7JOS/l7SegWh8OAEb+Uez4MpXO9j4R4/cXPTx9GVxbi/C2vs9T6WvN3fVRnEnXPbJD2iYKLg7445/RcKfsP5F+dcf5FLq0hmdsZ4ExrSu1bdlX55zG3ZkRffk9Qu6RYzO3+00cxqJf1V+uXdPgqrRGa2crwt2M3sbZJuS7/kvj8OM/u0gsmCayW9zTnXfoy3c4+foKlcb+7xE2NmS83sqGEmZhYys88q2Pn7Kefc4fQp7u8TMNXrXaz7my3uj2xxv0nSmxWsMb5Z0mrHFvd5YWa3K5js87iknZJ6JS2SdLWCHcEelHSDc27EV43lysyul3R9+uUcSe9Q8Bv6mnRbu3PuE2Pe/z0F2yPfp2B75GuV3h5Z0s1sVjOxqVzv9PJWSxT8PbM7ff4cHVkL+NPOudH/eWIcZnarpHsU9FB9WeOvFrHDOXdP1meuF/f4GzLV6809fmLSw3/+r6QnFGyl3iFptoKVOU6TtF/BL0Mbsz5zvbi/35CpXu+i3d+T2X6zUh+SFihYVm+fpBEFIfEOSdN911ZJj/RN/m0Fs+67FGwMcUjSjxWsTWu+ayzXh4LVZtwxHjvG+czFCn75OSxpUNIGBb/dh33/eUr9MZXrLem3JP1QwQ6+fQq2pd4l6TuSLvX9ZymHxySut5P02Dif4x4vwvXmHj/h632Wgn8VXq+gpzuh4JefX6T/W8yY4HPc30W43sW6v6u2RxwAAADwqSrHiAMAAAC+EcQBAAAADwjiAAAAgAcEcQAAAMADgjgAAADgAUEcAAAA8IAgDgAAAHhAEAcAAAA8IIgDAAAAHhDEAQAAAA8I4gAAAIAHBHEAAADAA4I4AAAA4AFBHAAAAPCAIA4AAAB4QBAHAAAAPCCIAwAAAB78f16kZUGUbX+JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 369
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae = VAE()\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in vae.parameters()))\n",
    "trainer = VAETrainer()\n",
    "trainer.train(vae, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTFN-DCJjZWM"
   },
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sSvVK7CxjV8"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter as Param\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter\n",
    "from torch_sparse import SparseTensor, matmul, masked_select_nnz\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (Tensor, Tensor) -> Tensor\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (SparseTensor, Tensor) -> SparseTensor\n",
    "    pass\n",
    "\n",
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return masked_select_nnz(edge_index, edge_mask, layout='coo')\n",
    "\n",
    "\n",
    "class RGCNConv(MessagePassing):\n",
    "    r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "    Relational Data with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}_{\\textrm{root}} \\cdot\n",
    "        \\mathbf{x}_i + \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
    "        \\frac{1}{|\\mathcal{N}_r(i)|} \\mathbf{\\Theta}_r \\cdot \\mathbf{x}_j,\n",
    "\n",
    "    where :math:`\\mathcal{R}` denotes the set of relations, *i.e.* edge types.\n",
    "    Edge type needs to be a one-dimensional :obj:`torch.long` tensor which\n",
    "    stores a relation identifier\n",
    "    :math:`\\in \\{ 0, \\ldots, |\\mathcal{R}| - 1\\}` for each edge.\n",
    "\n",
    "    .. note::\n",
    "        This implementation is as memory-efficient as possible by iterating\n",
    "        over each individual relation type.\n",
    "        Therefore, it may result in low GPU utilization in case the graph has a\n",
    "        large number of relations.\n",
    "        As an alternative approach, :class:`FastRGCNConv` does not iterate over\n",
    "        each individual type, but may consume a large amount of memory to\n",
    "        compensate.\n",
    "        We advise to check out both implementations to see which one fits your\n",
    "        needs.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\n",
    "            corresponds to the sizes of source and target dimensionalities.\n",
    "            In case no input features are given, this argument should\n",
    "            correspond to the number of nodes in your graph.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_relations (int): Number of relations.\n",
    "        num_bases (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the basis-decomposition regularization scheme where\n",
    "            :obj:`num_bases` denotes the number of bases to use.\n",
    "            (default: :obj:`None`)\n",
    "        num_blocks (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the block-diagonal-decomposition regularization scheme where\n",
    "            :obj:`num_blocks` denotes the number of blocks to use.\n",
    "            (default: :obj:`None`)\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
    "            (default: :obj:`\"mean\"`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        num_relations: int,\n",
    "        num_bases: Optional[int] = None,\n",
    "        num_blocks: Optional[int] = None,\n",
    "        aggr: str = 'mean',\n",
    "        root_weight: bool = True,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(aggr=aggr, node_dim=0, **kwargs)\n",
    "\n",
    "        if num_bases is not None and num_blocks is not None:\n",
    "            raise ValueError('Can not apply both basis-decomposition and '\n",
    "                             'block-diagonal-decomposition at the same time.')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_relations = num_relations\n",
    "        self.num_bases = num_bases\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "        self.in_channels_l = in_channels[0]\n",
    "\n",
    "        if num_bases is not None:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_bases, in_channels[0], out_channels))\n",
    "            self.comp = Parameter(torch.Tensor(num_relations, num_bases))\n",
    "\n",
    "        elif num_blocks is not None:\n",
    "            assert (in_channels[0] % num_blocks == 0\n",
    "                    and out_channels % num_blocks == 0)\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, num_blocks,\n",
    "                             in_channels[0] // num_blocks,\n",
    "                             out_channels // num_blocks))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        else:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, in_channels[0], out_channels))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        if root_weight:\n",
    "            self.root = Param(torch.Tensor(in_channels[1], out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Param(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        glorot(self.comp)\n",
    "        glorot(self.root)\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
    "                edge_index: Adj, edge_type: OptTensor = None):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x: The input node features. Can be either a :obj:`[num_nodes,\n",
    "                in_channels]` node feature matrix, or an optional\n",
    "                one-dimensional node index tensor (in which case input features\n",
    "                are treated as trainable node embeddings).\n",
    "                Furthermore, :obj:`x` can be of type :obj:`tuple` denoting\n",
    "                source and destination node features.\n",
    "            edge_type: The one-dimensional relation type/index for each edge in\n",
    "                :obj:`edge_index`.\n",
    "                Should be only :obj:`None` in case :obj:`edge_index` is of type\n",
    "                :class:`torch_sparse.tensor.SparseTensor`.\n",
    "                (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert input features to a pair of node features or node indices.\n",
    "        x_l: OptTensor = None\n",
    "        if isinstance(x, tuple):\n",
    "            x_l = x[0]\n",
    "        else:\n",
    "            x_l = x\n",
    "        if x_l is None:\n",
    "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
    "\n",
    "        x_r: Tensor = x_l\n",
    "        if isinstance(x, tuple):\n",
    "            x_r = x[1]\n",
    "\n",
    "        size = (x_l.size(0), x_r.size(0))\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_type = edge_index.storage.value()\n",
    "        assert edge_type is not None\n",
    "\n",
    "        # propagate_type: (x: Tensor)\n",
    "        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)\n",
    "\n",
    "        weight = self.weight\n",
    "        if self.num_bases is not None:  # Basis-decomposition =================\n",
    "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
    "                self.num_relations, self.in_channels_l, self.out_channels)\n",
    "\n",
    "        if self.num_blocks is not None:  # Block-diagonal-decomposition =====\n",
    "\n",
    "            if x_l.dtype == torch.long and self.num_blocks is not None:\n",
    "                raise ValueError('Block-diagonal decomposition not supported '\n",
    "                                 'for non-continuous input features.')\n",
    "\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                h = self.propagate(tmp, x=x_l, size=size)\n",
    "                h = h.view(-1, weight.size(1), weight.size(2))\n",
    "                h = torch.einsum('abc,bcd->abd', h, weight[i])\n",
    "                out += h.contiguous().view(-1, self.out_channels)\n",
    "\n",
    "        else:  # No regularization/Basis-decomposition ========================\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "\n",
    "                if x_l.dtype == torch.long:\n",
    "                    out += self.propagate(tmp, x=weight[i, x_l], size=size)\n",
    "                else:\n",
    "                    h = self.propagate(tmp, x=x_l, size=size)\n",
    "                    out = out + (h @ weight[i])\n",
    "\n",
    "        root = self.root\n",
    "        if root is not None:\n",
    "            out += root[x_r] if x_r.dtype == torch.long else x_r @ root\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor) -> Tensor:\n",
    "        return x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        adj_t = adj_t.set_value(None, layout=None)\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, num_relations={self.num_relations})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfsNpMLrEXLk"
   },
   "source": [
    "next edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DaVTonr_XB8",
    "outputId": "9e5fa8f9-604e-4273-a34d-fad73ad9ab7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 0 0]\n",
      " [1 1 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 0]\n",
      " [1 1 1 0]\n",
      " [0 0 0 0]]\n",
      "[[ 0  8 16 24]\n",
      " [ 1  9 17 25]\n",
      " [ 2 10 18 26]\n",
      " [ 3 11 19 27]\n",
      " [ 4 12 20 28]\n",
      " [ 5 13 21 29]\n",
      " [ 6 14 22 30]\n",
      " [ 7 15 23 31]]\n",
      "[(8, 1, 1), (8, 17, 1), (8, 25, 1), (16, 1, 1), (16, 9, 1), (16, 25, 1), (24, 1, 1), (24, 9, 1), (24, 17, 1), (1, 10, 1), (9, 2, 1), (17, 2, 1), (17, 10, 1), (25, 2, 1), (25, 10, 1), (2, 11, 1), (2, 19, 1), (2, 27, 1), (10, 3, 1), (10, 19, 1), (10, 27, 1), (3, 12, 1), (3, 28, 1), (11, 28, 1), (19, 12, 1), (19, 28, 1), (27, 12, 1), (12, 5, 1), (28, 5, 1), (5, 14, 1), (5, 22, 1)]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "a = np.random.randint(2, size=(4,8))\n",
    "a_t = a.transpose()\n",
    "print(a_t)\n",
    "inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "ts_acts = np.any(a_t, axis=1)\n",
    "ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "labels = np.arange(32).reshape(4, 8).transpose()\n",
    "print(labels)\n",
    "\n",
    "next_edges = []\n",
    "for i in range(len(ts_inds)-1):\n",
    "    ind_s = ts_inds[i]\n",
    "    ind_e = ts_inds[i+1]\n",
    "    s = inds[inds[:,0] == ind_s]\n",
    "    e = inds[inds[:,0] == ind_e]\n",
    "    e_inds = [t for t in list(itertools.product(s, e)) if t[0][1] != t[1][1]]\n",
    "    edges = [(labels[tuple(e[0])],labels[tuple(e[1])], ind_e-ind_s) for e in e_inds]\n",
    "    next_edges.extend(edges)\n",
    "\n",
    "print(next_edges)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ5JQm1aEbmb"
   },
   "source": [
    "onset edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DISmsJB3EatR",
    "outputId": "fc864608-63a6-4ad0-84d9-1478001ce60e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 0 0]\n",
      " [1 1 1 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 0]\n",
      " [1 1 1 0]\n",
      " [0 0 0 0]]\n",
      "[[ 0  8 16 24]\n",
      " [ 1  9 17 25]\n",
      " [ 2 10 18 26]\n",
      " [ 3 11 19 27]\n",
      " [ 4 12 20 28]\n",
      " [ 5 13 21 29]\n",
      " [ 6 14 22 30]\n",
      " [ 7 15 23 31]]\n",
      "[(8, 16, 0), (8, 24, 0), (16, 24, 0), (16, 8, 0), (24, 8, 0), (24, 16, 0), (1, 9, 0), (1, 17, 0), (1, 25, 0), (9, 17, 0), (9, 25, 0), (17, 25, 0), (9, 1, 0), (17, 1, 0), (25, 1, 0), (17, 9, 0), (25, 9, 0), (25, 17, 0), (2, 10, 0), (10, 2, 0), (3, 11, 0), (3, 19, 0), (3, 27, 0), (11, 19, 0), (11, 27, 0), (19, 27, 0), (11, 3, 0), (19, 3, 0), (27, 3, 0), (19, 11, 0), (27, 11, 0), (27, 19, 0), (12, 28, 0), (28, 12, 0), (6, 14, 0), (6, 22, 0), (14, 22, 0), (14, 6, 0), (22, 6, 0), (22, 14, 0)]\n"
     ]
    }
   ],
   "source": [
    "onset_edges = []\n",
    "print(a_t)\n",
    "print(labels)\n",
    "\n",
    "for i in ts_inds:\n",
    "    ts_acts_inds = list(inds[inds[:,0] == i])\n",
    "    if len(ts_acts_inds) < 2:\n",
    "        continue\n",
    "    e_inds = list(itertools.combinations(ts_acts_inds, 2))\n",
    "    edges = [(labels[tuple(e[0])], labels[tuple(e[1])], 0) for e in e_inds]\n",
    "    inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "    onset_edges.extend(edges)\n",
    "    onset_edges.extend(inv_edges)\n",
    "\n",
    "print(onset_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujitZCKaa7nu"
   },
   "source": [
    "track edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbVG1vdFa-7e",
    "outputId": "c042449b-eef2-4707-a524-5f66f3ec07c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]\n",
      " [1 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [1 1 0 0]\n",
      " [0 0 1 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 1]]\n",
      "[[ 0  8 16 24]\n",
      " [ 1  9 17 25]\n",
      " [ 2 10 18 26]\n",
      " [ 3 11 19 27]\n",
      " [ 4 12 20 28]\n",
      " [ 5 13 21 29]\n",
      " [ 6 14 22 30]\n",
      " [ 7 15 23 31]]\n",
      "[(array([0, 0]), array([1, 0])), (array([1, 0]), array([4, 0]))]\n",
      "[(array([0, 1]), array([2, 1])), (array([2, 1]), array([4, 1]))]\n",
      "[(array([0, 2]), array([5, 2])), (array([5, 2]), array([6, 2])), (array([6, 2]), array([7, 2]))]\n",
      "[(array([0, 3]), array([1, 3])), (array([1, 3]), array([5, 3])), (array([5, 3]), array([7, 3]))]\n",
      "[(0, 1, 1), (1, 4, 3), (8, 10, 2), (10, 12, 2), (16, 21, 5), (21, 22, 1), (22, 23, 1), (24, 25, 1), (25, 29, 4), (29, 31, 2)]\n"
     ]
    }
   ],
   "source": [
    "print(a_t)\n",
    "print(labels)\n",
    "track_edges = []\n",
    "\n",
    "for track in range(a_t.shape[1]):\n",
    "    tr_inds = list(inds[inds[:,1] == track])\n",
    "    e_inds = [(tr_inds[i],\n",
    "               tr_inds[i+1]) for i in range(len(tr_inds)-1)]\n",
    "    print(e_inds)\n",
    "    edges = [(labels[tuple(e[0])], labels[tuple(e[1])], e[1][0]-e[0][0]) for e in e_inds]\n",
    "    track_edges.extend(edges)\n",
    "\n",
    "print(track_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DzouJ5NqALB",
    "outputId": "20a76e82-6305-4154-d894-6d69a64435a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_edges = np.array(track_edges)\n",
    "onset_edges = np.array(onset_edges)\n",
    "np.concatenate((track_edges, onset_edges)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihIYkPWPzyGX"
   },
   "outputs": [],
   "source": [
    "pip install pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie0pU8NWAUNM"
   },
   "outputs": [],
   "source": [
    "import pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTbGBSrdAZGH"
   },
   "outputs": [],
   "source": [
    "multitrack = pypianoroll.read(\"tests_fur-elise.mid\")\n",
    "print(multitrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eVo_BKzAmz4"
   },
   "outputs": [],
   "source": [
    "multitrack.tracks[0].pianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPpWw-rLA7CI"
   },
   "outputs": [],
   "source": [
    "multitrack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-PYbS7FA-Gg"
   },
   "outputs": [],
   "source": [
    "multitrack.trim(0, 12 * multitrack.resolution)\n",
    "multitrack.binarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psxuoTsZBFXY"
   },
   "outputs": [],
   "source": [
    "multitrack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovyixmSvBG3w"
   },
   "outputs": [],
   "source": [
    "multitrack.tracks[0].pianoroll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHlKNufuBzLn"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOhjCOJb34P4bTid7qFDg58",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1NeVldMsPVJd6pXbxZDmuiUP-QJBRhYtj",
   "name": "midi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
