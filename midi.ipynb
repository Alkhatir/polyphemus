{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmanueleCosenza/Polyphemus/blob/main/midi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "50lpUn9bO0ug",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cosenza/thesis/Polyphemus\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  main\r\n",
      "* sparse\r\n"
     ]
    }
   ],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -C data -xvzf data/lmd_matched.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "She0QbN5Kopo",
    "outputId": "0f3fb4c7-bd7d-4ee4-b2cd-d567d8e490db",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install the required music libraries\n",
    "#!pip3 install muspy\n",
    "#!pip3 install pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uveQkY7O0CF",
    "outputId": "12e1f638-ee78-4617-844a-10e9a26c298e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install torch_geometric\n",
    "#!v=$(python3 -c \"import torch; print(torch.__version__)\"); \\\n",
    "#pip3 install torch-scatter -f https://data.pyg.org/whl/torch-${v}.html; \\\n",
    "#pip3 install torch-sparse -f https://data.pyg.org/whl/torch-${v}.html; \\\n",
    "#pip3 install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B45l1513wJ1Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import muspy\n",
    "from itertools import product\n",
    "import pypianoroll as pproll\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class MIDIPreprocessor():\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def preprocess_dataset(self, dir, early_exit=None):\n",
    "        pass\n",
    "    \n",
    "    def preprocess_file(self, f):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Todo: to config file (or separate files)\n",
    "MAX_SIMU_NOTES = 16 # 14 + SOS and EOS\n",
    "\n",
    "PITCH_SOS = 128\n",
    "PITCH_EOS = 129\n",
    "PITCH_PAD = 130\n",
    "DUR_PAD_IND = 2\n",
    "MAX_DUR = 511 # equivalent to 16 bars (with RESOLUTION=32)\n",
    "\n",
    "RESOLUTION = 32\n",
    "NUM_BARS = 1\n",
    "\n",
    "\n",
    "def preprocess_file(filepath, dest_dir, num_samples):\n",
    "\n",
    "    saved_samples = 0\n",
    "\n",
    "    print(\"Preprocessing file \" + filepath)\n",
    "\n",
    "    # Load the file both as a pypianoroll song and a muspy song\n",
    "    # (Need to load both since muspy.to_pypianoroll() is expensive)\n",
    "    try:\n",
    "        pproll_song = pproll.read(filepath, resolution=RESOLUTION)\n",
    "        muspy_song = muspy.read(filepath)\n",
    "    except Exception as e:\n",
    "        print(\"Song skipped (Invalid song format)\")\n",
    "        return 0\n",
    "    \n",
    "    # Only accept songs that have a time signature of 4/4 and no time changes\n",
    "    for t in muspy_song.time_signatures:\n",
    "        if t.numerator != 4 or t.denominator != 4:\n",
    "            print(\"Song skipped ({}/{} time signature)\".\n",
    "                            format(t.numerator, t.denominator))\n",
    "            return 0\n",
    "\n",
    "    # Gather tracks of pypianoroll song based on MIDI program number\n",
    "    drum_tracks = []\n",
    "    bass_tracks = []\n",
    "    guitar_tracks = []\n",
    "    strings_tracks = []\n",
    "\n",
    "    for track in pproll_song.tracks:\n",
    "        if track.is_drum:\n",
    "            track.name = 'Drums'\n",
    "            drum_tracks.append(track)\n",
    "        elif 0 <= track.program <= 31:\n",
    "            track.name = 'Guitar'\n",
    "            guitar_tracks.append(track)\n",
    "        elif 32 <= track.program <= 39:\n",
    "            track.name = 'Bass'\n",
    "            bass_tracks.append(track)\n",
    "        else:\n",
    "            # Tracks with program > 39 are all considered as strings tracks\n",
    "            # and will be merged into a single track later on\n",
    "            strings_tracks.append(track)\n",
    "\n",
    "    # Filter song if it does not contain drum, guitar, bass or strings tracks\n",
    "    if not drum_tracks or not guitar_tracks \\\n",
    "       or not bass_tracks or not strings_tracks:\n",
    "        print(\"Song skipped (does not contain drum or \"\n",
    "                \"guitar or bass or strings tracks)\")\n",
    "        return 0\n",
    "    \n",
    "    # Merge strings tracks into a single pypianoroll track\n",
    "    strings = pproll.Multitrack(tracks=strings_tracks)\n",
    "    strings_track = pproll.Track(pianoroll=strings.blend(mode='max'),\n",
    "                                 program=48, name='Strings')\n",
    "\n",
    "    combinations = list(product(drum_tracks, bass_tracks, guitar_tracks))\n",
    "\n",
    "    # Single instruments can have multiple tracks.\n",
    "    # Consider all possible combinations of drum, bass, and guitar tracks\n",
    "    for i, combination in enumerate(combinations):\n",
    "\n",
    "        print(\"Processing combination\", i+1, \"of\", len(combinations))\n",
    "        \n",
    "        # Process combination (called 'subsong' from now on)\n",
    "        drum_track, bass_track, guitar_track = combination\n",
    "        tracks = [drum_track, bass_track, guitar_track, strings_track]\n",
    "        \n",
    "        pproll_subsong = pproll.Multitrack(\n",
    "            tracks=tracks,\n",
    "            tempo=pproll_song.tempo,\n",
    "            resolution=RESOLUTION\n",
    "        )\n",
    "        muspy_subsong = muspy.from_pypianoroll(pproll_subsong)\n",
    "        \n",
    "        tracks_notes = [track.notes for track in muspy_subsong.tracks]\n",
    "        \n",
    "        # Obtain length of subsong (maximum of each track's length)\n",
    "        length = 0\n",
    "        for notes in tracks_notes:\n",
    "            track_length = max(note.end for note in notes) if notes else 0\n",
    "            length = max(length, track_length)\n",
    "        length += 1\n",
    "\n",
    "        # Add timesteps until length is a multiple of RESOLUTION\n",
    "        length = length if length%(RESOLUTION) == 0 \\\n",
    "                            else length + (RESOLUTION-(length%(RESOLUTION)))\n",
    "\n",
    "\n",
    "        tracks_tensors = []\n",
    "        tracks_activations = []\n",
    "\n",
    "        dur_bin_length = int(np.ceil(np.log2(MAX_DUR)))\n",
    "\n",
    "        # Todo: adapt to velocity\n",
    "        for notes in tracks_notes:\n",
    "\n",
    "            # Initialize encoder-ready track tensor\n",
    "            # track_tensor: (length x max_simu_notes x 2 (or 3 if velocity))\n",
    "            # The last dimension contains pitches and durations (and velocities)\n",
    "            # int16 is enough for small to medium duration values\n",
    "            track_tensor = np.zeros((length, MAX_SIMU_NOTES, 2), np.int16)\n",
    "\n",
    "            track_tensor[:, :, 0] = PITCH_PAD\n",
    "            track_tensor[:, 0, 0] = PITCH_SOS\n",
    "\n",
    "            # Keeps track of how many notes have been stored in each timestep\n",
    "            # (int8 imposes that MAX_SIMU_NOTES < 256)\n",
    "            notes_counter = np.ones(length, dtype=np.int8)\n",
    "\n",
    "            # Todo: np.put_along_axis?\n",
    "            for note in notes:\n",
    "                # Insert note in the lowest position available in the timestep\n",
    "                \n",
    "                t = note.time\n",
    "\n",
    "                if notes_counter[t] >= MAX_SIMU_NOTES-1:\n",
    "                    # Skip note if there is no more space\n",
    "                    continue\n",
    "\n",
    "                track_tensor[t, notes_counter[t], 0] = note.pitch\n",
    "                track_tensor[t, notes_counter[t], 1] = note.duration\n",
    "                notes_counter[t] += 1\n",
    "            \n",
    "            # Add end of sequence token\n",
    "            track_tensor[np.arange(0, length), notes_counter, 0] = PITCH_EOS\n",
    "\n",
    "            # Get track activations, a boolean tensor indicating whether notes\n",
    "            # are being played in a timestep (sustain does not count)\n",
    "            # (needed for graph rep.)\n",
    "            activations = np.array(notes_counter-1, dtype=bool)\n",
    "\n",
    "            tracks_tensors.append(track_tensor)\n",
    "            tracks_activations.append(activations)\n",
    "        \n",
    "        # (#tracks x length x max_simu_notes x 2 (or 3))\n",
    "        subsong_tensor = np.stack(tracks_tensors, axis=0)\n",
    "\n",
    "        # (#tracks x length)\n",
    "        subsong_activations = np.stack(tracks_activations, axis=0)\n",
    "\n",
    "\n",
    "        # Slide window over 'subsong_tensor' and 'subsong_activations' along the\n",
    "        # time axis (2nd dimension) with the stride of a bar\n",
    "        # Todo: np.lib.stride_tricks.as_strided(song_proll)\n",
    "        for i in range(0, length-NUM_BARS*RESOLUTION+1, RESOLUTION):\n",
    "            \n",
    "            # Get the sequence and its activations\n",
    "            seq_tensor = subsong_tensor[:, i:i+NUM_BARS*RESOLUTION, :]\n",
    "            seq_acts = subsong_activations[:, i:i+NUM_BARS*RESOLUTION]\n",
    "\n",
    "            if NUM_BARS > 1:\n",
    "                # Skip sequence if it contains more than one bar of consecutive\n",
    "                # silence in at least one track\n",
    "                bars = seq_acts.reshape(seq_acts.shape[0], NUM_BARS, -1)\n",
    "                bars_acts = np.any(bars, axis=2)\n",
    "\n",
    "                if 1 in np.diff(np.where(bars_acts == 0)[1]):\n",
    "                    continue\n",
    "            else:\n",
    "                # In the case of just 1 bar, skip it if all tracks are silenced\n",
    "                bar_acts = np.any(seq_acts, axis=1)\n",
    "                if not np.any(bar_acts):\n",
    "                    continue\n",
    "                \n",
    "\n",
    "            # Randomly transpose the pitches of the sequence (-5 to 6 semitones)\n",
    "            shift = np.random.choice(np.arange(-5, 7), 1)\n",
    "            cond = (seq_tensor[:, :, :, 0] != PITCH_PAD) &                     \\\n",
    "                   (seq_tensor[:, :, :, 0] != PITCH_SOS) &                     \\\n",
    "                   (seq_tensor[:, :, :, 0] != PITCH_EOS)\n",
    "            seq_tensor[cond, 0] += shift\n",
    "\n",
    "            # Save sample (seq_tensor and seq_acts) to file\n",
    "            curr_sample = str(num_samples + saved_samples)\n",
    "            sample_filepath = os.path.join(dest_dir, curr_sample)\n",
    "            np.savez(sample_filepath, seq_tensor=seq_tensor, seq_acts=seq_acts)\n",
    "\n",
    "            saved_samples += 1\n",
    "\n",
    "\n",
    "    print(\"File preprocessing finished. Saved samples:\", saved_samples)\n",
    "    print()\n",
    "\n",
    "    return saved_samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Total number of files: 116189\n",
    "# Number of unique files: 45129\n",
    "def preprocess_dataset(dataset_dir, dest_dir, num_files=45129, early_exit=None):\n",
    "\n",
    "    files_dict = {}\n",
    "    seen = 0\n",
    "    tot_samples = 0\n",
    "    not_filtered = 0\n",
    "    finished = False\n",
    "    \n",
    "    print(\"Starting preprocessing\")\n",
    "    \n",
    "    progress_bar = tqdm(range(early_exit)) if early_exit is not None else tqdm(range(num_files))\n",
    "    start = time.time()\n",
    "\n",
    "    # Visit recursively the directories inside the dataset directory\n",
    "    for dirpath, dirs, files in os.walk(dataset_dir):\n",
    "\n",
    "        # Sort alphabetically the found directories\n",
    "        # (to help guess the remaining time) \n",
    "        dirs.sort()\n",
    "        \n",
    "        print(\"Current path:\", dirpath)\n",
    "        print()\n",
    "\n",
    "        for f in files:\n",
    "            \n",
    "            seen += 1\n",
    "\n",
    "            if f in files_dict:\n",
    "                # Skip already seen file\n",
    "                files_dict[f] += 1\n",
    "                continue\n",
    "\n",
    "            # File never seen before, add to dictionary of files\n",
    "            # (from filename to # of occurrences)\n",
    "            files_dict[f] = 1\n",
    "\n",
    "            # Preprocess file\n",
    "            filepath = os.path.join(dirpath, f)\n",
    "            n_saved = preprocess_file(filepath, dest_dir, tot_samples)\n",
    "\n",
    "            tot_samples += n_saved\n",
    "            if n_saved > 0:\n",
    "                not_filtered += 1\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            # Todo: also print # of processed (not filtered) files\n",
    "            #       and # of produced sequences (samples)\n",
    "            print(\"Total number of seen files:\", seen)\n",
    "            print(\"Number of unique files:\", len(files_dict))\n",
    "            print(\"Total number of non filtered songs:\", not_filtered)\n",
    "            print(\"Total number of saved samples:\", tot_samples)\n",
    "            print()\n",
    "\n",
    "            # Exit when a maximum number of files has been processed (if set)\n",
    "            if early_exit != None and len(files_dict) >= early_exit:\n",
    "                finished = True\n",
    "                break\n",
    "\n",
    "        if finished:\n",
    "            break\n",
    "    \n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"Preprocessing completed in (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "              .format(int(hours),int(minutes),seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aYc5y-CYyetK"
   },
   "outputs": [],
   "source": [
    "!rm -rf data/preprocessed/\n",
    "!mkdir data/preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqnubg3oP4ES",
    "outputId": "40cc38a2-1f7d-4f6f-e6c9-9e14dfc7f683",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'data/lmd_matched'\n",
    "dest_dir = 'data/preprocessed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG88mekfrrcp"
   },
   "source": [
    "Check preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e452d70be30c4267b373b3bdfd3efa77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current path: data/lmd_matched\n",
      "\n",
      "Current path: data/lmd_matched/A\n",
      "\n",
      "Current path: data/lmd_matched/A/A\n",
      "\n",
      "Current path: data/lmd_matched/A/A/A\n",
      "\n",
      "Current path: data/lmd_matched/A/A/A/TRAAAGR128F425B14B\n",
      "\n",
      "Preprocessing file data/lmd_matched/A/A/A/TRAAAGR128F425B14B/1d9d16a9da90c090809c153754823c2b.mid\n",
      "Processing combination 1 of 7\n",
      "Processing combination 2 of 7\n",
      "Processing combination 3 of 7\n",
      "Processing combination 4 of 7\n",
      "Processing combination 5 of 7\n",
      "Processing combination 6 of 7\n",
      "Processing combination 7 of 7\n",
      "File preprocessing finished. Saved samples: 3031\n",
      "\n",
      "Total number of seen files: 1\n",
      "Number of unique files: 1\n",
      "Total number of non filtered songs: 1\n",
      "Total number of saved samples: 3031\n",
      "\n",
      "Preprocessing completed in (h:m:s): 00:00:02.88\n"
     ]
    }
   ],
   "source": [
    "preprocess_dataset(dataset_dir, dest_dir, early_exit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JlP6iUNugNtP"
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(dest_dir, \"5.npz\")\n",
    "data = np.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VUpOEObhwYQ",
    "outputId": "aac6e029-93b1-485f-f13a-2a00abedbc7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 32, 16, 2)\n",
      "(4, 32)\n"
     ]
    }
   ],
   "source": [
    "print(data[\"seq_tensor\"].shape)\n",
    "print(data[\"seq_acts\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6NA5IAAmtK8",
    "outputId": "6e661b3a-05a1-4e2d-9a3d-e1c037b4d04f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128,   0],\n",
       "       [129,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0]], dtype=int16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"seq_tensor\"][0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C19X9m-3iMlm"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zymqD-UqR8wq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import itertools\n",
    "\n",
    "\n",
    "def unpackbits(x, num_bits):\n",
    "\n",
    "    if np.issubdtype(x.dtype, np.floating):\n",
    "        raise ValueError(\"numpy data type needs to be int-like\")\n",
    "\n",
    "    xshape = list(x.shape)\n",
    "    x = x.reshape([-1, 1])\n",
    "    mask = 2**np.arange(num_bits, dtype=x.dtype).reshape([1, num_bits])\n",
    "\n",
    "    return (x & mask).astype(bool).astype(int).reshape(xshape + [num_bits])\n",
    "\n",
    "\n",
    "class MIDIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir):\n",
    "        self.dir = dir\n",
    "\n",
    "    def __len__(self):\n",
    "        _, _, files = next(os.walk(self.dir))\n",
    "        return len(files)\n",
    "\n",
    "    \n",
    "    def __get_track_edges(self, acts, edge_type_ind=0):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        \n",
    "        # Create node labels\n",
    "        labels = np.zeros(acts.shape)\n",
    "        acts_inds = np.where(acts == 1)\n",
    "        num_nodes = len(acts_inds[0])\n",
    "        labels[acts_inds] = np.arange(num_nodes)\n",
    "        labels = labels.transpose()\n",
    "\n",
    "        track_edges = []\n",
    "\n",
    "        for track in range(a_t.shape[1]):\n",
    "            tr_inds = list(inds[inds[:,1] == track])\n",
    "            e_inds = [(tr_inds[i],\n",
    "                    tr_inds[i+1]) for i in range(len(tr_inds)-1)]\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, e[1][0]-e[0][0]) for e in e_inds]\n",
    "            track_edges.extend(edges)\n",
    "\n",
    "        return np.array(track_edges, dtype='long')\n",
    "\n",
    "    \n",
    "    def __get_onset_edges(self, acts, edge_type_ind=1):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        # Create node labels\n",
    "        labels = np.zeros(acts.shape)\n",
    "        acts_inds = np.where(acts == 1)\n",
    "        num_nodes = len(acts_inds[0])\n",
    "        labels[acts_inds] = np.arange(num_nodes)\n",
    "        labels = labels.transpose()\n",
    "\n",
    "        onset_edges = []\n",
    "\n",
    "        for i in ts_inds:\n",
    "            ts_acts_inds = list(inds[inds[:,0] == i])\n",
    "            if len(ts_acts_inds) < 2:\n",
    "                continue\n",
    "            e_inds = list(itertools.combinations(ts_acts_inds, 2))\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, 0) for e in e_inds]\n",
    "            inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "            onset_edges.extend(edges)\n",
    "            onset_edges.extend(inv_edges)\n",
    "\n",
    "        return np.array(onset_edges, dtype='long')\n",
    "\n",
    "\n",
    "    def __get_next_edges(self, acts, edge_type_ind=2):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        # Create node labels\n",
    "        labels = np.zeros(acts.shape)\n",
    "        acts_inds = np.where(acts == 1)\n",
    "        num_nodes = len(acts_inds[0])\n",
    "        labels[acts_inds] = np.arange(num_nodes)\n",
    "        labels = labels.transpose()\n",
    "\n",
    "        next_edges = []\n",
    "\n",
    "        for i in range(len(ts_inds)-1):\n",
    "\n",
    "            ind_s = ts_inds[i]\n",
    "            ind_e = ts_inds[i+1]\n",
    "            s = inds[inds[:,0] == ind_s]\n",
    "            e = inds[inds[:,0] == ind_e]\n",
    "\n",
    "            e_inds = [t for t in list(itertools.product(s, e)) if t[0][1] != t[1][1]]\n",
    "            edges = [(labels[tuple(e[0])],labels[tuple(e[1])], edge_type_ind, ind_e-ind_s) for e in e_inds]\n",
    "\n",
    "            next_edges.extend(edges)\n",
    "\n",
    "        return np.array(next_edges, dtype='long')\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Load tensors\n",
    "        sample_path = os.path.join(self.dir, str(idx) + \".npz\")\n",
    "        data = np.load(sample_path)\n",
    "\n",
    "        seq_tensor = data[\"seq_tensor\"]\n",
    "        seq_acts = data[\"seq_acts\"]\n",
    "\n",
    "        # From decimals to one-hot (pitch)\n",
    "        pitches = seq_tensor[:, :, :, 0]\n",
    "        onehot = np.zeros((pitches.shape[0]*pitches.shape[1]*pitches.shape[2],\n",
    "                            131), dtype=float)\n",
    "        onehot[np.arange(0, onehot.shape[0]), pitches.reshape(-1)] = 1.\n",
    "        onehot = onehot.reshape(-1, pitches.shape[1], seq_tensor.shape[2], 131)\n",
    "\n",
    "        # From decimals to binary (pitch)\n",
    "        durs = seq_tensor[:, :, :, 1]\n",
    "        bin_durs = unpackbits(durs, 9)[:, :, :, ::-1]\n",
    "\n",
    "        # Concatenate pitches and durations\n",
    "        new_seq_tensor = np.concatenate((onehot[:, :, :, :], bin_durs),\n",
    "                             axis=-1)\n",
    "        \n",
    "        # Construct graph from boolean activations\n",
    "        # Todo: optimize and refactor\n",
    "        track_edges = self.__get_track_edges(seq_acts)\n",
    "        onset_edges = self.__get_onset_edges(seq_acts)\n",
    "        next_edges = self.__get_next_edges(seq_acts)\n",
    "        edges = [track_edges, onset_edges, next_edges]\n",
    "\n",
    "        # Concatenate edge tensors (N x 4) (if any)\n",
    "        no_edges = (len(track_edges) == 0 and \n",
    "                    len(onset_edges) == 0 and len(next_edges) == 0)\n",
    "        if not no_edges:\n",
    "            edge_list = np.concatenate([x for x in edges\n",
    "                                          if x.size > 0])\n",
    "            edge_list = torch.from_numpy(edge_list)\n",
    "        \n",
    "        # Adapt tensor to torch_geometric's Data\n",
    "        # Todo: re-check no edges case\n",
    "        edge_index = (torch.LongTensor([[], []]) if no_edges else\n",
    "                               edge_list[:, :2].t().contiguous())\n",
    "        edge_attr = (torch.Tensor([[0, 0]]) if no_edges else\n",
    "                                       edge_list[:, 2:])\n",
    "        \n",
    "        \n",
    "        #n = seq_acts.shape[0]*seq_acts.shape[1]\n",
    "        n = torch.sum(torch.Tensor(seq_acts), dtype=torch.long) # sparse\n",
    "        graph = Data(edge_index=edge_index, edge_attr=edge_attr, num_nodes=n)\n",
    "        \n",
    "        # Todo: start with torch at mount\n",
    "        return torch.Tensor(new_seq_tensor), torch.Tensor(seq_acts), graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "hSwcnlq4g50O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "\n",
    "# Todo: check and think about max_len\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 256):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *                     \\\n",
    "                             (-math.log(10000.0)/d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position*div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, features_dims=[256, 256, 256], num_relations=3,\n",
    "                    dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(len(features_dims)-1):\n",
    "            self.layers.append(GCNConv(features_dims[i], features_dims[i+1]))\n",
    "        self.p = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    # 140 = 128+3+9\n",
    "    def __init__(self, d_token=140, d_transf=256, nhead_transf=4, \n",
    "                 num_layers_transf=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Todo: one separate encoder for drums\n",
    "        # Transformer Encoder\n",
    "        self.embedding = nn.Linear(d_token, d_transf)\n",
    "        self.pos_encoder = PositionalEncoding(d_transf, dropout=dropout)\n",
    "        transf_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_transf,\n",
    "            nhead=nhead_transf,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            transf_layer,\n",
    "            num_layers=num_layers_transf\n",
    "        )\n",
    "\n",
    "        # Graph encoder\n",
    "        self.graph_encoder = GCN(dropout=dropout)\n",
    "\n",
    "        # (LSTM)\n",
    "        \n",
    "        # Linear layers that compute the final mu and log_var\n",
    "        # Todo: as parameters\n",
    "        self.linear_mu = nn.Linear(256, 256)\n",
    "        self.linear_log_var = nn.Linear(256, 256)\n",
    "\n",
    "    def forward(self, x_seq, x_acts, x_graph):\n",
    "\n",
    "        # Collapse track (and optionally batch) dimension\n",
    "        #print(\"Init input:\", x_seq.size())\n",
    "        x_seq = x_seq.view(-1, x_seq.size(-2), x_seq.size(-1))\n",
    "        #print(\"Reshaped input:\", x_seq.size())\n",
    "        \n",
    "        # Filter silences\n",
    "        x_acts = x_acts.view(-1)\n",
    "        x_seq = x_seq[x_acts.bool()]\n",
    "\n",
    "        # Compute embeddings\n",
    "        embs = self.embedding(x_seq)\n",
    "        #print(\"Embs:\", embs.size())\n",
    "\n",
    "        # batch_first = False\n",
    "        embs = embs.permute(1, 0, 2)\n",
    "        #print(\"Seq len first input:\", embs.size())\n",
    "\n",
    "        pos_encs = self.pos_encoder(embs)\n",
    "        #print(\"Pos encodings:\", pos_encs.size())\n",
    "\n",
    "        # Todo: src_key_padding_mask = (src != pad).unsqueeze(-2) ?\n",
    "        transformer_encs = self.transformer_encoder(pos_encs)\n",
    "        #print(\"Transf encodings:\", transformer_encs.size())\n",
    "\n",
    "        pooled_encs = torch.mean(transformer_encs, 0)\n",
    "        #print(\"Pooled encodings:\", pooled_encs.size())\n",
    "\n",
    "        # Compute node encodings\n",
    "        x_graph.x = pooled_encs\n",
    "        node_encs = self.graph_encoder(x_graph)\n",
    "        #print(\"Node encodings:\", node_encs.size())\n",
    "        \n",
    "        # Compute final graph latent vector(s)\n",
    "        # (taking into account the batch size)\n",
    "        encoding = scatter_mean(x_graph.x, x_graph.batch, dim=0)\n",
    "        #num_nodes = x_graph[0].num_nodes\n",
    "        #batch_sz = node_encs.size(0) // num_nodes\n",
    "        #node_encs = node_encs.view(batch_sz, num_nodes, -1)\n",
    "        #encoding = torch.mean(node_encs, 1)\n",
    "\n",
    "        # Compute mu and log(std^2)\n",
    "        mu = self.linear_mu(encoding)\n",
    "        log_var = self.linear_log_var(encoding)\n",
    "        \n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_z=256, n_tracks=4, resolution=32, d_token=140, d_model=256,\n",
    "                 d_transf=256, nhead_transf=4, num_layers_transf=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # (LSTM)\n",
    "\n",
    "        # Boolean activations decoder (CNN/MLP)\n",
    "        self.acts_decoder = nn.Linear(d_z, n_tracks*resolution)\n",
    "\n",
    "        # GNN\n",
    "        self.graph_decoder = GCN(dropout=dropout)\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        self.embedding = nn.Linear(d_token, d_transf)\n",
    "        self.pos_encoder = PositionalEncoding(d_transf, dropout=dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead_transf,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transf_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_layers_transf\n",
    "        )\n",
    "        \n",
    "        # Last linear layer\n",
    "        self.lin = nn.Linear(d_model, 140)\n",
    "\n",
    "\n",
    "    def forward(self, z, x_seq, x_acts, x_graph):\n",
    "\n",
    "        # Compute activations from z\n",
    "        acts_out = self.acts_decoder(z)\n",
    "        acts_out = acts_out.view(x_acts.size())\n",
    "        #print(\"Acts out:\", acts_out.size())\n",
    "\n",
    "        # Initialize node features with z and propagate with GNN\n",
    "        _, counts = torch.unique(x_graph.batch, return_counts=True)\n",
    "        node_features = torch.repeat_interleave(z, counts, axis=0)\n",
    "        print(\"Node features:\", node_features.size())\n",
    "\n",
    "        # Todo: use also edge info\n",
    "        x_graph.x = node_features\n",
    "        node_decs = self.graph_decoder(x_graph)\n",
    "        print(\"Node decodings:\", node_decs.size())\n",
    "        \n",
    "        node_decs = node_decs.repeat(16, 1, 1)\n",
    "        print(\"Tiled node decodings:\", node_decs.size())\n",
    "\n",
    "        # Decode features with transformer decoder\n",
    "        # forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
    "        \n",
    "        # Todo: same embeddings as encoder?\n",
    "        x_seq = x_seq.view(-1, x_seq.size(-2), x_seq.size(-1))\n",
    "        \n",
    "        # Filter silences\n",
    "        x_acts = x_acts.view(-1)\n",
    "        x_seq = x_seq[x_acts.bool()]\n",
    "        \n",
    "        embs = self.embedding(x_seq)\n",
    "        embs = embs.permute(1, 0, 2)\n",
    "        pos_encs = self.pos_encoder(embs)\n",
    "\n",
    "        seq_out = self.transf_decoder(pos_encs, node_decs)\n",
    "        print(\"Seq out:\", seq_out.size())\n",
    "        \n",
    "        seq_out = self.lin(seq_out)\n",
    "        print(\"Seq out after lin:\", seq_out.size())\n",
    "        \n",
    "        # Softmax on first 131 values (pitch), sigmoid on last 9 (dur)\n",
    "        #seq_out[:, :, :131] = F.log_softmax(seq_out[:, :, :131], dim=-1)\n",
    "        #seq_out[:, :, 131:] = torch.sigmoid(seq_out[:, :, 131:])\n",
    "        seq_out = seq_out.permute(1, 0, 2)\n",
    "        seq_out = seq_out.view(x_seq.size())\n",
    "        print(\"Seq out after reshape\", seq_out.size())\n",
    "        \n",
    "\n",
    "        return seq_out, acts_out\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(dropout=dropout)\n",
    "        self.decoder = Decoder(dropout=dropout)\n",
    "    \n",
    "    def forward(self, x_seq, x_acts, x_graph):\n",
    "        \n",
    "        mu, log_var = self.encoder(x_seq, x_acts, x_graph)\n",
    "        #print(\"Mu:\", mu.size())\n",
    "        #print(\"log_var:\", log_var.size())\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        sigma = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(sigma)\n",
    "        #print(\"eps:\", eps.size())\n",
    "        z = mu + eps*sigma\n",
    "        \n",
    "        out = self.decoder(z, x_seq, x_acts, x_graph)\n",
    "        \n",
    "        return out, mu, log_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import copy\n",
    "import time\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class VAETrainer():\n",
    "    \n",
    "    def __init__(self, model, models_path, optimizer, init_lr,\n",
    "                 name=None, lr_scheduler=None, device=torch.device(\"cuda\"), \n",
    "                 print_every=1, save_every=1):\n",
    "        \n",
    "        self.model = model\n",
    "        self.models_path = models_path\n",
    "        self.optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "        self.name = name if name is not None else str(uuid.uuid4())\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.print_every = print_every\n",
    "        self.save_every = save_every\n",
    "        \n",
    "        self.model_path = os.path.join(self.models_path, self.name)\n",
    "        \n",
    "        # Criterions with ignored padding\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.bce_unreduced = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.ce = nn.CrossEntropyLoss(ignore_index=130) #\n",
    "        \n",
    "        # Training stats\n",
    "        self.losses = defaultdict(list)\n",
    "        self.accuracies = defaultdict(list)\n",
    "        self.lrs = []\n",
    "        self.times = []\n",
    "        \n",
    "    \n",
    "    def train(self, trainloader, validloader=None, epochs=1,\n",
    "              early_exit=None):\n",
    "        \n",
    "        n_batches = len(trainloader)\n",
    "\n",
    "        beta = 0 # Todo: _update_params()\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        print(\"Starting training.\\n\")\n",
    "        \n",
    "        start = time.time()\n",
    "        self.times.append(start)\n",
    "        \n",
    "        tot_batches = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            self.cur_epoch = epoch\n",
    "            progress_bar = tqdm(range(n_batches))\n",
    "            \n",
    "            for batch_idx, inputs in enumerate(trainloader):\n",
    "                \n",
    "                self.cur_batch_idx = batch_idx\n",
    "                \n",
    "                # Zero out the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Get the inputs\n",
    "                x_seq, x_acts, x_graph = inputs\n",
    "                x_seq = x_seq.float().to(self.device)\n",
    "                x_acts = x_acts.to(self.device)\n",
    "                x_graph = x_graph.to(self.device)\n",
    "                inputs = (x_seq, x_acts, x_graph)\n",
    "\n",
    "                # Forward pass, get the reconstructions\n",
    "                outputs, mu, log_var = self.model(x_seq, x_acts, x_graph)\n",
    "                \n",
    "                # Compute the backprop loss and other required losses\n",
    "                tot_loss, losses = self._compute_losses(inputs, outputs, mu,\n",
    "                                                         log_var, beta)\n",
    "                \n",
    "                # Backprop and update lr\n",
    "                tot_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if self.lr_scheduler is not None:\n",
    "                    self.lr_scheduler.step()\n",
    "                    \n",
    "                # Update the stats\n",
    "                self._append_losses(losses)\n",
    "                \n",
    "                last_lr = (self.lr_scheduler.get_last_lr()[0] \n",
    "                               if self.lr_scheduler is not None else self.init_lr)\n",
    "                self.lrs.append(last_lr)\n",
    "                \n",
    "                accs = self._compute_accuracies(inputs, outputs)\n",
    "                self._append_accuracies(accs)\n",
    "                \n",
    "                now = time.time()\n",
    "                self.times.append(now)\n",
    "                \n",
    "                # Print stats\n",
    "                if (tot_batches + 1) % self.print_every == 0:\n",
    "                    print(\"Training on batch {}/{} of epoch {}/{} complete.\"\n",
    "                          .format(batch_idx+1, n_batches, epoch+1, epochs))\n",
    "                    self._print_stats()\n",
    "                    #print(\"Tot_loss: {:.4f} acts_loss: {:.4f} \"\n",
    "                          #.format(running_loss/self.print_every, acts_loss), end='')\n",
    "                    #print(\"pitches_loss: {:.4f} dur_loss: {:.4f} kld_loss: {:.4f}\"\n",
    "                          #.format(pitches_loss, dur_loss, kld_loss))\n",
    "                    print(\"\\n----------------------------------------\\n\")\n",
    "                    \n",
    "                # When appropriate, save model and stats on disk\n",
    "                if self.save_every > 0 and (tot_batches + 1) % self.save_every == 0:\n",
    "                    print(\"\\nSaving model to disk...\\n\")\n",
    "                    self._save_model()\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                # Stop prematurely if early_exit is set and reached\n",
    "                if early_exit is not None and (tot_batches + 1) > early_exit:\n",
    "                    break\n",
    "                \n",
    "                tot_batches += 1\n",
    "            \n",
    "\n",
    "        end = time.time()\n",
    "        # Todo: self.__print_time()\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"Training completed in (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "                  .format(int(hours),int(minutes),seconds))\n",
    "        \n",
    "        print(\"Saving model to disk...\")\n",
    "        self._save_model()\n",
    "        \n",
    "        print(\"Model saved.\")\n",
    "        \n",
    "    \n",
    "    def _compute_losses(self, inputs, outputs, mu, log_var, beta):\n",
    "        \n",
    "        x_seq, x_acts, _ = inputs\n",
    "        seq_rec, acts_rec = outputs\n",
    "        \n",
    "        # Shift outputs for transformer decoder loss\n",
    "        x_seq = x_seq[..., 1:, :]\n",
    "        x_seq = x_seq[x_acts.bool()]\n",
    "        print(x_seq.size())\n",
    "        print(seq_rec.size())\n",
    "        \n",
    "        pitches_true = torch.argmax(x_seq[..., :131], dim=-1)\n",
    "        mask = (pitches_true != 130).unsqueeze(-1)\n",
    "                \n",
    "        # Compute the losses\n",
    "        acts_loss = self.bce(acts_rec.view(-1), x_acts.view(-1).float())\n",
    "        pitches_loss = self.ce(seq_rec.reshape(-1, seq_rec.size(-1))[:, :131],\n",
    "                          x_seq.reshape(-1, x_seq.size(-1))[:, :131].argmax(dim=1))\n",
    "        dur_loss = self.bce_unreduced(seq_rec[..., 131:], \n",
    "                       x_seq[..., 131:])\n",
    "        dur_loss = mask * dur_loss\n",
    "        dur_loss = torch.sum(dur_loss) / torch.sum(mask)\n",
    "        kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        rec_loss = pitches_loss + dur_loss + acts_loss\n",
    "        tot_loss = rec_loss + beta*kld_loss\n",
    "        \n",
    "        losses = {\n",
    "            'tot': tot_loss.item(),\n",
    "            'pitches': pitches_loss.item(),\n",
    "            'dur': dur_loss.item(),\n",
    "            'acts': acts_loss.item(),\n",
    "            'rec': rec_loss.item(),\n",
    "            'kld': kld_loss.item(),\n",
    "            'beta*kld': beta*kld_loss.item()\n",
    "        }\n",
    "        \n",
    "        return tot_loss, losses\n",
    "\n",
    "    \n",
    "    def _append_losses(self, losses):\n",
    "        \n",
    "        for k, loss in losses.items():\n",
    "            self.losses[k].append(loss)\n",
    "            \n",
    "            \n",
    "    def _compute_accuracies(self, inputs, outputs):\n",
    "        \n",
    "        x_seq, x_acts, _ = inputs\n",
    "        seq_rec, acts_rec = outputs\n",
    "        \n",
    "        notes_acc = self._note_accuracy(seq_rec, x_seq)\n",
    "        pitches_acc = self._pitches_accuracy(seq_rec, x_seq)\n",
    "        dur_acc = self._dur_accuracy(seq_rec, x_seq)\n",
    "        acts_acc = self._acts_accuracy(acts_rec, x_acts)\n",
    "        \n",
    "        accs = {\n",
    "            'notes': notes_acc.item(),\n",
    "            'pitches': pitches_acc.item(),\n",
    "            'dur': dur_acc.item(),\n",
    "            'acts': acts_acc.item()\n",
    "        }\n",
    "        \n",
    "        return accs\n",
    "        \n",
    "        \n",
    "    def _append_accuracies(self, accs):\n",
    "        \n",
    "        for k, acc in accs.items():\n",
    "            self.accuracies[k].append(acc)\n",
    "    \n",
    "    \n",
    "    def _note_accuracy(self, seq_rec, x_seq):\n",
    "        \n",
    "        pitches_rec = F.softmax(seq_rec[..., :131], dim=-1)\n",
    "        pitches_rec = torch.argmax(pitches_rec, dim=-1)\n",
    "        pitches_true = torch.argmax(x_seq[..., :131], dim=-1)\n",
    "        \n",
    "        print(torch.all(pitches_rec == 130))\n",
    "        print(pitches_rec)\n",
    "        \n",
    "        mask = (pitches_true != 130)\n",
    "        #mask = torch.logical_and(pitches_true != 128,\n",
    "         #                        pitches_true != 129)\n",
    "        #mask = torch.logical_and(mask,\n",
    "         #                        pitches_true != 130)\n",
    "        \n",
    "        preds_pitches = (pitches_rec == pitches_true)\n",
    "        preds_pitches = torch.logical_and(preds_pitches, mask)\n",
    "        \n",
    "        dur_rec = torch.sigmoid(seq_rec[..., 131:])\n",
    "        dur_rec[dur_rec < 0.5] = 0\n",
    "        dur_rec[dur_rec >= 0.5] = 1\n",
    "        \n",
    "        print(torch.all(dur_rec == 0))\n",
    "        print(dur_rec)\n",
    "        \n",
    "        preds_dur = torch.all(dur_rec == x_seq[..., 131:], dim=-1)\n",
    "        preds_dur = torch.logical_and(preds_dur, mask)\n",
    "        \n",
    "        return torch.sum(torch.logical_and(preds_pitches, \n",
    "                                           preds_dur)) / torch.sum(mask)\n",
    "    \n",
    "    \n",
    "    def _acts_accuracy(self, acts_rec, x_acts):\n",
    "        \n",
    "        acts_rec = torch.sigmoid(acts_rec)\n",
    "        acts_rec[acts_rec < 0.5] = 0\n",
    "        acts_rec[acts_rec >= 0.5] = 1\n",
    "        \n",
    "        #print(torch.all(acts_rec == 0))\n",
    "        #print(acts_rec)\n",
    "        \n",
    "        return torch.sum(acts_rec == x_acts) / x_acts.numel()\n",
    "    \n",
    "    \n",
    "    def _pitches_accuracy(self, seq_rec, x_seq):\n",
    "        \n",
    "        pitches_rec = F.softmax(seq_rec[..., :131], dim=-1)\n",
    "        pitches_rec = torch.argmax(pitches_rec, dim=-1)\n",
    "        pitches_true = torch.argmax(x_seq[..., :131], dim=-1)\n",
    "        \n",
    "        mask = (pitches_true != 130)\n",
    "        #mask = torch.logical_and(pitches_true != 128,\n",
    "         #                        pitches_true != 129)\n",
    "        #mask = torch.logical_and(mask,\n",
    "         #                        pitches_true != 130)\n",
    "        \n",
    "        preds_pitches = (pitches_rec == pitches_true)\n",
    "        preds_pitches = torch.logical_and(preds_pitches, mask)\n",
    "        \n",
    "        return torch.sum(preds_pitches) / torch.sum(mask)\n",
    "    \n",
    "    \n",
    "    def _dur_accuracy(self, seq_rec, x_seq):\n",
    "        \n",
    "        pitches_true = torch.argmax(x_seq[..., :131], dim=-1)\n",
    "        \n",
    "        mask = (pitches_true != 130)\n",
    "        #mask = torch.logical_and(pitches_true != 128,\n",
    "         #                        pitches_true != 129)\n",
    "        #mask = torch.logical_and(mask,\n",
    "         #                        pitches_true != 130)\n",
    "        \n",
    "        dur_rec = torch.sigmoid(seq_rec[..., 131:])\n",
    "        dur_rec[dur_rec < 0.5] = 0\n",
    "        dur_rec[dur_rec >= 0.5] = 1\n",
    "        \n",
    "        preds_dur = torch.all(dur_rec == x_seq[..., 131:], dim=-1)\n",
    "        preds_dur = torch.logical_and(preds_dur, mask)\n",
    "        \n",
    "        return torch.sum(preds_dur) / torch.sum(mask)\n",
    "    \n",
    "    \n",
    "    def _save_model(self):\n",
    "        torch.save({\n",
    "            'epoch': self.cur_epoch,\n",
    "            'batch': self.cur_batch_idx,\n",
    "            'save_every': self.save_every,\n",
    "            'lrs': self.lrs,\n",
    "            'losses': self.losses,\n",
    "            'accuracies': self.accuracies,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }, self.model_path)\n",
    "        \n",
    "        \n",
    "    def _print_stats(self):\n",
    "        \n",
    "        hours, rem = divmod(self.times[-1]-self.times[0], 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"Elapsed time from start (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "                  .format(int(hours), int(minutes), seconds))\n",
    "        \n",
    "        avg_lr = mean(self.lrs[-self.print_every:])\n",
    "        \n",
    "        # Take mean of the last non-printed batches for each stat\n",
    "        \n",
    "        avg_losses = {}\n",
    "        for k, l in self.losses.items():\n",
    "            avg_losses[k] = mean(l[-self.print_every:])\n",
    "        \n",
    "        avg_accs = {}\n",
    "        for k, l in self.accuracies.items():\n",
    "            avg_accs[k] = mean(l[-self.print_every:])\n",
    "        \n",
    "        print(\"Losses:\")\n",
    "        print(avg_losses)\n",
    "        print(\"Accuracies:\")\n",
    "        print(avg_accs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"models/\"\n",
    "os.makedirs(models_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3031"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dir = \"data/preprocessed\"\n",
    "dataset = MIDIDataset(ds_dir)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current device idx: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#decive = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Current device idx:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘models/vae’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm models/vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "def print_params(model):\n",
    "    \n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, parameter in model.named_parameters():\n",
    "        \n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "            \n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params += param\n",
    "        \n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model and moving it to the specified device...\n",
      "+----------------------------------------------------------------+------------+\n",
      "|                            Modules                             | Parameters |\n",
      "+----------------------------------------------------------------+------------+\n",
      "|                    encoder.embedding.weight                    |   35840    |\n",
      "|                     encoder.embedding.bias                     |    256     |\n",
      "| encoder.transformer_encoder.layers.0.self_attn.in_proj_weight  |   196608   |\n",
      "|  encoder.transformer_encoder.layers.0.self_attn.in_proj_bias   |    768     |\n",
      "| encoder.transformer_encoder.layers.0.self_attn.out_proj.weight |   65536    |\n",
      "|  encoder.transformer_encoder.layers.0.self_attn.out_proj.bias  |    256     |\n",
      "|      encoder.transformer_encoder.layers.0.linear1.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.0.linear1.bias        |    2048    |\n",
      "|      encoder.transformer_encoder.layers.0.linear2.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.0.linear2.bias        |    256     |\n",
      "|       encoder.transformer_encoder.layers.0.norm1.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.0.norm1.bias         |    256     |\n",
      "|       encoder.transformer_encoder.layers.0.norm2.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.0.norm2.bias         |    256     |\n",
      "| encoder.transformer_encoder.layers.1.self_attn.in_proj_weight  |   196608   |\n",
      "|  encoder.transformer_encoder.layers.1.self_attn.in_proj_bias   |    768     |\n",
      "| encoder.transformer_encoder.layers.1.self_attn.out_proj.weight |   65536    |\n",
      "|  encoder.transformer_encoder.layers.1.self_attn.out_proj.bias  |    256     |\n",
      "|      encoder.transformer_encoder.layers.1.linear1.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.1.linear1.bias        |    2048    |\n",
      "|      encoder.transformer_encoder.layers.1.linear2.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.1.linear2.bias        |    256     |\n",
      "|       encoder.transformer_encoder.layers.1.norm1.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.1.norm1.bias         |    256     |\n",
      "|       encoder.transformer_encoder.layers.1.norm2.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.1.norm2.bias         |    256     |\n",
      "|              encoder.graph_encoder.layers.0.bias               |    256     |\n",
      "|           encoder.graph_encoder.layers.0.lin.weight            |   65536    |\n",
      "|              encoder.graph_encoder.layers.1.bias               |    256     |\n",
      "|           encoder.graph_encoder.layers.1.lin.weight            |   65536    |\n",
      "|                    encoder.linear_mu.weight                    |   65536    |\n",
      "|                     encoder.linear_mu.bias                     |    256     |\n",
      "|                 encoder.linear_log_var.weight                  |   65536    |\n",
      "|                  encoder.linear_log_var.bias                   |    256     |\n",
      "|                  decoder.acts_decoder.weight                   |   32768    |\n",
      "|                   decoder.acts_decoder.bias                    |    128     |\n",
      "|              decoder.graph_decoder.layers.0.bias               |    256     |\n",
      "|           decoder.graph_decoder.layers.0.lin.weight            |   65536    |\n",
      "|              decoder.graph_decoder.layers.1.bias               |    256     |\n",
      "|           decoder.graph_decoder.layers.1.lin.weight            |   65536    |\n",
      "|                    decoder.embedding.weight                    |   35840    |\n",
      "|                     decoder.embedding.bias                     |    256     |\n",
      "|    decoder.transf_decoder.layers.0.self_attn.in_proj_weight    |   196608   |\n",
      "|     decoder.transf_decoder.layers.0.self_attn.in_proj_bias     |    768     |\n",
      "|   decoder.transf_decoder.layers.0.self_attn.out_proj.weight    |   65536    |\n",
      "|    decoder.transf_decoder.layers.0.self_attn.out_proj.bias     |    256     |\n",
      "| decoder.transf_decoder.layers.0.multihead_attn.in_proj_weight  |   196608   |\n",
      "|  decoder.transf_decoder.layers.0.multihead_attn.in_proj_bias   |    768     |\n",
      "| decoder.transf_decoder.layers.0.multihead_attn.out_proj.weight |   65536    |\n",
      "|  decoder.transf_decoder.layers.0.multihead_attn.out_proj.bias  |    256     |\n",
      "|         decoder.transf_decoder.layers.0.linear1.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.0.linear1.bias          |    2048    |\n",
      "|         decoder.transf_decoder.layers.0.linear2.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.0.linear2.bias          |    256     |\n",
      "|          decoder.transf_decoder.layers.0.norm1.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.0.norm1.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.0.norm2.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.0.norm2.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.0.norm3.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.0.norm3.bias           |    256     |\n",
      "|    decoder.transf_decoder.layers.1.self_attn.in_proj_weight    |   196608   |\n",
      "|     decoder.transf_decoder.layers.1.self_attn.in_proj_bias     |    768     |\n",
      "|   decoder.transf_decoder.layers.1.self_attn.out_proj.weight    |   65536    |\n",
      "|    decoder.transf_decoder.layers.1.self_attn.out_proj.bias     |    256     |\n",
      "| decoder.transf_decoder.layers.1.multihead_attn.in_proj_weight  |   196608   |\n",
      "|  decoder.transf_decoder.layers.1.multihead_attn.in_proj_bias   |    768     |\n",
      "| decoder.transf_decoder.layers.1.multihead_attn.out_proj.weight |   65536    |\n",
      "|  decoder.transf_decoder.layers.1.multihead_attn.out_proj.bias  |    256     |\n",
      "|         decoder.transf_decoder.layers.1.linear1.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.1.linear1.bias          |    2048    |\n",
      "|         decoder.transf_decoder.layers.1.linear2.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.1.linear2.bias          |    256     |\n",
      "|          decoder.transf_decoder.layers.1.norm1.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.1.norm1.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.1.norm2.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.1.norm2.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.1.norm3.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.1.norm3.bias           |    256     |\n",
      "|                       decoder.lin.weight                       |   35840    |\n",
      "|                        decoder.lin.bias                        |    140     |\n",
      "+----------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 6323468\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Starting training.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69551ff1a2d4a4bbc092b419ef4f637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features: torch.Size([259, 256])\n",
      "Node decodings: torch.Size([259, 256])\n",
      "Tiled node decodings: torch.Size([16, 259, 256])\n",
      "Seq out: torch.Size([16, 259, 256])\n",
      "Seq out after lin: torch.Size([16, 259, 140])\n",
      "Seq out after reshape torch.Size([259, 16, 140])\n",
      "torch.Size([259, 15, 140])\n",
      "torch.Size([259, 16, 140])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (4144) to match target batch_size (3885).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_123146/2637439172.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_123146/317864102.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainloader, validloader, epochs, early_exit)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# Compute the backprop loss and other required losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 tot_loss, losses = self._compute_losses(inputs, outputs, mu,\n\u001b[0;32m---> 80\u001b[0;31m                                                          log_var, beta)\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;31m# Backprop and update lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_123146/317864102.py\u001b[0m in \u001b[0;36m_compute_losses\u001b[0;34m(self, inputs, outputs, mu, log_var, beta)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0macts_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macts_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_acts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         pitches_loss = self.ce(seq_rec.reshape(-1, seq_rec.size(-1))[:, :131],\n\u001b[0;32m--> 156\u001b[0;31m                           x_seq.reshape(-1, x_seq.size(-1))[:, :131].argmax(dim=1))\n\u001b[0m\u001b[1;32m    157\u001b[0m         dur_loss = self.bce_unreduced(seq_rec[..., 131:], \n\u001b[1;32m    158\u001b[0m                        x_seq[..., 131:])\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 962\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2261\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 2262\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2264\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (4144) to match target batch_size (3885)."
     ]
    }
   ],
   "source": [
    "print(\"Creating the model and moving it to the specified device...\")\n",
    "\n",
    "vae = VAE().to(device)\n",
    "print_params(vae)\n",
    "print()\n",
    "\n",
    "init_lr = 1e-5\n",
    "gamma = 0.999\n",
    "optimizer = optim.Adam(vae.parameters(), lr=init_lr)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "\n",
    "print('--------------------------------------------------\\n')\n",
    "\n",
    "trainer = VAETrainer(\n",
    "    vae,\n",
    "    models_path,\n",
    "    optimizer,\n",
    "    init_lr,\n",
    "    name='vae',\n",
    "    lr_scheduler=scheduler,\n",
    "    save_every=100, \n",
    "    device=device\n",
    ")\n",
    "trainer.train(loader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('models/vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = checkpoint['losses']\n",
    "\n",
    "#plt.plot(range(1, len(losses['tot'])+1), losses['tot'], label='Loss')\n",
    "plt.plot(range(1, len(losses['acts'])+1), losses['acts'], label='acts')\n",
    "#plt.plot(range(1, len(losses['pitches'])+1), losses['pitches'], label='pitches')\n",
    "#plt.plot(range(1, len(losses['dur'])+1), losses['dur'], label='dur')\n",
    "#plt.plot(range(1, len(losses['kld'])+1), losses['kld'], label='kld')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = checkpoint['accuracies']\n",
    "plt.plot(range(1, len(accs['notes'])+1), accs['notes'], label='notes')\n",
    "plt.plot(range(1, len(accs['acts'])+1), accs['acts'], label='acts')\n",
    "plt.plot(range(1, len(accs['pitches'])+1), accs['pitches'], label='pitches')\n",
    "plt.plot(range(1, len(accs['dur'])+1), accs['dur'], label='dur')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(checkpoint['lrs'])+1), checkpoint['lrs'], label='lr')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0].size()\n",
    "notes = []\n",
    "notes.append(muspy.Note(1, 48, 20, 64))\n",
    "drums = muspy.Track(is_drum=True)\n",
    "bass = muspy.Track(program=34, notes=notes)\n",
    "guitar = muspy.Track(program=26, notes=notes)\n",
    "strings = muspy.Track(program=41)\n",
    "\n",
    "tracks = [drums, bass, guitar, strings]\n",
    "\n",
    "meta = muspy.Metadata(title='gay')\n",
    "music = muspy.Music(tracks=tracks, metadata=meta, resolution=32)\n",
    "muspy.show_pianoroll(music)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTFN-DCJjZWM"
   },
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sSvVK7CxjV8"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter as Param\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter\n",
    "from torch_sparse import SparseTensor, matmul, masked_select_nnz\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (Tensor, Tensor) -> Tensor\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (SparseTensor, Tensor) -> SparseTensor\n",
    "    pass\n",
    "\n",
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return masked_select_nnz(edge_index, edge_mask, layout='coo')\n",
    "\n",
    "\n",
    "class RGCNConv(MessagePassing):\n",
    "    r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "    Relational Data with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}_{\\textrm{root}} \\cdot\n",
    "        \\mathbf{x}_i + \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
    "        \\frac{1}{|\\mathcal{N}_r(i)|} \\mathbf{\\Theta}_r \\cdot \\mathbf{x}_j,\n",
    "\n",
    "    where :math:`\\mathcal{R}` denotes the set of relations, *i.e.* edge types.\n",
    "    Edge type needs to be a one-dimensional :obj:`torch.long` tensor which\n",
    "    stores a relation identifier\n",
    "    :math:`\\in \\{ 0, \\ldots, |\\mathcal{R}| - 1\\}` for each edge.\n",
    "\n",
    "    .. note::\n",
    "        This implementation is as memory-efficient as possible by iterating\n",
    "        over each individual relation type.\n",
    "        Therefore, it may result in low GPU utilization in case the graph has a\n",
    "        large number of relations.\n",
    "        As an alternative approach, :class:`FastRGCNConv` does not iterate over\n",
    "        each individual type, but may consume a large amount of memory to\n",
    "        compensate.\n",
    "        We advise to check out both implementations to see which one fits your\n",
    "        needs.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\n",
    "            corresponds to the sizes of source and target dimensionalities.\n",
    "            In case no input features are given, this argument should\n",
    "            correspond to the number of nodes in your graph.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_relations (int): Number of relations.\n",
    "        num_bases (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the basis-decomposition regularization scheme where\n",
    "            :obj:`num_bases` denotes the number of bases to use.\n",
    "            (default: :obj:`None`)\n",
    "        num_blocks (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the block-diagonal-decomposition regularization scheme where\n",
    "            :obj:`num_blocks` denotes the number of blocks to use.\n",
    "            (default: :obj:`None`)\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
    "            (default: :obj:`\"mean\"`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        num_relations: int,\n",
    "        num_bases: Optional[int] = None,\n",
    "        num_blocks: Optional[int] = None,\n",
    "        aggr: str = 'mean',\n",
    "        root_weight: bool = True,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(aggr=aggr, node_dim=0, **kwargs)\n",
    "\n",
    "        if num_bases is not None and num_blocks is not None:\n",
    "            raise ValueError('Can not apply both basis-decomposition and '\n",
    "                             'block-diagonal-decomposition at the same time.')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_relations = num_relations\n",
    "        self.num_bases = num_bases\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "        self.in_channels_l = in_channels[0]\n",
    "\n",
    "        if num_bases is not None:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_bases, in_channels[0], out_channels))\n",
    "            self.comp = Parameter(torch.Tensor(num_relations, num_bases))\n",
    "\n",
    "        elif num_blocks is not None:\n",
    "            assert (in_channels[0] % num_blocks == 0\n",
    "                    and out_channels % num_blocks == 0)\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, num_blocks,\n",
    "                             in_channels[0] // num_blocks,\n",
    "                             out_channels // num_blocks))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        else:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, in_channels[0], out_channels))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        if root_weight:\n",
    "            self.root = Param(torch.Tensor(in_channels[1], out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Param(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        glorot(self.comp)\n",
    "        glorot(self.root)\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
    "                edge_index: Adj, edge_type: OptTensor = None):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x: The input node features. Can be either a :obj:`[num_nodes,\n",
    "                in_channels]` node feature matrix, or an optional\n",
    "                one-dimensional node index tensor (in which case input features\n",
    "                are treated as trainable node embeddings).\n",
    "                Furthermore, :obj:`x` can be of type :obj:`tuple` denoting\n",
    "                source and destination node features.\n",
    "            edge_type: The one-dimensional relation type/index for each edge in\n",
    "                :obj:`edge_index`.\n",
    "                Should be only :obj:`None` in case :obj:`edge_index` is of type\n",
    "                :class:`torch_sparse.tensor.SparseTensor`.\n",
    "                (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert input features to a pair of node features or node indices.\n",
    "        x_l: OptTensor = None\n",
    "        if isinstance(x, tuple):\n",
    "            x_l = x[0]\n",
    "        else:\n",
    "            x_l = x\n",
    "        if x_l is None:\n",
    "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
    "\n",
    "        x_r: Tensor = x_l\n",
    "        if isinstance(x, tuple):\n",
    "            x_r = x[1]\n",
    "\n",
    "        size = (x_l.size(0), x_r.size(0))\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_type = edge_index.storage.value()\n",
    "        assert edge_type is not None\n",
    "\n",
    "        # propagate_type: (x: Tensor)\n",
    "        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)\n",
    "\n",
    "        weight = self.weight\n",
    "        if self.num_bases is not None:  # Basis-decomposition =================\n",
    "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
    "                self.num_relations, self.in_channels_l, self.out_channels)\n",
    "\n",
    "        if self.num_blocks is not None:  # Block-diagonal-decomposition =====\n",
    "\n",
    "            if x_l.dtype == torch.long and self.num_blocks is not None:\n",
    "                raise ValueError('Block-diagonal decomposition not supported '\n",
    "                                 'for non-continuous input features.')\n",
    "\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                h = self.propagate(tmp, x=x_l, size=size)\n",
    "                h = h.view(-1, weight.size(1), weight.size(2))\n",
    "                h = torch.einsum('abc,bcd->abd', h, weight[i])\n",
    "                out += h.contiguous().view(-1, self.out_channels)\n",
    "\n",
    "        else:  # No regularization/Basis-decomposition ========================\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "\n",
    "                if x_l.dtype == torch.long:\n",
    "                    out += self.propagate(tmp, x=weight[i, x_l], size=size)\n",
    "                else:\n",
    "                    h = self.propagate(tmp, x=x_l, size=size)\n",
    "                    out = out + (h @ weight[i])\n",
    "\n",
    "        root = self.root\n",
    "        if root is not None:\n",
    "            out += root[x_r] if x_r.dtype == torch.long else x_r @ root\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor) -> Tensor:\n",
    "        return x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        adj_t = adj_t.set_value(None, layout=None)\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, num_relations={self.num_relations})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfsNpMLrEXLk"
   },
   "source": [
    "next edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DaVTonr_XB8",
    "outputId": "9e5fa8f9-604e-4273-a34d-fad73ad9ab7e"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "a = np.random.randint(2, size=(4,8))\n",
    "a_t = a.transpose()\n",
    "print(a_t)\n",
    "inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "ts_acts = np.any(a_t, axis=1)\n",
    "ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "labels = np.arange(32).reshape(4, 8).transpose()\n",
    "print(labels)\n",
    "\n",
    "next_edges = []\n",
    "for i in range(len(ts_inds)-1):\n",
    "    ind_s = ts_inds[i]\n",
    "    ind_e = ts_inds[i+1]\n",
    "    s = inds[inds[:,0] == ind_s]\n",
    "    e = inds[inds[:,0] == ind_e]\n",
    "    e_inds = [t for t in list(itertools.product(s, e)) if t[0][1] != t[1][1]]\n",
    "    edges = [(labels[tuple(e[0])],labels[tuple(e[1])], ind_e-ind_s) for e in e_inds]\n",
    "    next_edges.extend(edges)\n",
    "\n",
    "print(next_edges)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ5JQm1aEbmb"
   },
   "source": [
    "onset edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DISmsJB3EatR",
    "outputId": "fc864608-63a6-4ad0-84d9-1478001ce60e"
   },
   "outputs": [],
   "source": [
    "onset_edges = []\n",
    "print(a_t)\n",
    "print(labels)\n",
    "\n",
    "for i in ts_inds:\n",
    "    ts_acts_inds = list(inds[inds[:,0] == i])\n",
    "    if len(ts_acts_inds) < 2:\n",
    "        continue\n",
    "    e_inds = list(itertools.combinations(ts_acts_inds, 2))\n",
    "    edges = [(labels[tuple(e[0])], labels[tuple(e[1])], 0) for e in e_inds]\n",
    "    inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "    onset_edges.extend(edges)\n",
    "    onset_edges.extend(inv_edges)\n",
    "\n",
    "print(onset_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujitZCKaa7nu"
   },
   "source": [
    "track edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbVG1vdFa-7e",
    "outputId": "c042449b-eef2-4707-a524-5f66f3ec07c7"
   },
   "outputs": [],
   "source": [
    "print(a_t)\n",
    "print(labels)\n",
    "track_edges = []\n",
    "\n",
    "for track in range(a_t.shape[1]):\n",
    "    tr_inds = list(inds[inds[:,1] == track])\n",
    "    e_inds = [(tr_inds[i],\n",
    "               tr_inds[i+1]) for i in range(len(tr_inds)-1)]\n",
    "    print(e_inds)\n",
    "    edges = [(labels[tuple(e[0])], labels[tuple(e[1])], e[1][0]-e[0][0]) for e in e_inds]\n",
    "    track_edges.extend(edges)\n",
    "\n",
    "print(track_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DzouJ5NqALB",
    "outputId": "20a76e82-6305-4154-d894-6d69a64435a1"
   },
   "outputs": [],
   "source": [
    "track_edges = np.array(track_edges)\n",
    "onset_edges = np.array(onset_edges)\n",
    "np.concatenate((track_edges, onset_edges)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihIYkPWPzyGX"
   },
   "outputs": [],
   "source": [
    "pip install pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie0pU8NWAUNM"
   },
   "outputs": [],
   "source": [
    "import pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTbGBSrdAZGH"
   },
   "outputs": [],
   "source": [
    "multitrack = pypianoroll.read(\"tests_fur-elise.mid\")\n",
    "print(multitrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eVo_BKzAmz4"
   },
   "outputs": [],
   "source": [
    "multitrack.tracks[0].pianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPpWw-rLA7CI"
   },
   "outputs": [],
   "source": [
    "multitrack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-PYbS7FA-Gg"
   },
   "outputs": [],
   "source": [
    "multitrack.trim(0, 12 * multitrack.resolution)\n",
    "multitrack.binarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psxuoTsZBFXY"
   },
   "outputs": [],
   "source": [
    "multitrack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovyixmSvBG3w"
   },
   "outputs": [],
   "source": [
    "multitrack.tracks[0].pianoroll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHlKNufuBzLn"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOhjCOJb34P4bTid7qFDg58",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1NeVldMsPVJd6pXbxZDmuiUP-QJBRhYtj",
   "name": "midi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
