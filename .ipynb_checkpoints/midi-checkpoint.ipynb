{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmanueleCosenza/Polyphemus/blob/main/midi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "50lpUn9bO0ug",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cosenza/thesis/Polyphemus\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -C data -xvzf data/lmd_matched.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "She0QbN5Kopo",
    "outputId": "0f3fb4c7-bd7d-4ee4-b2cd-d567d8e490db",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install the required music libraries\n",
    "#!pip3 install muspy\n",
    "#!pip3 install pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uveQkY7O0CF",
    "outputId": "12e1f638-ee78-4617-844a-10e9a26c298e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install torch_geometric\n",
    "#!v=$(python3 -c \"import torch; print(torch.__version__)\"); \\\n",
    "#pip3 install torch-scatter -f https://data.pyg.org/whl/torch-${v}.html; \\\n",
    "#pip3 install torch-sparse -f https://data.pyg.org/whl/torch-${v}.html; \\\n",
    "#pip3 install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B45l1513wJ1Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import muspy\n",
    "from itertools import product\n",
    "import pypianoroll as pproll\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class MIDIPreprocessor():\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def preprocess_dataset(self, dir, early_exit=None):\n",
    "        pass\n",
    "    \n",
    "    def preprocess_file(self, f):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Todo: to config file (or separate files)\n",
    "MAX_SIMU_NOTES = 16 # 14 + SOS and EOS\n",
    "\n",
    "PITCH_SOS = 128\n",
    "PITCH_EOS = 129\n",
    "PITCH_PAD = 130\n",
    "DUR_PAD_IND = 2\n",
    "MAX_DUR = 511 # equivalent to 16 bars (with RESOLUTION=32)\n",
    "\n",
    "RESOLUTION = 32\n",
    "NUM_BARS = 1\n",
    "\n",
    "\n",
    "def preprocess_file(filepath, dest_dir, num_samples):\n",
    "\n",
    "    saved_samples = 0\n",
    "\n",
    "    print(\"Preprocessing file \" + filepath)\n",
    "\n",
    "    # Load the file both as a pypianoroll song and a muspy song\n",
    "    # (Need to load both since muspy.to_pypianoroll() is expensive)\n",
    "    try:\n",
    "        pproll_song = pproll.read(filepath, resolution=RESOLUTION)\n",
    "        muspy_song = muspy.read(filepath)\n",
    "    except Exception as e:\n",
    "        print(\"Song skipped (Invalid song format)\")\n",
    "        return 0\n",
    "    \n",
    "    # Only accept songs that have a time signature of 4/4 and no time changes\n",
    "    for t in muspy_song.time_signatures:\n",
    "        if t.numerator != 4 or t.denominator != 4:\n",
    "            print(\"Song skipped ({}/{} time signature)\".\n",
    "                            format(t.numerator, t.denominator))\n",
    "            return 0\n",
    "\n",
    "    # Gather tracks of pypianoroll song based on MIDI program number\n",
    "    drum_tracks = []\n",
    "    bass_tracks = []\n",
    "    guitar_tracks = []\n",
    "    strings_tracks = []\n",
    "\n",
    "    for track in pproll_song.tracks:\n",
    "        if track.is_drum:\n",
    "            track.name = 'Drums'\n",
    "            drum_tracks.append(track)\n",
    "        elif 0 <= track.program <= 31:\n",
    "            track.name = 'Guitar'\n",
    "            guitar_tracks.append(track)\n",
    "        elif 32 <= track.program <= 39:\n",
    "            track.name = 'Bass'\n",
    "            bass_tracks.append(track)\n",
    "        else:\n",
    "            # Tracks with program > 39 are all considered as strings tracks\n",
    "            # and will be merged into a single track later on\n",
    "            strings_tracks.append(track)\n",
    "\n",
    "    # Filter song if it does not contain drum, guitar, bass or strings tracks\n",
    "    if not drum_tracks or not guitar_tracks \\\n",
    "       or not bass_tracks or not strings_tracks:\n",
    "        print(\"Song skipped (does not contain drum or \"\n",
    "                \"guitar or bass or strings tracks)\")\n",
    "        return 0\n",
    "    \n",
    "    # Merge strings tracks into a single pypianoroll track\n",
    "    strings = pproll.Multitrack(tracks=strings_tracks)\n",
    "    strings_track = pproll.Track(pianoroll=strings.blend(mode='max'),\n",
    "                                 program=48, name='Strings')\n",
    "\n",
    "    combinations = list(product(drum_tracks, bass_tracks, guitar_tracks))\n",
    "\n",
    "    # Single instruments can have multiple tracks.\n",
    "    # Consider all possible combinations of drum, bass, and guitar tracks\n",
    "    for i, combination in enumerate(combinations):\n",
    "\n",
    "        print(\"Processing combination\", i+1, \"of\", len(combinations))\n",
    "        \n",
    "        # Process combination (called 'subsong' from now on)\n",
    "        drum_track, bass_track, guitar_track = combination\n",
    "        tracks = [drum_track, bass_track, guitar_track, strings_track]\n",
    "        \n",
    "        pproll_subsong = pproll.Multitrack(\n",
    "            tracks=tracks,\n",
    "            tempo=pproll_song.tempo,\n",
    "            resolution=RESOLUTION\n",
    "        )\n",
    "        muspy_subsong = muspy.from_pypianoroll(pproll_subsong)\n",
    "        \n",
    "        tracks_notes = [track.notes for track in muspy_subsong.tracks]\n",
    "        \n",
    "        # Obtain length of subsong (maximum of each track's length)\n",
    "        length = 0\n",
    "        for notes in tracks_notes:\n",
    "            track_length = max(note.end for note in notes) if notes else 0\n",
    "            length = max(length, track_length)\n",
    "        length += 1\n",
    "\n",
    "        # Add timesteps until length is a multiple of RESOLUTION\n",
    "        length = length if length%(RESOLUTION) == 0 \\\n",
    "                                else length + (RESOLUTION-(length%(RESOLUTION)))\n",
    "\n",
    "\n",
    "        tracks_tensors = []\n",
    "        tracks_activations = []\n",
    "\n",
    "        dur_bin_length = int(np.ceil(np.log2(MAX_DUR)))\n",
    "\n",
    "        # Todo: adapt to velocity\n",
    "        for notes in tracks_notes:\n",
    "\n",
    "            # Initialize encoder-ready track tensor\n",
    "            # track_tensor: (length x max_simu_notes x 2 (or 3 if velocity))\n",
    "            # The last dimension contains pitches and durations (and velocities)\n",
    "            # int16 is enough for small to medium duration values\n",
    "            track_tensor = np.zeros((length, MAX_SIMU_NOTES, 2), np.int16)\n",
    "\n",
    "            track_tensor[:, :, 0] = PITCH_PAD\n",
    "            track_tensor[:, 0, 0] = PITCH_SOS\n",
    "\n",
    "            # Keeps track of how many notes have been stored in each timestep\n",
    "            # (int8 imposes that MAX_SIMU_NOTES < 256)\n",
    "            notes_counter = np.ones(length, dtype=np.int8)\n",
    "\n",
    "            # Todo: np.put_along_axis?\n",
    "            for note in notes:\n",
    "                # Insert note in the lowest position available in the timestep\n",
    "                \n",
    "                t = note.time\n",
    "\n",
    "                if notes_counter[t] >= MAX_SIMU_NOTES-1:\n",
    "                    # Skip note if there is no more space\n",
    "                    continue\n",
    "\n",
    "                track_tensor[t, notes_counter[t], 0] = note.pitch\n",
    "                track_tensor[t, notes_counter[t], 1] = note.duration\n",
    "                notes_counter[t] += 1\n",
    "            \n",
    "            # Add end of sequence token\n",
    "            track_tensor[np.arange(0, length), notes_counter, 0] = PITCH_EOS\n",
    "\n",
    "            # Get track activations, a boolean tensor indicating whether notes\n",
    "            # are being played in a timestep (sustain does not count)\n",
    "            # (needed for graph rep.)\n",
    "            activations = np.array(notes_counter-1, dtype=bool)\n",
    "\n",
    "            tracks_tensors.append(track_tensor)\n",
    "            tracks_activations.append(activations)\n",
    "        \n",
    "        # (#tracks x length x max_simu_notes x 2 (or 3))\n",
    "        subsong_tensor = np.stack(tracks_tensors, axis=0)\n",
    "\n",
    "        # (#tracks x length)\n",
    "        subsong_activations = np.stack(tracks_activations, axis=0)\n",
    "\n",
    "\n",
    "        # Slide window over 'subsong_tensor' and 'subsong_activations' along the\n",
    "        # time axis (2nd dimension) with the stride of a bar\n",
    "        # Todo: np.lib.stride_tricks.as_strided(song_proll)\n",
    "        for i in range(0, length-NUM_BARS*RESOLUTION+1, RESOLUTION):\n",
    "            \n",
    "            # Get the sequence and its activations\n",
    "            seq_tensor = subsong_tensor[:, i:i+NUM_BARS*RESOLUTION, :]\n",
    "            seq_acts = subsong_activations[:, i:i+NUM_BARS*RESOLUTION]\n",
    "\n",
    "            if NUM_BARS > 1:\n",
    "                # Skip sequence if it contains more than one bar of consecutive\n",
    "                # silence in at least one track\n",
    "                bars = seq_acts.reshape(seq_acts.shape[0], NUM_BARS, -1)\n",
    "                bars_acts = np.any(bars, axis=2)\n",
    "\n",
    "                if 1 in np.diff(np.where(bars_acts == 0)[1]):\n",
    "                    continue\n",
    "            else:\n",
    "                # In the case of just 1 bar, skip it if all tracks are silenced\n",
    "                bar_acts = np.any(seq_acts, axis=1)\n",
    "                if not np.any(bar_acts):\n",
    "                    continue\n",
    "                \n",
    "\n",
    "            # Randomly transpose the pitches of the sequence (-5 to 6 semitones)\n",
    "            shift = np.random.choice(np.arange(-5, 7), 1)\n",
    "            cond = (seq_tensor[:, :, :, 0] != PITCH_PAD) &                     \\\n",
    "                   (seq_tensor[:, :, :, 0] != PITCH_SOS) &                     \\\n",
    "                   (seq_tensor[:, :, :, 0] != PITCH_EOS)\n",
    "            seq_tensor[cond, 0] += shift\n",
    "\n",
    "            # Save sample (seq_tensor and seq_acts) to file\n",
    "            curr_sample = str(num_samples + saved_samples)\n",
    "            sample_filepath = os.path.join(dest_dir, curr_sample)\n",
    "            np.savez(sample_filepath, seq_tensor=seq_tensor, seq_acts=seq_acts)\n",
    "\n",
    "            saved_samples += 1\n",
    "\n",
    "\n",
    "    print(\"File preprocessing finished. Saved samples:\", saved_samples)\n",
    "    print()\n",
    "\n",
    "    return saved_samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Total number of files: 116189\n",
    "# Number of unique files: 45129\n",
    "def preprocess_dataset(dataset_dir, dest_dir, num_files=45129, early_exit=None):\n",
    "\n",
    "    files_dict = {}\n",
    "    seen = 0\n",
    "    tot_samples = 0\n",
    "    not_filtered = 0\n",
    "    finished = False\n",
    "    \n",
    "    print(\"Starting preprocessing\")\n",
    "    \n",
    "    progress_bar = tqdm(range(early_exit)) if early_exit is not None else tqdm(range(num_files))\n",
    "    start = time.time()\n",
    "\n",
    "    # Visit recursively the directories inside the dataset directory\n",
    "    for dirpath, dirs, files in os.walk(dataset_dir):\n",
    "\n",
    "        # Sort alphabetically the found directories\n",
    "        # (to help guess the remaining time) \n",
    "        dirs.sort()\n",
    "        \n",
    "        print(\"Current path:\", dirpath)\n",
    "        print()\n",
    "\n",
    "        for f in files:\n",
    "            \n",
    "            seen += 1\n",
    "\n",
    "            if f in files_dict:\n",
    "                # Skip already seen file\n",
    "                files_dict[f] += 1\n",
    "                continue\n",
    "\n",
    "            # File never seen before, add to dictionary of files\n",
    "            # (from filename to # of occurrences)\n",
    "            files_dict[f] = 1\n",
    "\n",
    "            # Preprocess file\n",
    "            filepath = os.path.join(dirpath, f)\n",
    "            n_saved = preprocess_file(filepath, dest_dir, tot_samples)\n",
    "\n",
    "            tot_samples += n_saved\n",
    "            if n_saved > 0:\n",
    "                not_filtered += 1\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            # Todo: also print # of processed (not filtered) files\n",
    "            #       and # of produced sequences (samples)\n",
    "            print(\"Total number of seen files:\", seen)\n",
    "            print(\"Number of unique files:\", len(files_dict))\n",
    "            print(\"Total number of non filtered songs:\", not_filtered)\n",
    "            print(\"Total number of saved samples:\", tot_samples)\n",
    "            print()\n",
    "\n",
    "            # Exit when a maximum number of files has been processed (if set)\n",
    "            if early_exit != None and len(files_dict) >= early_exit:\n",
    "                finished = True\n",
    "                break\n",
    "\n",
    "        if finished:\n",
    "            break\n",
    "    \n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"Preprocessing completed in (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "              .format(int(hours),int(minutes),seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aYc5y-CYyetK"
   },
   "outputs": [],
   "source": [
    "!rm -rf data/preprocessed/\n",
    "!mkdir data/preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqnubg3oP4ES",
    "outputId": "40cc38a2-1f7d-4f6f-e6c9-9e14dfc7f683",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'data/lmd_matched'\n",
    "dest_dir = 'data/preprocessed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG88mekfrrcp"
   },
   "source": [
    "Check preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687850285b484d799f29ef7a086c6aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current path: data/lmd_matched\n",
      "\n",
      "Current path: data/lmd_matched/A\n",
      "\n",
      "Current path: data/lmd_matched/A/A\n",
      "\n",
      "Current path: data/lmd_matched/A/A/A\n",
      "\n",
      "Current path: data/lmd_matched/A/A/A/TRAAAGR128F425B14B\n",
      "\n",
      "Preprocessing file data/lmd_matched/A/A/A/TRAAAGR128F425B14B/1d9d16a9da90c090809c153754823c2b.mid\n",
      "Processing combination 1 of 7\n",
      "Processing combination 2 of 7\n",
      "Processing combination 3 of 7\n",
      "Processing combination 4 of 7\n",
      "Processing combination 5 of 7\n",
      "Processing combination 6 of 7\n",
      "Processing combination 7 of 7\n",
      "File preprocessing finished. Saved samples: 3031\n",
      "\n",
      "Total number of seen files: 1\n",
      "Number of unique files: 1\n",
      "Total number of non filtered songs: 1\n",
      "Total number of saved samples: 3031\n",
      "\n",
      "Preprocessing file data/lmd_matched/A/A/A/TRAAAGR128F425B14B/5dd29e99ed7bd3cc0c5177a6e9de22ea.mid\n",
      "Processing combination 1 of 5\n",
      "Processing combination 2 of 5\n",
      "Processing combination 3 of 5\n",
      "Processing combination 4 of 5\n",
      "Processing combination 5 of 5\n",
      "File preprocessing finished. Saved samples: 2105\n",
      "\n",
      "Total number of seen files: 2\n",
      "Number of unique files: 2\n",
      "Total number of non filtered songs: 2\n",
      "Total number of saved samples: 5136\n",
      "\n",
      "Preprocessing file data/lmd_matched/A/A/A/TRAAAGR128F425B14B/dac3cdd0db6341d8dc14641e44ed0d44.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cosenza/anaconda3/envs/thesis/lib/python3.7/site-packages/pretty_midi/pretty_midi.py:101: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing combination 1 of 2\n",
      "Processing combination 2 of 2\n",
      "File preprocessing finished. Saved samples: 1107\n",
      "\n",
      "Total number of seen files: 3\n",
      "Number of unique files: 3\n",
      "Total number of non filtered songs: 3\n",
      "Total number of saved samples: 6243\n",
      "\n",
      "Preprocessing file data/lmd_matched/A/A/A/TRAAAGR128F425B14B/b97c529ab9ef783a849b896816001748.mid\n",
      "Processing combination 1 of 4\n",
      "Processing combination 2 of 4\n",
      "Processing combination 3 of 4\n",
      "Processing combination 4 of 4\n",
      "File preprocessing finished. Saved samples: 1812\n",
      "\n",
      "Total number of seen files: 4\n",
      "Number of unique files: 4\n",
      "Total number of non filtered songs: 4\n",
      "Total number of saved samples: 8055\n",
      "\n",
      "Current path: data/lmd_matched/A/A/A/TRAAAZF12903CCCF6B\n",
      "\n",
      "Preprocessing file data/lmd_matched/A/A/A/TRAAAZF12903CCCF6B/c24989559d170135b9c6546d1d2df20b.mid\n",
      "Processing combination 1 of 16\n",
      "Processing combination 2 of 16\n",
      "Processing combination 3 of 16\n",
      "Processing combination 4 of 16\n",
      "Processing combination 5 of 16\n",
      "Processing combination 6 of 16\n",
      "Processing combination 7 of 16\n",
      "Processing combination 8 of 16\n",
      "Processing combination 9 of 16\n",
      "Processing combination 10 of 16\n",
      "Processing combination 11 of 16\n",
      "Processing combination 12 of 16\n",
      "Processing combination 13 of 16\n",
      "Processing combination 14 of 16\n",
      "Processing combination 15 of 16\n",
      "Processing combination 16 of 16\n",
      "File preprocessing finished. Saved samples: 5089\n",
      "\n",
      "Total number of seen files: 5\n",
      "Number of unique files: 5\n",
      "Total number of non filtered songs: 5\n",
      "Total number of saved samples: 13144\n",
      "\n",
      "Preprocessing completed in (h:m:s): 00:00:11.23\n"
     ]
    }
   ],
   "source": [
    "preprocess_dataset(dataset_dir, dest_dir, early_exit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JlP6iUNugNtP"
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(dest_dir, \"5.npz\")\n",
    "data = np.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VUpOEObhwYQ",
    "outputId": "aac6e029-93b1-485f-f13a-2a00abedbc7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 32, 16, 2)\n",
      "(4, 32)\n"
     ]
    }
   ],
   "source": [
    "print(data[\"seq_tensor\"].shape)\n",
    "print(data[\"seq_acts\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6NA5IAAmtK8",
    "outputId": "6e661b3a-05a1-4e2d-9a3d-e1c037b4d04f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128,   0],\n",
       "       [129,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0],\n",
       "       [130,   0]], dtype=int16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"seq_tensor\"][0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C19X9m-3iMlm"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zymqD-UqR8wq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import itertools\n",
    "\n",
    "\n",
    "def unpackbits(x, num_bits):\n",
    "\n",
    "    if np.issubdtype(x.dtype, np.floating):\n",
    "        raise ValueError(\"numpy data type needs to be int-like\")\n",
    "\n",
    "    xshape = list(x.shape)\n",
    "    x = x.reshape([-1, 1])\n",
    "    mask = 2**np.arange(num_bits, dtype=x.dtype).reshape([1, num_bits])\n",
    "\n",
    "    return (x & mask).astype(bool).astype(int).reshape(xshape + [num_bits])\n",
    "\n",
    "\n",
    "class MIDIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir):\n",
    "        self.dir = dir\n",
    "\n",
    "    def __len__(self):\n",
    "        _, _, files = next(os.walk(self.dir))\n",
    "        return len(files)\n",
    "\n",
    "    \n",
    "    def __get_track_edges(self, acts, edge_type_ind=0):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "\n",
    "        labels = np.arange(acts.shape[0]*acts.shape[1])\n",
    "        labels = labels.reshape(acts.shape[0], acts.shape[1]).transpose()\n",
    "\n",
    "        track_edges = []\n",
    "\n",
    "        for track in range(a_t.shape[1]):\n",
    "            tr_inds = list(inds[inds[:,1] == track])\n",
    "            e_inds = [(tr_inds[i],\n",
    "                    tr_inds[i+1]) for i in range(len(tr_inds)-1)]\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, e[1][0]-e[0][0]) for e in e_inds]\n",
    "            track_edges.extend(edges)\n",
    "\n",
    "        return np.array(track_edges, dtype='long')\n",
    "\n",
    "    \n",
    "    def __get_onset_edges(self, acts, edge_type_ind=1):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        labels = np.arange(acts.shape[0]*acts.shape[1])\n",
    "        labels = labels.reshape(acts.shape[0], acts.shape[1]).transpose()\n",
    "\n",
    "        onset_edges = []\n",
    "\n",
    "        for i in ts_inds:\n",
    "            ts_acts_inds = list(inds[inds[:,0] == i])\n",
    "            if len(ts_acts_inds) < 2:\n",
    "                continue\n",
    "            e_inds = list(itertools.combinations(ts_acts_inds, 2))\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, 0) for e in e_inds]\n",
    "            inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "            onset_edges.extend(edges)\n",
    "            onset_edges.extend(inv_edges)\n",
    "\n",
    "        return np.array(onset_edges, dtype='long')\n",
    "\n",
    "\n",
    "    def __get_next_edges(self, acts, edge_type_ind=2):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        labels = np.arange(acts.shape[0]*acts.shape[1])\n",
    "        labels = labels.reshape(acts.shape[0], acts.shape[1]).transpose()\n",
    "\n",
    "        next_edges = []\n",
    "\n",
    "        for i in range(len(ts_inds)-1):\n",
    "\n",
    "            ind_s = ts_inds[i]\n",
    "            ind_e = ts_inds[i+1]\n",
    "            s = inds[inds[:,0] == ind_s]\n",
    "            e = inds[inds[:,0] == ind_e]\n",
    "\n",
    "            e_inds = [t for t in list(itertools.product(s, e)) if t[0][1] != t[1][1]]\n",
    "            edges = [(labels[tuple(e[0])],labels[tuple(e[1])], edge_type_ind, ind_e-ind_s) for e in e_inds]\n",
    "\n",
    "            next_edges.extend(edges)\n",
    "\n",
    "        return np.array(next_edges, dtype='long')\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Load tensors\n",
    "        sample_path = os.path.join(self.dir, str(idx) + \".npz\")\n",
    "        data = np.load(sample_path)\n",
    "\n",
    "        seq_tensor = data[\"seq_tensor\"]\n",
    "        seq_acts = data[\"seq_acts\"]\n",
    "\n",
    "        # From decimals to one-hot (pitch)\n",
    "        pitches = seq_tensor[:, :, :, 0]\n",
    "        onehot = np.zeros((pitches.shape[0]*pitches.shape[1]*pitches.shape[2],\n",
    "                            131), dtype=float)\n",
    "        onehot[np.arange(0, onehot.shape[0]), pitches.reshape(-1)] = 1.\n",
    "        onehot = onehot.reshape(-1, pitches.shape[1], seq_tensor.shape[2], 131)\n",
    "\n",
    "        # From decimals to binary (pitch)\n",
    "        durs = seq_tensor[:, :, :, 1]\n",
    "        bin_durs = unpackbits(durs, 9)[:, :, :, ::-1]\n",
    "\n",
    "        # Concatenate pitches and durations\n",
    "        new_seq_tensor = np.concatenate((onehot[:, :, :, :], bin_durs),\n",
    "                             axis=-1)\n",
    "        \n",
    "        # Construct graph from boolean activations\n",
    "        track_edges = self.__get_track_edges(seq_acts)\n",
    "        onset_edges = self.__get_onset_edges(seq_acts)\n",
    "        next_edges = self.__get_next_edges(seq_acts)\n",
    "        edges = [track_edges, onset_edges, next_edges]\n",
    "\n",
    "        # Concatenate edge tensors (N x 4) (if any)\n",
    "        no_edges = (len(track_edges) == 0 and \n",
    "                    len(onset_edges) == 0 and len(next_edges) == 0)\n",
    "        if not no_edges:\n",
    "            edge_list = np.concatenate([x for x in edges\n",
    "                                          if x.size > 0])\n",
    "            edge_list = torch.from_numpy(edge_list)\n",
    "        \n",
    "        # Adapt tensor to torch_geometric's Data\n",
    "        # Todo: re-check no edges case\n",
    "        edge_index = (torch.LongTensor([[], []]) if no_edges else\n",
    "                               edge_list[:, :2].t().contiguous())\n",
    "        edge_attr = (torch.Tensor([[0, 0]]) if no_edges else\n",
    "                                       edge_list[:, 2:])\n",
    "        \n",
    "        n = seq_acts.shape[0]*seq_acts.shape[1]\n",
    "        graph = Data(edge_index=edge_index, edge_attr=edge_attr, num_nodes=n)\n",
    "        \n",
    "        # Todo: start with torch at mount\n",
    "        return torch.Tensor(new_seq_tensor), torch.Tensor(seq_acts), graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hSwcnlq4g50O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Todo: check and think about max_len\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 256):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *                     \\\n",
    "                             (-math.log(10000.0)/d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position*div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, features_dims=[256, 256, 256], num_relations=3):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(len(features_dims)-1):\n",
    "            self.layers.append(GCNConv(features_dims[i], features_dims[i+1]))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    # 140 = 128+3+9\n",
    "    def __init__(self, d_token=140, d_transf=256, nhead_transf=4, \n",
    "                 num_layers_transf=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Todo: one separate encoder for drums\n",
    "        # Transformer Encoder\n",
    "        self.embedding = nn.Linear(d_token, d_transf)\n",
    "        self.pos_encoder = PositionalEncoding(d_transf, dropout)\n",
    "        transf_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_transf,\n",
    "            nhead=nhead_transf\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            transf_layer,\n",
    "            num_layers=num_layers_transf\n",
    "        )\n",
    "\n",
    "        # Graph encoder\n",
    "        self.graph_encoder = GCN()\n",
    "\n",
    "        # (LSTM)\n",
    "        \n",
    "        # Linear layers that compute the final mu and log_var\n",
    "        # Todo: as parameters\n",
    "        self.linear_mu = nn.Linear(256, 256)\n",
    "        self.linear_log_var = nn.Linear(256, 256)\n",
    "\n",
    "    def forward(self, x_seq, x_acts, x_graph):\n",
    "\n",
    "        # Collapse track (and optionally batch) dimension\n",
    "        #print(\"Init input:\", x_seq.size())\n",
    "        x_seq = x_seq.view(-1, x_seq.size(-2), x_seq.size(-1))\n",
    "        #print(\"Reshaped input:\", x_seq.size())\n",
    "\n",
    "        # Compute embeddings\n",
    "        embs = self.embedding(x_seq)\n",
    "        #print(\"Embs:\", embs.size())\n",
    "\n",
    "        # batch_first = False\n",
    "        embs = embs.permute(1, 0, 2)\n",
    "        #print(\"Seq len first input:\", embs.size())\n",
    "\n",
    "        pos_encs = self.pos_encoder(embs)\n",
    "        #print(\"Pos encodings:\", pos_encs.size())\n",
    "\n",
    "        # Todo: src_key_padding_mask = (src != pad).unsqueeze(-2) ?\n",
    "        transformer_encs = self.transformer_encoder(pos_encs)\n",
    "        #print(\"Transf encodings:\", transformer_encs.size())\n",
    "\n",
    "        pooled_encs = torch.mean(transformer_encs, 0)\n",
    "        #print(\"Pooled encodings:\", pooled_encs.size())\n",
    "\n",
    "        # Compute node encodings\n",
    "        x_graph.x = pooled_encs\n",
    "        node_encs = self.graph_encoder(x_graph)\n",
    "        #print(\"Node encodings:\", node_encs.size())\n",
    "        \n",
    "        # Compute final graph latent vector(s)\n",
    "        # (taking into account the batch size)\n",
    "        num_nodes = x_graph[0].num_nodes\n",
    "        batch_sz = node_encs.size(0) // num_nodes\n",
    "        node_encs = node_encs.view(batch_sz, num_nodes, -1)\n",
    "        encoding = torch.mean(node_encs, 1)\n",
    "\n",
    "        # Compute mu and log(std^2)\n",
    "        mu = self.linear_mu(encoding)\n",
    "        log_var = self.linear_log_var(encoding)\n",
    "        \n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_z=256, n_tracks=4, resolution=32, d_token=140, d_model=256,\n",
    "                 d_transf=256, nhead_transf=4, num_layers_transf=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # (LSTM)\n",
    "\n",
    "        # Boolean activations decoder (CNN/MLP)\n",
    "        self.acts_decoder = nn.Linear(d_z, n_tracks*resolution)\n",
    "\n",
    "        # GNN\n",
    "        self.graph_decoder = GCN()\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        self.embedding = nn.Linear(d_token, d_transf)\n",
    "        self.pos_encoder = PositionalEncoding(d_transf,dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead_transf\n",
    "        )\n",
    "        self.transf_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_layers_transf\n",
    "        )\n",
    "        \n",
    "        # Last linear layer\n",
    "        self.lin = nn.Linear(d_model, 140)\n",
    "\n",
    "\n",
    "    def forward(self, z, x_seq, x_acts, x_graph):\n",
    "\n",
    "        # Compute activations from z\n",
    "        acts_out = self.acts_decoder(z)\n",
    "        acts_out = acts_out.view(x_acts.size())\n",
    "        #print(\"Acts out:\", acts_out.size())\n",
    "\n",
    "        # Initialize node features with z and propagate with GNN\n",
    "        node_features = torch.repeat_interleave(\n",
    "                            z, x_acts.size(-1)*x_acts.size(-2), axis=0)\n",
    "        #print(\"Node features:\", node_features.size())\n",
    "\n",
    "        # Todo: use also edge info\n",
    "        x_graph.x = node_features\n",
    "        node_decs = self.graph_decoder(x_graph)\n",
    "        #print(\"Node decodings:\", node_decs.size())\n",
    "        \n",
    "        node_decs = node_decs.repeat(16, 1, 1)\n",
    "        #print(\"Tiled node decodings:\", node_decs.size())\n",
    "\n",
    "        # Decode features with transformer decoder\n",
    "        # forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
    "        \n",
    "        # Todo: same embeddings as encoder?\n",
    "        seq = x_seq.view(-1, x_seq.size(-2), x_seq.size(-1))\n",
    "        embs = self.embedding(seq)\n",
    "        embs = embs.permute(1, 0, 2)\n",
    "        pos_encs = self.pos_encoder(embs)\n",
    "\n",
    "        seq_out = self.transf_decoder(pos_encs, node_decs)\n",
    "        #print(\"Seq out:\", seq_out.size())\n",
    "        \n",
    "        seq_out = self.lin(seq_out)\n",
    "        #print(\"Seq out after lin:\", seq_out.size())\n",
    "        \n",
    "        # Softmax on first 131 values (pitch), sigmoid on last 9 (dur)\n",
    "        #seq_out[:, :, :131] = F.log_softmax(seq_out[:, :, :131], dim=-1)\n",
    "        #seq_out[:, :, 131:] = torch.sigmoid(seq_out[:, :, 131:])\n",
    "        seq_out = seq_out.permute(1, 0, 2)\n",
    "        seq_out = seq_out.view(x_seq.size())\n",
    "        #print(\"Seq out after reshape\", seq_out.size())\n",
    "        \n",
    "\n",
    "        return seq_out, acts_out\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x_seq, x_acts, x_graph):\n",
    "        \n",
    "        mu, log_var = self.encoder(x_seq, x_acts, x_graph)\n",
    "        #print(\"Mu:\", mu.size())\n",
    "        #print(\"log_var:\", log_var.size())\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        sigma = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(sigma)\n",
    "        #print(\"eps:\", eps.size())\n",
    "        z = mu + eps*sigma\n",
    "        \n",
    "        out = self.decoder(z, x_seq, x_acts, x_graph)\n",
    "        \n",
    "        return out, mu, log_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "class VAETrainer():\n",
    "    \n",
    "    def __init__(self, models_path, optimizer, init_lr, lr_scheduler=None, \n",
    "                 device=torch.device(\"cpu\"), print_every=1, save_every=1):\n",
    "        self.models_path = models_path\n",
    "        self.optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.print_every = print_every\n",
    "        self.save_every = save_every\n",
    "        \n",
    "    \n",
    "    def train(self, model, trainloader, validloader=None, epochs=1, name=None):\n",
    "        \n",
    "        if name is None:\n",
    "            name = str(uuid.uuid4())\n",
    "        \n",
    "        path = os.path.join(models_path, name)\n",
    "        \n",
    "        n_batches = len(trainloader)\n",
    "                        \n",
    "        losses = []\n",
    "        acts_losses = []\n",
    "        pitches_losses = []\n",
    "        dur_losses = []\n",
    "        kld_losses = []\n",
    "        lrs = []\n",
    "        \n",
    "        ce = nn.CrossEntropyLoss()\n",
    "        bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        beta = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        print(\"Starting training.\\n\")\n",
    "        \n",
    "        progress_bar = tqdm(range(n_batches))\n",
    "        start = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for batch_idx, (x_seq, x_acts, x_graph) in enumerate(trainloader):\n",
    "                \n",
    "                # Zero out the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Get the inputs\n",
    "                x_seq = x_seq.float().to(self.device)\n",
    "                x_acts = x_acts.to(self.device)\n",
    "                x_graph = x_graph.to(self.device)\n",
    "\n",
    "                # Forward pass, get the reconstructions\n",
    "                out, mu, log_var = model(x_seq, x_acts, x_graph)\n",
    "                seq_rec, acts_rec = out\n",
    "                \n",
    "                # Compute the loss\n",
    "                acts_loss = bce(acts_rec.view(-1), x_acts.view(-1).float())\n",
    "                pitches_loss = ce(seq_rec.reshape(-1, seq_rec.size(-1))[:, :131],\n",
    "                                  x_seq.reshape(-1, x_seq.size(-1))[:, :131].argmax(dim=1))\n",
    "                dur_loss = bce(seq_rec[..., 131:].reshape(-1), \n",
    "                               x_seq[..., 131:].reshape(-1))\n",
    "                kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "                rec_loss = pitches_loss + dur_loss + acts_loss\n",
    "                loss = rec_loss + beta*kld_loss\n",
    "                \n",
    "                # Compute gradients, update weights and lr\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if self.lr_scheduler is not None:\n",
    "                    self.lr_scheduler.step()\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                acts_losses.append(acts_loss.item())\n",
    "                pitches_losses.append(pitches_loss.item())\n",
    "                dur_losses.append(dur_loss.item())\n",
    "                kld_losses.append((beta*kld_loss).item())\n",
    "                last_lr = (self.lr_scheduler.get_last_lr() if self.lr_scheduler is not None\n",
    "                               else self.init_lr)\n",
    "                lrs.append(last_lr)\n",
    "                \n",
    "                # Compute accuracies\n",
    "                note_acc = self.__note_accuracy(seq_rec, x_seq)\n",
    "                pitch_acc = self.__pitches_accuracy(seq_rec, x_seq)\n",
    "                dur_acc = self.__dur_accuracy(seq_rec, x_seq)\n",
    "                acts_acc = self.__acts_accuracy(acts_rec, x_acts)\n",
    "                print(note_acc, pitch_acc, dur_acc, acts_acc)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Print training loss/accuracy information\n",
    "                # Todo: LOSSES MUST BE DIVIED BY PRINT EVERY!\n",
    "                if (batch_idx + 1) % self.print_every == 0:\n",
    "                    print(\"Training on batch {}/{} of epoch {} complete.\"\n",
    "                          .format(batch_idx+1, n_batches, epoch+1))\n",
    "                    print(\"Tot_loss: {:.4f} acts_loss: {:.4f} \"\n",
    "                          .format(running_loss/self.print_every, acts_loss), end='')\n",
    "                    print(\"pitches_loss: {:.4f} dur_loss: {:.4f} kld_loss: {:.4f}\"\n",
    "                          .format(pitches_loss, dur_loss, kld_loss))\n",
    "                    print(\"----------------------------------------\")\n",
    "                    running_loss = 0.0\n",
    "                    \n",
    "                # When appropriate, save model and stats on disk\n",
    "                if self.save_every > 0 and (batch_idx + 1) % self.save_every == 0:\n",
    "                    print(\"\\nSaving model to disk...\\n\")\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'batch': batch_idx,\n",
    "                        'save_every': self.save_every,\n",
    "                        'lrs': lrs,\n",
    "                        'losses': losses,\n",
    "                        'acts_losses': acts_losses,\n",
    "                        'pitches_losses': pitches_losses,\n",
    "                        'dur_losses': dur_losses,\n",
    "                        'kld_losses': kld_losses,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict()\n",
    "                    }, path)\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                    \n",
    "                if batch_idx > 16:\n",
    "                    break\n",
    "            \n",
    "\n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"Training completed in (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "                  .format(int(hours),int(minutes),seconds))\n",
    "        \n",
    "        print(\"Saving model to disk...\")\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'batch': batch_idx,\n",
    "            'save_every': self.save_every,\n",
    "            'lrs': lrs,\n",
    "            'losses': losses,\n",
    "            'acts_losses': acts_losses,\n",
    "            'pitches_losses': pitches_losses,\n",
    "            'dur_losses': dur_losses,\n",
    "            'kld_losses': kld_losses,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }, path)\n",
    "        \n",
    "        print(\"Model saved.\")\n",
    "\n",
    "    \n",
    "    def __acts_accuracy(self, acts_rec, x_acts):\n",
    "        \n",
    "        acts_rec = torch.sigmoid(acts_rec)\n",
    "        acts_rec[acts_rec < 0.5] = 0\n",
    "        acts_rec[acts_rec >= 0.5] = 1\n",
    "        \n",
    "        return torch.sum(acts_rec == x_acts) / x_acts.numel()\n",
    "    \n",
    "    \n",
    "    def __pitches_accuracy(self, seq_rec, x_seq):\n",
    "        \n",
    "        pitches_rec = F.softmax(seq_rec[..., :131], dim=-1)\n",
    "        pitches_rec = torch.argmax(pitches_rec, dim=-1)\n",
    "        pitches_true = torch.argmax(x_seq[..., :131], dim=-1)\n",
    "        \n",
    "        return torch.sum(pitches_rec == pitches_true) / pitches_true.numel()\n",
    "    \n",
    "    \n",
    "    def __dur_accuracy(self, seq_rec, x_seq):\n",
    "        \n",
    "        dur_rec = torch.sigmoid(seq_rec[..., 131:])\n",
    "        dur_rec[dur_rec < 0.5] = 0\n",
    "        dur_rec[dur_rec >= 0.5] = 1\n",
    "        \n",
    "        preds = torch.all(dur_rec == x_seq[..., 131:], dim=-1)\n",
    "        \n",
    "        return torch.sum(preds) / preds.numel()\n",
    "    \n",
    "    \n",
    "    def __note_accuracy(self, seq_rec, x_seq):\n",
    "        \n",
    "        pitches_rec = F.softmax(seq_rec[..., :131], dim=-1)\n",
    "        pitches_rec = torch.argmax(pitches_rec, dim=-1)\n",
    "        pitches_true = torch.argmax(x_seq[..., :131], dim=-1)\n",
    "        \n",
    "        preds_pitches = (pitches_rec == pitches_true)\n",
    "\n",
    "        dur_rec = torch.sigmoid(seq_rec[..., 131:])\n",
    "        dur_rec[dur_rec < 0.5] = 0\n",
    "        dur_rec[dur_rec >= 0.5] = 1\n",
    "        \n",
    "        preds_dur = torch.all(dur_rec == x_seq[..., 131:], dim=-1)\n",
    "        \n",
    "        return torch.sum(torch.logical_and(preds_pitches, preds_dur)) / preds_dur.numel()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"models/\"\n",
    "os.makedirs(models_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13144"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dir = \"data/preprocessed\"\n",
    "dataset = MIDIDataset(ds_dir)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current device idx: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#decive = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Current device idx:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm models/vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model and moving it to the specified device...\n",
      "+----------------------------------------------------------------+------------+\n",
      "|                            Modules                             | Parameters |\n",
      "+----------------------------------------------------------------+------------+\n",
      "|                    encoder.embedding.weight                    |   35840    |\n",
      "|                     encoder.embedding.bias                     |    256     |\n",
      "| encoder.transformer_encoder.layers.0.self_attn.in_proj_weight  |   196608   |\n",
      "|  encoder.transformer_encoder.layers.0.self_attn.in_proj_bias   |    768     |\n",
      "| encoder.transformer_encoder.layers.0.self_attn.out_proj.weight |   65536    |\n",
      "|  encoder.transformer_encoder.layers.0.self_attn.out_proj.bias  |    256     |\n",
      "|      encoder.transformer_encoder.layers.0.linear1.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.0.linear1.bias        |    2048    |\n",
      "|      encoder.transformer_encoder.layers.0.linear2.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.0.linear2.bias        |    256     |\n",
      "|       encoder.transformer_encoder.layers.0.norm1.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.0.norm1.bias         |    256     |\n",
      "|       encoder.transformer_encoder.layers.0.norm2.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.0.norm2.bias         |    256     |\n",
      "| encoder.transformer_encoder.layers.1.self_attn.in_proj_weight  |   196608   |\n",
      "|  encoder.transformer_encoder.layers.1.self_attn.in_proj_bias   |    768     |\n",
      "| encoder.transformer_encoder.layers.1.self_attn.out_proj.weight |   65536    |\n",
      "|  encoder.transformer_encoder.layers.1.self_attn.out_proj.bias  |    256     |\n",
      "|      encoder.transformer_encoder.layers.1.linear1.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.1.linear1.bias        |    2048    |\n",
      "|      encoder.transformer_encoder.layers.1.linear2.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.1.linear2.bias        |    256     |\n",
      "|       encoder.transformer_encoder.layers.1.norm1.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.1.norm1.bias         |    256     |\n",
      "|       encoder.transformer_encoder.layers.1.norm2.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.1.norm2.bias         |    256     |\n",
      "|              encoder.graph_encoder.layers.0.bias               |    256     |\n",
      "|           encoder.graph_encoder.layers.0.lin.weight            |   65536    |\n",
      "|              encoder.graph_encoder.layers.1.bias               |    256     |\n",
      "|           encoder.graph_encoder.layers.1.lin.weight            |   65536    |\n",
      "|                    encoder.linear_mu.weight                    |   65536    |\n",
      "|                     encoder.linear_mu.bias                     |    256     |\n",
      "|                 encoder.linear_log_var.weight                  |   65536    |\n",
      "|                  encoder.linear_log_var.bias                   |    256     |\n",
      "|                  decoder.acts_decoder.weight                   |   32768    |\n",
      "|                   decoder.acts_decoder.bias                    |    128     |\n",
      "|              decoder.graph_decoder.layers.0.bias               |    256     |\n",
      "|           decoder.graph_decoder.layers.0.lin.weight            |   65536    |\n",
      "|              decoder.graph_decoder.layers.1.bias               |    256     |\n",
      "|           decoder.graph_decoder.layers.1.lin.weight            |   65536    |\n",
      "|                    decoder.embedding.weight                    |   35840    |\n",
      "|                     decoder.embedding.bias                     |    256     |\n",
      "|    decoder.transf_decoder.layers.0.self_attn.in_proj_weight    |   196608   |\n",
      "|     decoder.transf_decoder.layers.0.self_attn.in_proj_bias     |    768     |\n",
      "|   decoder.transf_decoder.layers.0.self_attn.out_proj.weight    |   65536    |\n",
      "|    decoder.transf_decoder.layers.0.self_attn.out_proj.bias     |    256     |\n",
      "| decoder.transf_decoder.layers.0.multihead_attn.in_proj_weight  |   196608   |\n",
      "|  decoder.transf_decoder.layers.0.multihead_attn.in_proj_bias   |    768     |\n",
      "| decoder.transf_decoder.layers.0.multihead_attn.out_proj.weight |   65536    |\n",
      "|  decoder.transf_decoder.layers.0.multihead_attn.out_proj.bias  |    256     |\n",
      "|         decoder.transf_decoder.layers.0.linear1.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.0.linear1.bias          |    2048    |\n",
      "|         decoder.transf_decoder.layers.0.linear2.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.0.linear2.bias          |    256     |\n",
      "|          decoder.transf_decoder.layers.0.norm1.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.0.norm1.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.0.norm2.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.0.norm2.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.0.norm3.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.0.norm3.bias           |    256     |\n",
      "|    decoder.transf_decoder.layers.1.self_attn.in_proj_weight    |   196608   |\n",
      "|     decoder.transf_decoder.layers.1.self_attn.in_proj_bias     |    768     |\n",
      "|   decoder.transf_decoder.layers.1.self_attn.out_proj.weight    |   65536    |\n",
      "|    decoder.transf_decoder.layers.1.self_attn.out_proj.bias     |    256     |\n",
      "| decoder.transf_decoder.layers.1.multihead_attn.in_proj_weight  |   196608   |\n",
      "|  decoder.transf_decoder.layers.1.multihead_attn.in_proj_bias   |    768     |\n",
      "| decoder.transf_decoder.layers.1.multihead_attn.out_proj.weight |   65536    |\n",
      "|  decoder.transf_decoder.layers.1.multihead_attn.out_proj.bias  |    256     |\n",
      "|         decoder.transf_decoder.layers.1.linear1.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.1.linear1.bias          |    2048    |\n",
      "|         decoder.transf_decoder.layers.1.linear2.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.1.linear2.bias          |    256     |\n",
      "|          decoder.transf_decoder.layers.1.norm1.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.1.norm1.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.1.norm2.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.1.norm2.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.1.norm3.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.1.norm3.bias           |    256     |\n",
      "|                       decoder.lin.weight                       |   35840    |\n",
      "|                        decoder.lin.bias                        |    140     |\n",
      "+----------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 6323468\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Starting training.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9502eabc040e47289bd991107d0dbbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:1')\n",
      "Training on batch 1/411 of epoch 1 complete.\n",
      "Tot_loss: 6.4145 acts_loss: 0.7382 pitches_loss: 4.9653 dur_loss: 0.7110 kld_loss: 675.7560\n",
      "----------------------------------------\n",
      "tensor(0., device='cuda:1')\n",
      "Training on batch 2/411 of epoch 1 complete.\n",
      "Tot_loss: 6.2859 acts_loss: 0.7456 pitches_loss: 4.8346 dur_loss: 0.7057 kld_loss: 665.6460\n",
      "----------------------------------------\n",
      "tensor(0., device='cuda:1')\n",
      "Training on batch 3/411 of epoch 1 complete.\n",
      "Tot_loss: 6.1732 acts_loss: 0.7418 pitches_loss: 4.7331 dur_loss: 0.6983 kld_loss: 675.7481\n",
      "----------------------------------------\n",
      "tensor(0., device='cuda:1')\n",
      "Training on batch 4/411 of epoch 1 complete.\n",
      "Tot_loss: 6.1012 acts_loss: 0.7505 pitches_loss: 4.6557 dur_loss: 0.6950 kld_loss: 679.3285\n",
      "----------------------------------------\n",
      "tensor(0., device='cuda:1')\n",
      "Training on batch 5/411 of epoch 1 complete.\n",
      "Tot_loss: 5.9366 acts_loss: 0.7390 pitches_loss: 4.5129 dur_loss: 0.6847 kld_loss: 674.4521\n",
      "----------------------------------------\n",
      "tensor(4.5776e-05, device='cuda:1')\n",
      "Training on batch 6/411 of epoch 1 complete.\n",
      "Tot_loss: 5.8359 acts_loss: 0.7319 pitches_loss: 4.4155 dur_loss: 0.6886 kld_loss: 688.0739\n",
      "----------------------------------------\n",
      "tensor(0.0001, device='cuda:1')\n",
      "Training on batch 7/411 of epoch 1 complete.\n",
      "Tot_loss: 5.7142 acts_loss: 0.7447 pitches_loss: 4.2923 dur_loss: 0.6772 kld_loss: 696.8761\n",
      "----------------------------------------\n",
      "tensor(0.0003, device='cuda:1')\n",
      "Training on batch 8/411 of epoch 1 complete.\n",
      "Tot_loss: 5.5984 acts_loss: 0.7400 pitches_loss: 4.1806 dur_loss: 0.6777 kld_loss: 707.1577\n",
      "----------------------------------------\n",
      "tensor(0.0002, device='cuda:1')\n",
      "Training on batch 9/411 of epoch 1 complete.\n",
      "Tot_loss: 5.4928 acts_loss: 0.7322 pitches_loss: 4.0822 dur_loss: 0.6785 kld_loss: 710.8967\n",
      "----------------------------------------\n",
      "tensor(0.0006, device='cuda:1')\n",
      "Training on batch 10/411 of epoch 1 complete.\n",
      "Tot_loss: 5.4003 acts_loss: 0.7399 pitches_loss: 3.9922 dur_loss: 0.6681 kld_loss: 728.7313\n",
      "----------------------------------------\n",
      "tensor(0.0012, device='cuda:1')\n",
      "Training on batch 11/411 of epoch 1 complete.\n",
      "Tot_loss: 5.2811 acts_loss: 0.7453 pitches_loss: 3.8766 dur_loss: 0.6592 kld_loss: 744.5844\n",
      "----------------------------------------\n",
      "tensor(0.0017, device='cuda:1')\n",
      "Training on batch 12/411 of epoch 1 complete.\n",
      "Tot_loss: 5.1553 acts_loss: 0.7350 pitches_loss: 3.7662 dur_loss: 0.6540 kld_loss: 755.0392\n",
      "----------------------------------------\n",
      "tensor(0.0031, device='cuda:1')\n",
      "Training on batch 13/411 of epoch 1 complete.\n",
      "Tot_loss: 5.0503 acts_loss: 0.7407 pitches_loss: 3.6625 dur_loss: 0.6471 kld_loss: 771.7662\n",
      "----------------------------------------\n",
      "tensor(0.0031, device='cuda:1')\n",
      "Training on batch 14/411 of epoch 1 complete.\n",
      "Tot_loss: 4.9509 acts_loss: 0.7338 pitches_loss: 3.5690 dur_loss: 0.6482 kld_loss: 786.6100\n",
      "----------------------------------------\n",
      "tensor(0.0054, device='cuda:1')\n",
      "Training on batch 15/411 of epoch 1 complete.\n",
      "Tot_loss: 4.8319 acts_loss: 0.7365 pitches_loss: 3.4600 dur_loss: 0.6354 kld_loss: 799.5132\n",
      "----------------------------------------\n",
      "tensor(0.0052, device='cuda:1')\n",
      "Training on batch 16/411 of epoch 1 complete.\n",
      "Tot_loss: 4.7412 acts_loss: 0.7322 pitches_loss: 3.3703 dur_loss: 0.6387 kld_loss: 822.2694\n",
      "----------------------------------------\n",
      "tensor(0.0053, device='cuda:1')\n",
      "Training on batch 17/411 of epoch 1 complete.\n",
      "Tot_loss: 4.6745 acts_loss: 0.7430 pitches_loss: 3.2956 dur_loss: 0.6359 kld_loss: 834.2996\n",
      "----------------------------------------\n",
      "tensor(0.0062, device='cuda:1')\n",
      "Training on batch 18/411 of epoch 1 complete.\n",
      "Tot_loss: 4.5135 acts_loss: 0.7247 pitches_loss: 3.1573 dur_loss: 0.6315 kld_loss: 855.5588\n",
      "----------------------------------------\n",
      "Training completed in (h:m:s): 00:00:07.64\n",
      "Saving model to disk...\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "def print_params(model):\n",
    "    \n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, parameter in model.named_parameters():\n",
    "        \n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "            \n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params += param\n",
    "        \n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "    return total_params\n",
    "\n",
    "\n",
    "print(\"Creating the model and moving it to the specified device...\")\n",
    "\n",
    "vae = VAE().to(device)\n",
    "print_params(vae)\n",
    "print()\n",
    "\n",
    "init_lr = 1e-5\n",
    "gamma = 0.999\n",
    "optimizer = optim.Adam(vae.parameters(), lr=init_lr)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "\n",
    "print('--------------------------------------------------\\n')\n",
    "\n",
    "trainer = VAETrainer(\n",
    "    models_path,\n",
    "    optimizer, \n",
    "    init_lr, \n",
    "    lr_scheduler=scheduler,\n",
    "    save_every=100, \n",
    "    device=device\n",
    ")\n",
    "trainer.train(vae, loader, name='vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.load('models/vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7c4d92ddd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 387
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(range(1, len(m['losses'])+1), m['losses'], label='Loss')\n",
    "#plt.plot(range(1, len(m['acts_losses'])+1), m['acts_losses'], label='acts')\n",
    "#plt.plot(range(1, len(m['pitches_losses'])+1), m['pitches_losses'], label='pitches')\n",
    "#plt.plot(range(1, len(m['dur_losses'])+1), m['dur_losses'], label='dur')\n",
    "#plt.plot(range(1, len(m['kld_losses'])+1), m['kld_losses'], label='kld')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9.990000000000001e-06],\n",
       " [9.980010000000002e-06],\n",
       " [9.970029990000002e-06],\n",
       " [9.960059960010001e-06],\n",
       " [9.950099900049992e-06],\n",
       " [9.940149800149942e-06],\n",
       " [9.930209650349793e-06],\n",
       " [9.920279440699443e-06],\n",
       " [9.910359161258744e-06],\n",
       " [9.900448802097486e-06],\n",
       " [9.890548353295388e-06],\n",
       " [9.880657804942094e-06],\n",
       " [9.870777147137152e-06],\n",
       " [9.860906369990014e-06],\n",
       " [9.851045463620024e-06],\n",
       " [9.841194418156404e-06],\n",
       " [9.831353223738247e-06],\n",
       " [9.82152187051451e-06],\n",
       " [9.811700348643995e-06],\n",
       " [9.80188864829535e-06],\n",
       " [9.792086759647056e-06],\n",
       " [9.782294672887408e-06],\n",
       " [9.77251237821452e-06],\n",
       " [9.762739865836305e-06],\n",
       " [9.752977125970469e-06],\n",
       " [9.743224148844498e-06],\n",
       " [9.733480924695653e-06],\n",
       " [9.723747443770958e-06],\n",
       " [9.714023696327187e-06],\n",
       " [9.70430967263086e-06],\n",
       " [9.69460536295823e-06],\n",
       " [9.684910757595272e-06],\n",
       " [9.675225846837678e-06],\n",
       " [9.665550620990839e-06],\n",
       " [9.655885070369848e-06],\n",
       " [9.646229185299477e-06],\n",
       " [9.636582956114178e-06],\n",
       " [9.626946373158064e-06],\n",
       " [9.617319426784907e-06],\n",
       " [9.607702107358121e-06],\n",
       " [9.598094405250763e-06],\n",
       " [9.588496310845512e-06],\n",
       " [9.578907814534666e-06],\n",
       " [9.569328906720132e-06],\n",
       " [9.559759577813412e-06],\n",
       " [9.550199818235598e-06],\n",
       " [9.540649618417363e-06],\n",
       " [9.531108968798946e-06],\n",
       " [9.521577859830147e-06],\n",
       " [9.512056281970317e-06],\n",
       " [9.502544225688347e-06],\n",
       " [9.493041681462659e-06],\n",
       " [9.483548639781196e-06],\n",
       " [9.474065091141415e-06],\n",
       " [9.464591026050274e-06],\n",
       " [9.455126435024224e-06],\n",
       " [9.4456713085892e-06],\n",
       " [9.43622563728061e-06],\n",
       " [9.426789411643329e-06],\n",
       " [9.417362622231686e-06],\n",
       " [9.407945259609454e-06],\n",
       " [9.398537314349844e-06],\n",
       " [9.389138777035493e-06],\n",
       " [9.379749638258457e-06],\n",
       " [9.370369888620198e-06],\n",
       " [9.360999518731577e-06],\n",
       " [9.351638519212846e-06],\n",
       " [9.342286880693634e-06],\n",
       " [9.33294459381294e-06],\n",
       " [9.323611649219126e-06],\n",
       " [9.314288037569906e-06],\n",
       " [9.304973749532336e-06],\n",
       " [9.295668775782803e-06],\n",
       " [9.28637310700702e-06],\n",
       " [9.277086733900013e-06],\n",
       " [9.267809647166112e-06],\n",
       " [9.258541837518946e-06],\n",
       " [9.249283295681428e-06],\n",
       " [9.240034012385745e-06],\n",
       " [9.23079397837336e-06],\n",
       " [9.221563184394987e-06],\n",
       " [9.212341621210593e-06],\n",
       " [9.203129279589382e-06],\n",
       " [9.193926150309793e-06],\n",
       " [9.184732224159483e-06],\n",
       " [9.175547491935324e-06],\n",
       " [9.166371944443388e-06],\n",
       " [9.157205572498944e-06],\n",
       " [9.148048366926444e-06],\n",
       " [9.138900318559518e-06],\n",
       " [9.129761418240958e-06],\n",
       " [9.120631656822717e-06],\n",
       " [9.111511025165895e-06],\n",
       " [9.102399514140729e-06],\n",
       " [9.093297114626589e-06],\n",
       " [9.084203817511962e-06],\n",
       " [9.07511961369445e-06],\n",
       " [9.066044494080755e-06],\n",
       " [9.056978449586675e-06],\n",
       " [9.047921471137089e-06],\n",
       " [9.038873549665951e-06],\n",
       " [9.029834676116285e-06],\n",
       " [9.02080484144017e-06],\n",
       " [9.011784036598729e-06],\n",
       " [9.00277225256213e-06],\n",
       " [8.993769480309567e-06],\n",
       " [8.984775710829257e-06],\n",
       " [8.975790935118428e-06],\n",
       " [8.96681514418331e-06],\n",
       " [8.957848329039127e-06],\n",
       " [8.948890480710087e-06],\n",
       " [8.939941590229377e-06],\n",
       " [8.931001648639148e-06],\n",
       " [8.92207064699051e-06],\n",
       " [8.913148576343519e-06],\n",
       " [8.904235427767176e-06],\n",
       " [8.895331192339408e-06],\n",
       " [8.886435861147068e-06],\n",
       " [8.877549425285922e-06],\n",
       " [8.868671875860635e-06],\n",
       " [8.859803203984774e-06],\n",
       " [8.85094340078079e-06],\n",
       " [8.842092457380008e-06],\n",
       " [8.833250364922628e-06],\n",
       " [8.824417114557706e-06],\n",
       " [8.815592697443148e-06],\n",
       " [8.806777104745706e-06],\n",
       " [8.79797032764096e-06],\n",
       " [8.78917235731332e-06],\n",
       " [8.780383184956005e-06],\n",
       " [8.771602801771048e-06],\n",
       " [8.762831198969278e-06],\n",
       " [8.754068367770309e-06],\n",
       " [8.745314299402539e-06],\n",
       " [8.736568985103137e-06],\n",
       " [8.727832416118034e-06],\n",
       " [8.719104583701917e-06],\n",
       " [8.710385479118214e-06],\n",
       " [8.701675093639096e-06],\n",
       " [8.692973418545457e-06],\n",
       " [8.684280445126911e-06],\n",
       " [8.675596164681785e-06],\n",
       " [8.666920568517103e-06],\n",
       " [8.658253647948586e-06],\n",
       " [8.649595394300638e-06],\n",
       " [8.640945798906336e-06],\n",
       " [8.63230485310743e-06],\n",
       " [8.623672548254323e-06],\n",
       " [8.615048875706069e-06],\n",
       " [8.606433826830362e-06],\n",
       " [8.597827393003533e-06],\n",
       " [8.58922956561053e-06],\n",
       " [8.58064033604492e-06],\n",
       " [8.572059695708875e-06],\n",
       " [8.563487636013165e-06],\n",
       " [8.554924148377152e-06],\n",
       " [8.546369224228774e-06],\n",
       " [8.537822855004545e-06],\n",
       " [8.52928503214954e-06],\n",
       " [8.520755747117391e-06],\n",
       " [8.512234991370275e-06],\n",
       " [8.503722756378904e-06],\n",
       " [8.495219033622525e-06],\n",
       " [8.486723814588903e-06],\n",
       " [8.478237090774314e-06],\n",
       " [8.46975885368354e-06],\n",
       " [8.461289094829856e-06],\n",
       " [8.452827805735026e-06],\n",
       " [8.444374977929291e-06],\n",
       " [8.435930602951361e-06],\n",
       " [8.42749467234841e-06],\n",
       " [8.41906717767606e-06],\n",
       " [8.410648110498384e-06],\n",
       " [8.402237462387885e-06],\n",
       " [8.393835224925496e-06],\n",
       " [8.38544138970057e-06],\n",
       " [8.37705594831087e-06],\n",
       " [8.368678892362559e-06],\n",
       " [8.360310213470196e-06],\n",
       " [8.351949903256725e-06],\n",
       " [8.343597953353469e-06],\n",
       " [8.335254355400116e-06],\n",
       " [8.326919101044715e-06],\n",
       " [8.31859218194367e-06],\n",
       " [8.310273589761726e-06],\n",
       " [8.301963316171964e-06],\n",
       " [8.293661352855792e-06],\n",
       " [8.285367691502936e-06],\n",
       " [8.277082323811432e-06],\n",
       " [8.268805241487621e-06],\n",
       " [8.260536436246134e-06],\n",
       " [8.252275899809887e-06],\n",
       " [8.244023623910077e-06],\n",
       " [8.235779600286167e-06],\n",
       " [8.22754382068588e-06],\n",
       " [8.219316276865194e-06],\n",
       " [8.211096960588328e-06],\n",
       " [8.20288586362774e-06],\n",
       " [8.194682977764113e-06],\n",
       " [8.186488294786349e-06],\n",
       " [8.178301806491562e-06],\n",
       " [8.17012350468507e-06],\n",
       " [8.161953381180385e-06],\n",
       " [8.153791427799204e-06],\n",
       " [8.145637636371405e-06],\n",
       " [8.137491998735034e-06],\n",
       " [8.129354506736298e-06],\n",
       " [8.121225152229562e-06],\n",
       " [8.113103927077332e-06],\n",
       " [8.104990823150254e-06],\n",
       " [8.096885832327104e-06],\n",
       " [8.088788946494778e-06],\n",
       " [8.080700157548283e-06],\n",
       " [8.072619457390734e-06],\n",
       " [8.064546837933343e-06],\n",
       " [8.056482291095409e-06],\n",
       " [8.048425808804313e-06],\n",
       " [8.040377382995509e-06],\n",
       " [8.032337005612514e-06],\n",
       " [8.024304668606902e-06],\n",
       " [8.016280363938295e-06],\n",
       " [8.008264083574356e-06],\n",
       " [8.000255819490782e-06],\n",
       " [7.992255563671292e-06],\n",
       " [7.984263308107621e-06],\n",
       " [7.976279044799513e-06],\n",
       " [7.968302765754713e-06],\n",
       " [7.960334462988958e-06],\n",
       " [7.95237412852597e-06],\n",
       " [7.944421754397444e-06],\n",
       " [7.936477332643047e-06],\n",
       " [7.928540855310404e-06],\n",
       " [7.920612314455093e-06],\n",
       " [7.912691702140638e-06],\n",
       " [7.904779010438497e-06],\n",
       " [7.896874231428059e-06],\n",
       " [7.88897735719663e-06],\n",
       " [7.881088379839433e-06],\n",
       " [7.873207291459594e-06],\n",
       " [7.865334084168135e-06],\n",
       " [7.857468750083966e-06],\n",
       " [7.849611281333883e-06],\n",
       " [7.841761670052548e-06],\n",
       " [7.833919908382495e-06],\n",
       " [7.826085988474112e-06],\n",
       " [7.818259902485638e-06],\n",
       " [7.810441642583152e-06],\n",
       " [7.80263120094057e-06],\n",
       " [7.79482856973963e-06],\n",
       " [7.787033741169889e-06],\n",
       " [7.77924670742872e-06],\n",
       " [7.771467460721291e-06],\n",
       " [7.76369599326057e-06],\n",
       " [7.75593229726731e-06],\n",
       " [7.748176364970042e-06],\n",
       " [7.740428188605072e-06],\n",
       " [7.732687760416467e-06],\n",
       " [7.72495507265605e-06],\n",
       " [7.717230117583394e-06],\n",
       " [7.70951288746581e-06],\n",
       " [7.701803374578343e-06],\n",
       " [7.694101571203765e-06],\n",
       " [7.686407469632561e-06],\n",
       " [7.678721062162929e-06],\n",
       " [7.671042341100766e-06],\n",
       " [7.663371298759665e-06],\n",
       " [7.655707927460905e-06],\n",
       " [7.648052219533445e-06],\n",
       " [7.640404167313911e-06],\n",
       " [7.632763763146597e-06],\n",
       " [7.6251309993834505e-06],\n",
       " [7.617505868384067e-06],\n",
       " [7.609888362515683e-06],\n",
       " [7.602278474153167e-06],\n",
       " [7.594676195679013e-06],\n",
       " [7.587081519483334e-06],\n",
       " [7.579494437963851e-06],\n",
       " [7.571914943525887e-06],\n",
       " [7.564343028582361e-06],\n",
       " [7.556778685553778e-06],\n",
       " [7.549221906868224e-06],\n",
       " [7.541672684961356e-06],\n",
       " [7.534131012276394e-06],\n",
       " [7.526596881264118e-06],\n",
       " [7.519070284382854e-06],\n",
       " [7.511551214098471e-06],\n",
       " [7.5040396628843724e-06],\n",
       " [7.496535623221488e-06],\n",
       " [7.489039087598267e-06],\n",
       " [7.481550048510668e-06],\n",
       " [7.474068498462157e-06],\n",
       " [7.4665944299636955e-06],\n",
       " [7.459127835533732e-06],\n",
       " [7.4516687076981985e-06],\n",
       " [7.4442170389905e-06],\n",
       " [7.4367728219515095e-06],\n",
       " [7.429336049129558e-06],\n",
       " [7.4219067130804285e-06],\n",
       " [7.414484806367348e-06],\n",
       " [7.407070321560981e-06],\n",
       " [7.399663251239419e-06],\n",
       " [7.39226358798818e-06],\n",
       " [7.384871324400192e-06],\n",
       " [7.377486453075792e-06],\n",
       " [7.370108966622715e-06],\n",
       " [7.362738857656093e-06],\n",
       " [7.355376118798437e-06],\n",
       " [7.348020742679639e-06],\n",
       " [7.340672721936959e-06],\n",
       " [7.333332049215022e-06],\n",
       " [7.325998717165807e-06],\n",
       " [7.3186727184486414e-06],\n",
       " [7.3113540457301924e-06],\n",
       " [7.304042691684463e-06],\n",
       " [7.296738648992778e-06],\n",
       " [7.289441910343785e-06],\n",
       " [7.2821524684334415e-06],\n",
       " [7.274870315965008e-06],\n",
       " [7.267595445649043e-06],\n",
       " [7.260327850203394e-06],\n",
       " [7.253067522353191e-06],\n",
       " [7.245814454830838e-06],\n",
       " [7.2385686403760074e-06],\n",
       " [7.231330071735631e-06],\n",
       " [7.2240987416638955e-06],\n",
       " [7.216874642922231e-06],\n",
       " [7.209657768279309e-06],\n",
       " [7.2024481105110295e-06],\n",
       " [7.195245662400519e-06],\n",
       " [7.1880504167381185e-06],\n",
       " [7.180862366321381e-06],\n",
       " [7.173681503955059e-06],\n",
       " [7.166507822451104e-06],\n",
       " [7.1593413146286526e-06],\n",
       " [7.1521819733140235e-06],\n",
       " [7.145029791340709e-06],\n",
       " [7.137884761549369e-06],\n",
       " [7.130746876787819e-06],\n",
       " [7.123616129911031e-06],\n",
       " [7.11649251378112e-06],\n",
       " [7.109376021267339e-06],\n",
       " [7.102266645246071e-06],\n",
       " [7.0951643786008255e-06],\n",
       " [7.0880692142222245e-06],\n",
       " [7.080981145008002e-06],\n",
       " [7.0739001638629944e-06],\n",
       " [7.066826263699132e-06],\n",
       " [7.059759437435432e-06],\n",
       " [7.052699677997997e-06],\n",
       " [7.045646978319999e-06],\n",
       " [7.038601331341678e-06],\n",
       " [7.031562730010337e-06],\n",
       " [7.024531167280327e-06],\n",
       " [7.017506636113047e-06],\n",
       " [7.010489129476934e-06],\n",
       " [7.003478640347457e-06],\n",
       " [6.996475161707109e-06],\n",
       " [6.989478686545402e-06],\n",
       " [6.982489207858857e-06],\n",
       " [6.975506718650998e-06],\n",
       " [6.968531211932347e-06],\n",
       " [6.961562680720415e-06],\n",
       " [6.954601118039694e-06],\n",
       " [6.947646516921654e-06],\n",
       " [6.9406988704047325e-06],\n",
       " [6.9337581715343274e-06],\n",
       " [6.926824413362793e-06],\n",
       " [6.9198975889494305e-06],\n",
       " [6.912977691360481e-06],\n",
       " [6.9060647136691205e-06],\n",
       " [6.899158648955451e-06],\n",
       " [6.892259490306496e-06],\n",
       " [6.885367230816189e-06],\n",
       " [6.878481863585373e-06],\n",
       " [6.871603381721787e-06],\n",
       " [6.8647317783400655e-06],\n",
       " [6.857867046561725e-06],\n",
       " [6.851009179515163e-06],\n",
       " [6.844158170335648e-06],\n",
       " [6.837314012165312e-06],\n",
       " [6.830476698153147e-06],\n",
       " [6.823646221454994e-06],\n",
       " [6.816822575233539e-06],\n",
       " [6.810005752658305e-06],\n",
       " [6.803195746905647e-06],\n",
       " [6.796392551158741e-06],\n",
       " [6.789596158607583e-06],\n",
       " [6.782806562448975e-06],\n",
       " [6.776023755886527e-06],\n",
       " [6.76924773213064e-06],\n",
       " [6.762478484398509e-06],\n",
       " [6.75571600591411e-06],\n",
       " [6.748960289908196e-06],\n",
       " [6.742211329618288e-06],\n",
       " [6.73546911828867e-06],\n",
       " [6.728733649170381e-06],\n",
       " [6.72200491552121e-06],\n",
       " [6.715282910605689e-06],\n",
       " [6.708567627695083e-06],\n",
       " [6.701859060067388e-06],\n",
       " [6.69515720100732e-06],\n",
       " [6.688462043806313e-06],\n",
       " [6.681773581762507e-06],\n",
       " [6.675091808180744e-06],\n",
       " [6.668416716372563e-06],\n",
       " [6.6617482996561906e-06],\n",
       " [6.655086551356534e-06],\n",
       " [6.648431464805177e-06],\n",
       " [6.641783033340372e-06],\n",
       " [6.635141250307031e-06],\n",
       " [6.628506109056724e-06],\n",
       " [6.621877602947667e-06],\n",
       " [6.61525572534472e-06],\n",
       " [6.608640469619375e-06],\n",
       " [6.602031829149755e-06],\n",
       " [6.595429797320605e-06],\n",
       " [6.5888343675232844e-06],\n",
       " [6.582245533155761e-06],\n",
       " [6.5756632876226055e-06],\n",
       " [6.569087624334983e-06],\n",
       " [6.5625185367106485e-06],\n",
       " [6.555956018173938e-06],\n",
       " [6.549400062155764e-06],\n",
       " [6.542850662093609e-06],\n",
       " [6.536307811431515e-06],\n",
       " [6.529771503620083e-06],\n",
       " [6.5232417321164635e-06],\n",
       " [6.516718490384347e-06],\n",
       " [6.5102017718939625e-06],\n",
       " [6.503691570122068e-06],\n",
       " [6.497187878551946e-06],\n",
       " [6.490690690673394e-06],\n",
       " [6.48419999998272e-06],\n",
       " [6.477715799982738e-06],\n",
       " [6.4712380841827555e-06],\n",
       " [6.464766846098572e-06],\n",
       " [6.4583020792524735e-06],\n",
       " [6.451843777173221e-06],\n",
       " [6.4453919333960485e-06],\n",
       " [6.438946541462652e-06],\n",
       " [6.43250759492119e-06],\n",
       " [6.426075087326269e-06],\n",
       " [6.419649012238943e-06],\n",
       " [6.413229363226704e-06],\n",
       " [6.4068161338634774e-06],\n",
       " [6.400409317729614e-06],\n",
       " [6.394008908411885e-06],\n",
       " [6.3876148995034726e-06],\n",
       " [6.3812272846039695e-06],\n",
       " [6.374846057319365e-06],\n",
       " [6.368471211262046e-06],\n",
       " [6.362102740050784e-06],\n",
       " [6.355740637310733e-06],\n",
       " [6.3493848966734225e-06],\n",
       " [6.343035511776749e-06],\n",
       " [6.336692476264972e-06],\n",
       " [6.330355783788708e-06],\n",
       " [6.324025428004919e-06],\n",
       " [6.3177014025769144e-06],\n",
       " [6.3113837011743375e-06],\n",
       " [6.305072317473163e-06],\n",
       " [6.29876724515569e-06],\n",
       " [6.292468477910534e-06],\n",
       " [6.2861760094326235e-06],\n",
       " [6.279889833423191e-06],\n",
       " [6.2736099435897676e-06],\n",
       " [6.267336333646177e-06],\n",
       " [6.261068997312532e-06],\n",
       " [6.254807928315219e-06],\n",
       " [6.2485531203869034e-06],\n",
       " [6.242304567266517e-06],\n",
       " [6.23606226269925e-06],\n",
       " [6.229826200436551e-06],\n",
       " [6.2235963742361144e-06],\n",
       " [6.217372777861879e-06],\n",
       " [6.211155405084017e-06],\n",
       " [6.204944249678933e-06],\n",
       " [6.198739305429254e-06],\n",
       " [6.192540566123825e-06],\n",
       " [6.186348025557701e-06],\n",
       " [6.180161677532144e-06],\n",
       " [6.173981515854612e-06],\n",
       " [6.167807534338757e-06],\n",
       " [6.161639726804419e-06],\n",
       " [6.155478087077614e-06],\n",
       " [6.149322608990537e-06],\n",
       " [6.143173286381547e-06],\n",
       " [6.137030113095165e-06],\n",
       " [6.13089308298207e-06],\n",
       " [6.1247621898990875e-06],\n",
       " [6.118637427709188e-06],\n",
       " [6.112518790281479e-06],\n",
       " [6.106406271491198e-06],\n",
       " [6.1002998652197065e-06],\n",
       " [6.094199565354487e-06],\n",
       " [6.088105365789132e-06],\n",
       " [6.0820172604233436e-06],\n",
       " [6.075935243162921e-06],\n",
       " [6.0698593079197575e-06],\n",
       " [6.0637894486118374e-06],\n",
       " [6.057725659163225e-06],\n",
       " [6.051667933504062e-06],\n",
       " [6.045616265570558e-06],\n",
       " [6.039570649304988e-06],\n",
       " [6.033531078655683e-06],\n",
       " [6.027497547577028e-06],\n",
       " [6.021470050029451e-06],\n",
       " [6.015448579979421e-06],\n",
       " [6.009433131399442e-06],\n",
       " [6.003423698268042e-06],\n",
       " [5.997420274569774e-06],\n",
       " [5.991422854295204e-06],\n",
       " [5.985431431440908e-06],\n",
       " [5.979446000009467e-06],\n",
       " [5.973466554009458e-06],\n",
       " [5.967493087455449e-06],\n",
       " [5.961525594367993e-06],\n",
       " [5.9555640687736256e-06],\n",
       " [5.949608504704852e-06],\n",
       " [5.943658896200147e-06],\n",
       " [5.937715237303947e-06],\n",
       " [5.931777522066643e-06],\n",
       " [5.925845744544576e-06],\n",
       " [5.919919898800032e-06],\n",
       " [5.913999978901231e-06],\n",
       " [5.90808597892233e-06],\n",
       " [5.902177892943407e-06],\n",
       " [5.8962757150504636e-06],\n",
       " [5.890379439335413e-06],\n",
       " [5.884489059896077e-06],\n",
       " [5.878604570836181e-06],\n",
       " [5.872725966265345e-06],\n",
       " [5.86685324029908e-06],\n",
       " [5.860986387058781e-06],\n",
       " [5.855125400671722e-06],\n",
       " [5.8492702752710504e-06],\n",
       " [5.84342100499578e-06],\n",
       " [5.837577583990784e-06],\n",
       " [5.8317400064067936e-06],\n",
       " [5.825908266400387e-06],\n",
       " [5.820082358133986e-06],\n",
       " [5.8142622757758526e-06],\n",
       " [5.808448013500077e-06],\n",
       " [5.802639565486577e-06],\n",
       " [5.79683692592109e-06],\n",
       " [5.791040088995169e-06],\n",
       " [5.785249048906174e-06],\n",
       " [5.779463799857268e-06],\n",
       " [5.7736843360574104e-06],\n",
       " [5.767910651721353e-06],\n",
       " [5.762142741069631e-06],\n",
       " [5.756380598328562e-06],\n",
       " [5.750624217730234e-06],\n",
       " [5.744873593512503e-06],\n",
       " [5.7391287199189904e-06],\n",
       " [5.7333895911990714e-06],\n",
       " [5.727656201607872e-06],\n",
       " [5.721928545406265e-06],\n",
       " [5.716206616860859e-06],\n",
       " [5.710490410243998e-06],\n",
       " [5.7047799198337545e-06],\n",
       " [5.6990751399139205e-06],\n",
       " [5.693376064774007e-06],\n",
       " [5.687682688709233e-06],\n",
       " [5.6819950060205235e-06],\n",
       " [5.676313011014503e-06],\n",
       " [5.670636698003488e-06],\n",
       " [5.6649660613054845e-06],\n",
       " [5.659301095244179e-06],\n",
       " [5.653641794148935e-06],\n",
       " [5.647988152354786e-06],\n",
       " [5.642340164202431e-06],\n",
       " [5.636697824038229e-06],\n",
       " [5.63106112621419e-06],\n",
       " [5.625430065087976e-06],\n",
       " [5.619804635022888e-06],\n",
       " [5.614184830387865e-06],\n",
       " [5.6085706455574765e-06],\n",
       " [5.602962074911919e-06],\n",
       " [5.597359112837007e-06],\n",
       " [5.59176175372417e-06],\n",
       " [5.586169991970446e-06],\n",
       " [5.580583821978476e-06],\n",
       " [5.575003238156497e-06],\n",
       " [5.5694282349183405e-06],\n",
       " [5.563858806683422e-06],\n",
       " [5.558294947876739e-06],\n",
       " [5.552736652928862e-06],\n",
       " [5.547183916275933e-06],\n",
       " [5.541636732359657e-06],\n",
       " [5.536095095627297e-06],\n",
       " [5.53055900053167e-06],\n",
       " [5.5250284415311386e-06],\n",
       " [5.5195034130896074e-06],\n",
       " [5.513983909676518e-06],\n",
       " [5.508469925766841e-06],\n",
       " [5.502961455841074e-06],\n",
       " [5.497458494385233e-06],\n",
       " [5.491961035890848e-06],\n",
       " [5.486469074854957e-06],\n",
       " [5.480982605780102e-06],\n",
       " [5.475501623174322e-06],\n",
       " [5.470026121551148e-06],\n",
       " [5.464556095429596e-06],\n",
       " [5.459091539334167e-06],\n",
       " [5.453632447794833e-06],\n",
       " [5.448178815347038e-06],\n",
       " [5.442730636531691e-06],\n",
       " [5.43728790589516e-06],\n",
       " [5.4318506179892645e-06],\n",
       " [5.426418767371275e-06],\n",
       " [5.420992348603904e-06],\n",
       " [5.4155713562553e-06],\n",
       " [5.410155784899045e-06],\n",
       " [5.404745629114146e-06],\n",
       " [5.399340883485032e-06],\n",
       " [5.393941542601547e-06],\n",
       " [5.388547601058945e-06],\n",
       " [5.3831590534578865e-06],\n",
       " [5.377775894404429e-06],\n",
       " [5.3723981185100245e-06],\n",
       " [5.3670257203915145e-06],\n",
       " [5.361658694671123e-06],\n",
       " [5.356297035976452e-06],\n",
       " [5.350940738940476e-06],\n",
       " [5.345589798201535e-06],\n",
       " [5.340244208403334e-06],\n",
       " [5.33490396419493e-06],\n",
       " [5.329569060230735e-06],\n",
       " [5.324239491170504e-06],\n",
       " [5.3189152516793335e-06],\n",
       " [5.313596336427654e-06],\n",
       " [5.3082827400912264e-06],\n",
       " [5.302974457351135e-06],\n",
       " [5.297671482893784e-06],\n",
       " [5.29237381141089e-06],\n",
       " [5.287081437599479e-06],\n",
       " [5.281794356161879e-06],\n",
       " [5.276512561805718e-06],\n",
       " [5.271236049243912e-06],\n",
       " [5.265964813194668e-06],\n",
       " [5.2606988483814735e-06],\n",
       " [5.255438149533092e-06],\n",
       " [5.250182711383559e-06],\n",
       " [5.2449325286721755e-06],\n",
       " [5.239687596143503e-06],\n",
       " [5.234447908547359e-06],\n",
       " [5.229213460638812e-06],\n",
       " [5.223984247178173e-06],\n",
       " [5.218760262930995e-06],\n",
       " [5.213541502668064e-06],\n",
       " [5.208327961165397e-06],\n",
       " [5.203119633204231e-06],\n",
       " [5.197916513571027e-06],\n",
       " [5.192718597057456e-06],\n",
       " [5.187525878460398e-06],\n",
       " [5.182338352581938e-06],\n",
       " [5.177156014229355e-06],\n",
       " [5.171978858215126e-06],\n",
       " [5.166806879356911e-06],\n",
       " [5.161640072477554e-06],\n",
       " [5.156478432405077e-06],\n",
       " [5.151321953972672e-06],\n",
       " [5.146170632018699e-06],\n",
       " [5.141024461386681e-06],\n",
       " [5.1358834369252944e-06],\n",
       " [5.1307475534883694e-06],\n",
       " [5.125616805934881e-06],\n",
       " [5.120491189128946e-06],\n",
       " [5.115370697939817e-06],\n",
       " [5.110255327241877e-06],\n",
       " [5.105145071914636e-06],\n",
       " [5.100039926842721e-06],\n",
       " [5.094939886915878e-06],\n",
       " [5.089844947028962e-06],\n",
       " [5.0847551020819335e-06],\n",
       " [5.0796703469798516e-06],\n",
       " [5.074590676632872e-06],\n",
       " [5.069516085956239e-06],\n",
       " [5.064446569870283e-06],\n",
       " [5.059382123300413e-06],\n",
       " [5.054322741177112e-06],\n",
       " [5.049268418435935e-06],\n",
       " [5.044219150017499e-06],\n",
       " [5.0391749308674815e-06],\n",
       " [5.034135755936614e-06],\n",
       " [5.029101620180677e-06],\n",
       " [5.0240725185604965e-06],\n",
       " [5.019048446041936e-06],\n",
       " [5.014029397595894e-06],\n",
       " [5.0090153681982985e-06],\n",
       " [5.0040063528301e-06],\n",
       " [4.99900234647727e-06],\n",
       " [4.994003344130792e-06],\n",
       " [4.989009340786661e-06],\n",
       " [4.984020331445874e-06],\n",
       " [4.9790363111144284e-06],\n",
       " [4.974057274803314e-06],\n",
       " [4.969083217528511e-06],\n",
       " [4.9641141343109825e-06],\n",
       " [4.959150020176672e-06],\n",
       " [4.954190870156495e-06],\n",
       " [4.949236679286339e-06],\n",
       " [4.944287442607053e-06],\n",
       " [4.939343155164446e-06],\n",
       " [4.934403812009281e-06],\n",
       " [4.929469408197272e-06],\n",
       " [4.9245399387890746e-06],\n",
       " [4.919615398850285e-06],\n",
       " [4.914695783451435e-06],\n",
       " [4.909781087667983e-06],\n",
       " [4.904871306580315e-06],\n",
       " [4.899966435273735e-06],\n",
       " [4.895066468838461e-06],\n",
       " [4.890171402369623e-06],\n",
       " [4.8852812309672535e-06],\n",
       " [4.880395949736286e-06],\n",
       " [4.87551555378655e-06],\n",
       " [4.870640038232763e-06],\n",
       " [4.865769398194531e-06],\n",
       " [4.860903628796336e-06],\n",
       " [4.856042725167539e-06],\n",
       " [4.8511866824423714e-06],\n",
       " [4.846335495759929e-06],\n",
       " [4.841489160264169e-06],\n",
       " [4.836647671103905e-06],\n",
       " [4.831811023432801e-06],\n",
       " [4.826979212409368e-06],\n",
       " [4.822152233196958e-06],\n",
       " [4.817330080963761e-06],\n",
       " [4.812512750882797e-06],\n",
       " [4.807700238131914e-06],\n",
       " [4.802892537893782e-06],\n",
       " [4.798089645355888e-06],\n",
       " [4.793291555710532e-06],\n",
       " [4.788498264154822e-06],\n",
       " [4.783709765890667e-06],\n",
       " [4.778926056124776e-06],\n",
       " [4.774147130068651e-06],\n",
       " [4.769372982938583e-06],\n",
       " [4.7646036099556445e-06],\n",
       " [4.7598390063456885e-06],\n",
       " [4.7550791673393425e-06],\n",
       " [4.750324088172003e-06],\n",
       " [4.745573764083831e-06],\n",
       " [4.740828190319747e-06],\n",
       " [4.7360873621294274e-06],\n",
       " [4.731351274767298e-06],\n",
       " [4.72661992349253e-06],\n",
       " [4.721893303569038e-06],\n",
       " [4.717171410265469e-06],\n",
       " [4.712454238855204e-06],\n",
       " [4.707741784616348e-06],\n",
       " [4.703034042831732e-06],\n",
       " [4.6983310087889005e-06],\n",
       " [4.6936326777801115e-06],\n",
       " [4.688939045102331e-06],\n",
       " [4.684250106057228e-06],\n",
       " [4.679565855951171e-06],\n",
       " [4.67488629009522e-06],\n",
       " [4.670211403805125e-06],\n",
       " [4.66554119240132e-06],\n",
       " [4.6608756512089186e-06],\n",
       " [4.65621477555771e-06],\n",
       " [4.651558560782152e-06],\n",
       " [4.64690700222137e-06],\n",
       " [4.642260095219148e-06],\n",
       " [4.637617835123928e-06],\n",
       " [4.6329802172888044e-06],\n",
       " [4.628347237071515e-06],\n",
       " [4.623718889834444e-06],\n",
       " [4.61909517094461e-06],\n",
       " [4.614476075773666e-06],\n",
       " [4.609861599697892e-06],\n",
       " [4.605251738098195e-06],\n",
       " [4.6006464863600965e-06],\n",
       " [4.596045839873736e-06],\n",
       " [4.5914497940338625e-06],\n",
       " [4.586858344239829e-06],\n",
       " [4.582271485895589e-06],\n",
       " [4.577689214409693e-06],\n",
       " [4.573111525195283e-06],\n",
       " [4.568538413670088e-06],\n",
       " [4.563969875256417e-06],\n",
       " [4.559405905381161e-06],\n",
       " [4.55484649947578e-06],\n",
       " [4.550291652976305e-06],\n",
       " [4.5457413613233285e-06],\n",
       " [4.541195619962005e-06],\n",
       " [4.536654424342043e-06],\n",
       " [4.532117769917701e-06],\n",
       " [4.527585652147783e-06],\n",
       " [4.5230580664956355e-06],\n",
       " [4.51853500842914e-06],\n",
       " [4.5140164734207105e-06],\n",
       " [4.50950245694729e-06],\n",
       " [4.504992954490342e-06],\n",
       " [4.500487961535852e-06],\n",
       " [4.495987473574316e-06],\n",
       " [4.491491486100742e-06],\n",
       " [4.486999994614641e-06],\n",
       " [4.482512994620027e-06],\n",
       " [4.478030481625407e-06],\n",
       " [4.4735524511437815e-06],\n",
       " [4.469078898692638e-06],\n",
       " [4.464609819793945e-06],\n",
       " [4.460145209974151e-06],\n",
       " [4.4556850647641774e-06],\n",
       " [4.4512293796994135e-06],\n",
       " [4.446778150319714e-06],\n",
       " [4.442331372169394e-06],\n",
       " [4.437889040797225e-06],\n",
       " [4.433451151756428e-06],\n",
       " [4.429017700604671e-06],\n",
       " [4.424588682904066e-06],\n",
       " [4.420164094221162e-06],\n",
       " [4.415743930126941e-06],\n",
       " [4.411328186196814e-06],\n",
       " [4.406916858010618e-06],\n",
       " [4.402509941152607e-06],\n",
       " [4.398107431211454e-06],\n",
       " [4.393709323780243e-06],\n",
       " [4.389315614456462e-06],\n",
       " [4.384926298842006e-06],\n",
       " [4.3805413725431645e-06],\n",
       " [4.376160831170622e-06],\n",
       " [4.371784670339451e-06],\n",
       " [4.367412885669111e-06],\n",
       " [4.363045472783442e-06],\n",
       " [4.358682427310659e-06],\n",
       " [4.354323744883348e-06],\n",
       " [4.349969421138465e-06],\n",
       " [4.345619451717327e-06],\n",
       " [4.341273832265609e-06],\n",
       " [4.336932558433343e-06],\n",
       " [4.33259562587491e-06],\n",
       " [4.328263030249035e-06],\n",
       " [4.323934767218786e-06],\n",
       " [4.319610832451567e-06],\n",
       " [4.315291221619116e-06],\n",
       " [4.310975930397497e-06],\n",
       " [4.306664954467099e-06],\n",
       " [4.302358289512632e-06],\n",
       " [4.298055931223119e-06],\n",
       " [4.293757875291896e-06],\n",
       " [4.289464117416604e-06],\n",
       " [4.285174653299188e-06],\n",
       " [4.280889478645888e-06],\n",
       " [4.276608589167242e-06],\n",
       " [4.272331980578075e-06],\n",
       " [4.268059648597496e-06],\n",
       " [4.263791588948899e-06],\n",
       " [4.2595277973599504e-06],\n",
       " [4.2552682695625905e-06],\n",
       " [4.251013001293028e-06],\n",
       " [4.246761988291735e-06],\n",
       " [4.242515226303444e-06],\n",
       " [4.2382727110771405e-06],\n",
       " [4.234034438366063e-06],\n",
       " [4.229800403927697e-06],\n",
       " [4.22557060352377e-06],\n",
       " [4.221345032920246e-06],\n",
       " [4.217123687887325e-06],\n",
       " [4.212906564199438e-06],\n",
       " [4.2086936576352385e-06],\n",
       " [4.204484963977603e-06],\n",
       " [4.200280479013626e-06],\n",
       " [4.196080198534612e-06],\n",
       " [4.191884118336078e-06],\n",
       " [4.187692234217742e-06],\n",
       " [4.183504541983524e-06],\n",
       " [4.17932103744154e-06],\n",
       " [4.175141716404099e-06],\n",
       " [4.1709665746876955e-06],\n",
       " [4.1667956081130076e-06],\n",
       " [4.162628812504895e-06],\n",
       " [4.15846618369239e-06],\n",
       " [4.154307717508698e-06],\n",
       " [4.150153409791189e-06],\n",
       " [4.146003256381398e-06],\n",
       " [4.141857253125017e-06],\n",
       " [4.137715395871892e-06],\n",
       " [4.13357768047602e-06],\n",
       " [4.129444102795543e-06],\n",
       " [4.125314658692748e-06],\n",
       " [4.121189344034056e-06],\n",
       " [4.117068154690022e-06],\n",
       " [4.112951086535332e-06],\n",
       " [4.1088381354487965e-06],\n",
       " [4.104729297313348e-06],\n",
       " [4.100624568016035e-06],\n",
       " [4.096523943448018e-06],\n",
       " [4.09242741950457e-06],\n",
       " [4.088334992085066e-06],\n",
       " [4.084246657092981e-06],\n",
       " [4.080162410435888e-06],\n",
       " [4.076082248025452e-06],\n",
       " [4.072006165777427e-06],\n",
       " [4.06793415961165e-06],\n",
       " [4.063866225452038e-06],\n",
       " [4.059802359226586e-06],\n",
       " [4.055742556867359e-06],\n",
       " [4.051686814310491e-06],\n",
       " [4.047635127496181e-06],\n",
       " [4.043587492368685e-06],\n",
       " [4.039543904876317e-06],\n",
       " [4.03550436097144e-06],\n",
       " [4.031468856610469e-06],\n",
       " [4.0274373877538584e-06],\n",
       " [4.023409950366105e-06],\n",
       " [4.019386540415739e-06],\n",
       " [4.015367153875323e-06],\n",
       " [4.011351786721448e-06],\n",
       " [4.007340434934727e-06],\n",
       " [4.003333094499792e-06],\n",
       " [3.999329761405292e-06],\n",
       " [3.995330431643887e-06],\n",
       " [3.991335101212243e-06],\n",
       " [3.987343766111031e-06],\n",
       " [3.983356422344921e-06],\n",
       " [3.979373065922576e-06],\n",
       " [3.975393692856653e-06],\n",
       " [3.971418299163797e-06],\n",
       " [3.967446880864633e-06],\n",
       " [3.963479433983768e-06],\n",
       " [3.9595159545497845e-06],\n",
       " [3.955556438595235e-06],\n",
       " [3.95160088215664e-06],\n",
       " [3.947649281274483e-06],\n",
       " [3.9437016319932085e-06],\n",
       " [3.939757930361215e-06],\n",
       " [3.935818172430854e-06],\n",
       " [3.9318823542584235e-06],\n",
       " [3.927950471904165e-06],\n",
       " [3.924022521432261e-06],\n",
       " [3.920098498910829e-06],\n",
       " [3.916178400411918e-06],\n",
       " [3.912262222011506e-06],\n",
       " [3.908349959789495e-06],\n",
       " [3.904441609829705e-06],\n",
       " [3.900537168219876e-06],\n",
       " [3.896636631051656e-06],\n",
       " [3.892739994420604e-06],\n",
       " [3.888847254426183e-06],\n",
       " [3.884958407171758e-06],\n",
       " [3.8810734487645855e-06],\n",
       " [3.877192375315821e-06],\n",
       " [3.873315182940506e-06],\n",
       " [3.869441867757565e-06],\n",
       " [3.865572425889807e-06],\n",
       " [3.861706853463917e-06],\n",
       " [3.857845146610453e-06],\n",
       " [3.853987301463842e-06],\n",
       " [3.850133314162378e-06],\n",
       " [3.846283180848216e-06],\n",
       " [3.842436897667367e-06],\n",
       " [3.8385944607696995e-06],\n",
       " [3.83475586630893e-06],\n",
       " [3.830921110442621e-06],\n",
       " [3.827090189332178e-06],\n",
       " [3.8232630991428466e-06],\n",
       " [3.819439836043704e-06],\n",
       " [3.8156203962076605e-06],\n",
       " [3.811804775811453e-06],\n",
       " [3.8079929710356417e-06],\n",
       " [3.804184978064606e-06],\n",
       " [3.8003807930865415e-06],\n",
       " [3.796580412293455e-06],\n",
       " [3.7927838318811614e-06],\n",
       " [3.78899104804928e-06],\n",
       " [3.7852020570012307e-06],\n",
       " [3.7814168549442294e-06],\n",
       " [3.7776354380892854e-06],\n",
       " [3.773857802651196e-06],\n",
       " [3.770083944848545e-06],\n",
       " [3.766313860903696e-06],\n",
       " [3.7625475470427927e-06],\n",
       " [3.75878499949575e-06],\n",
       " [3.7550262144962542e-06],\n",
       " [3.751271188281758e-06],\n",
       " [3.7475199170934763e-06],\n",
       " [3.743772397176383e-06],\n",
       " [3.7400286247792066e-06],\n",
       " [3.7362885961544275e-06],\n",
       " [3.732552307558273e-06],\n",
       " [3.728819755250715e-06],\n",
       " [3.7250909354954643e-06],\n",
       " [3.7213658445599687e-06],\n",
       " [3.7176444787154087e-06],\n",
       " [3.713926834236693e-06]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m['lrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7c45f71e90>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 258,
       "width": 372
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, len(m['lrs'])+1), m['lrs'], label='lr')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTFN-DCJjZWM"
   },
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sSvVK7CxjV8"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter as Param\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter\n",
    "from torch_sparse import SparseTensor, matmul, masked_select_nnz\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (Tensor, Tensor) -> Tensor\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (SparseTensor, Tensor) -> SparseTensor\n",
    "    pass\n",
    "\n",
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return masked_select_nnz(edge_index, edge_mask, layout='coo')\n",
    "\n",
    "\n",
    "class RGCNConv(MessagePassing):\n",
    "    r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "    Relational Data with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}_{\\textrm{root}} \\cdot\n",
    "        \\mathbf{x}_i + \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
    "        \\frac{1}{|\\mathcal{N}_r(i)|} \\mathbf{\\Theta}_r \\cdot \\mathbf{x}_j,\n",
    "\n",
    "    where :math:`\\mathcal{R}` denotes the set of relations, *i.e.* edge types.\n",
    "    Edge type needs to be a one-dimensional :obj:`torch.long` tensor which\n",
    "    stores a relation identifier\n",
    "    :math:`\\in \\{ 0, \\ldots, |\\mathcal{R}| - 1\\}` for each edge.\n",
    "\n",
    "    .. note::\n",
    "        This implementation is as memory-efficient as possible by iterating\n",
    "        over each individual relation type.\n",
    "        Therefore, it may result in low GPU utilization in case the graph has a\n",
    "        large number of relations.\n",
    "        As an alternative approach, :class:`FastRGCNConv` does not iterate over\n",
    "        each individual type, but may consume a large amount of memory to\n",
    "        compensate.\n",
    "        We advise to check out both implementations to see which one fits your\n",
    "        needs.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\n",
    "            corresponds to the sizes of source and target dimensionalities.\n",
    "            In case no input features are given, this argument should\n",
    "            correspond to the number of nodes in your graph.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_relations (int): Number of relations.\n",
    "        num_bases (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the basis-decomposition regularization scheme where\n",
    "            :obj:`num_bases` denotes the number of bases to use.\n",
    "            (default: :obj:`None`)\n",
    "        num_blocks (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the block-diagonal-decomposition regularization scheme where\n",
    "            :obj:`num_blocks` denotes the number of blocks to use.\n",
    "            (default: :obj:`None`)\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
    "            (default: :obj:`\"mean\"`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        num_relations: int,\n",
    "        num_bases: Optional[int] = None,\n",
    "        num_blocks: Optional[int] = None,\n",
    "        aggr: str = 'mean',\n",
    "        root_weight: bool = True,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(aggr=aggr, node_dim=0, **kwargs)\n",
    "\n",
    "        if num_bases is not None and num_blocks is not None:\n",
    "            raise ValueError('Can not apply both basis-decomposition and '\n",
    "                             'block-diagonal-decomposition at the same time.')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_relations = num_relations\n",
    "        self.num_bases = num_bases\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "        self.in_channels_l = in_channels[0]\n",
    "\n",
    "        if num_bases is not None:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_bases, in_channels[0], out_channels))\n",
    "            self.comp = Parameter(torch.Tensor(num_relations, num_bases))\n",
    "\n",
    "        elif num_blocks is not None:\n",
    "            assert (in_channels[0] % num_blocks == 0\n",
    "                    and out_channels % num_blocks == 0)\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, num_blocks,\n",
    "                             in_channels[0] // num_blocks,\n",
    "                             out_channels // num_blocks))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        else:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, in_channels[0], out_channels))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        if root_weight:\n",
    "            self.root = Param(torch.Tensor(in_channels[1], out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Param(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        glorot(self.comp)\n",
    "        glorot(self.root)\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
    "                edge_index: Adj, edge_type: OptTensor = None):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x: The input node features. Can be either a :obj:`[num_nodes,\n",
    "                in_channels]` node feature matrix, or an optional\n",
    "                one-dimensional node index tensor (in which case input features\n",
    "                are treated as trainable node embeddings).\n",
    "                Furthermore, :obj:`x` can be of type :obj:`tuple` denoting\n",
    "                source and destination node features.\n",
    "            edge_type: The one-dimensional relation type/index for each edge in\n",
    "                :obj:`edge_index`.\n",
    "                Should be only :obj:`None` in case :obj:`edge_index` is of type\n",
    "                :class:`torch_sparse.tensor.SparseTensor`.\n",
    "                (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert input features to a pair of node features or node indices.\n",
    "        x_l: OptTensor = None\n",
    "        if isinstance(x, tuple):\n",
    "            x_l = x[0]\n",
    "        else:\n",
    "            x_l = x\n",
    "        if x_l is None:\n",
    "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
    "\n",
    "        x_r: Tensor = x_l\n",
    "        if isinstance(x, tuple):\n",
    "            x_r = x[1]\n",
    "\n",
    "        size = (x_l.size(0), x_r.size(0))\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_type = edge_index.storage.value()\n",
    "        assert edge_type is not None\n",
    "\n",
    "        # propagate_type: (x: Tensor)\n",
    "        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)\n",
    "\n",
    "        weight = self.weight\n",
    "        if self.num_bases is not None:  # Basis-decomposition =================\n",
    "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
    "                self.num_relations, self.in_channels_l, self.out_channels)\n",
    "\n",
    "        if self.num_blocks is not None:  # Block-diagonal-decomposition =====\n",
    "\n",
    "            if x_l.dtype == torch.long and self.num_blocks is not None:\n",
    "                raise ValueError('Block-diagonal decomposition not supported '\n",
    "                                 'for non-continuous input features.')\n",
    "\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                h = self.propagate(tmp, x=x_l, size=size)\n",
    "                h = h.view(-1, weight.size(1), weight.size(2))\n",
    "                h = torch.einsum('abc,bcd->abd', h, weight[i])\n",
    "                out += h.contiguous().view(-1, self.out_channels)\n",
    "\n",
    "        else:  # No regularization/Basis-decomposition ========================\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "\n",
    "                if x_l.dtype == torch.long:\n",
    "                    out += self.propagate(tmp, x=weight[i, x_l], size=size)\n",
    "                else:\n",
    "                    h = self.propagate(tmp, x=x_l, size=size)\n",
    "                    out = out + (h @ weight[i])\n",
    "\n",
    "        root = self.root\n",
    "        if root is not None:\n",
    "            out += root[x_r] if x_r.dtype == torch.long else x_r @ root\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor) -> Tensor:\n",
    "        return x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        adj_t = adj_t.set_value(None, layout=None)\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, num_relations={self.num_relations})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfsNpMLrEXLk"
   },
   "source": [
    "next edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DaVTonr_XB8",
    "outputId": "9e5fa8f9-604e-4273-a34d-fad73ad9ab7e"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "a = np.random.randint(2, size=(4,8))\n",
    "a_t = a.transpose()\n",
    "print(a_t)\n",
    "inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "ts_acts = np.any(a_t, axis=1)\n",
    "ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "labels = np.arange(32).reshape(4, 8).transpose()\n",
    "print(labels)\n",
    "\n",
    "next_edges = []\n",
    "for i in range(len(ts_inds)-1):\n",
    "    ind_s = ts_inds[i]\n",
    "    ind_e = ts_inds[i+1]\n",
    "    s = inds[inds[:,0] == ind_s]\n",
    "    e = inds[inds[:,0] == ind_e]\n",
    "    e_inds = [t for t in list(itertools.product(s, e)) if t[0][1] != t[1][1]]\n",
    "    edges = [(labels[tuple(e[0])],labels[tuple(e[1])], ind_e-ind_s) for e in e_inds]\n",
    "    next_edges.extend(edges)\n",
    "\n",
    "print(next_edges)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ5JQm1aEbmb"
   },
   "source": [
    "onset edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DISmsJB3EatR",
    "outputId": "fc864608-63a6-4ad0-84d9-1478001ce60e"
   },
   "outputs": [],
   "source": [
    "onset_edges = []\n",
    "print(a_t)\n",
    "print(labels)\n",
    "\n",
    "for i in ts_inds:\n",
    "    ts_acts_inds = list(inds[inds[:,0] == i])\n",
    "    if len(ts_acts_inds) < 2:\n",
    "        continue\n",
    "    e_inds = list(itertools.combinations(ts_acts_inds, 2))\n",
    "    edges = [(labels[tuple(e[0])], labels[tuple(e[1])], 0) for e in e_inds]\n",
    "    inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "    onset_edges.extend(edges)\n",
    "    onset_edges.extend(inv_edges)\n",
    "\n",
    "print(onset_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujitZCKaa7nu"
   },
   "source": [
    "track edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbVG1vdFa-7e",
    "outputId": "c042449b-eef2-4707-a524-5f66f3ec07c7"
   },
   "outputs": [],
   "source": [
    "print(a_t)\n",
    "print(labels)\n",
    "track_edges = []\n",
    "\n",
    "for track in range(a_t.shape[1]):\n",
    "    tr_inds = list(inds[inds[:,1] == track])\n",
    "    e_inds = [(tr_inds[i],\n",
    "               tr_inds[i+1]) for i in range(len(tr_inds)-1)]\n",
    "    print(e_inds)\n",
    "    edges = [(labels[tuple(e[0])], labels[tuple(e[1])], e[1][0]-e[0][0]) for e in e_inds]\n",
    "    track_edges.extend(edges)\n",
    "\n",
    "print(track_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DzouJ5NqALB",
    "outputId": "20a76e82-6305-4154-d894-6d69a64435a1"
   },
   "outputs": [],
   "source": [
    "track_edges = np.array(track_edges)\n",
    "onset_edges = np.array(onset_edges)\n",
    "np.concatenate((track_edges, onset_edges)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihIYkPWPzyGX"
   },
   "outputs": [],
   "source": [
    "pip install pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie0pU8NWAUNM"
   },
   "outputs": [],
   "source": [
    "import pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTbGBSrdAZGH"
   },
   "outputs": [],
   "source": [
    "multitrack = pypianoroll.read(\"tests_fur-elise.mid\")\n",
    "print(multitrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eVo_BKzAmz4"
   },
   "outputs": [],
   "source": [
    "multitrack.tracks[0].pianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPpWw-rLA7CI"
   },
   "outputs": [],
   "source": [
    "multitrack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-PYbS7FA-Gg"
   },
   "outputs": [],
   "source": [
    "multitrack.trim(0, 12 * multitrack.resolution)\n",
    "multitrack.binarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psxuoTsZBFXY"
   },
   "outputs": [],
   "source": [
    "multitrack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovyixmSvBG3w"
   },
   "outputs": [],
   "source": [
    "multitrack.tracks[0].pianoroll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHlKNufuBzLn"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOhjCOJb34P4bTid7qFDg58",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1NeVldMsPVJd6pXbxZDmuiUP-QJBRhYtj",
   "name": "midi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
