{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99881b47",
   "metadata": {},
   "source": [
    "Load model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c5c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from plot import plot_losses, plot_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6333170",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'models/LMD2fix'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b15e2",
   "metadata": {},
   "source": [
    "Plot losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12838fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 260,
       "width": 368
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = [\n",
    "    #'tot',\n",
    "    #'structure',\n",
    "    'pitch',\n",
    "    'dur',\n",
    "    #'reconstruction',\n",
    "    #'kld',\n",
    "    #'beta*kld'\n",
    "]\n",
    "plot_losses(model_dir, losses, plot_val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37d02c",
   "metadata": {},
   "source": [
    "Plot accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d73b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether to plot validation data\n",
    "plot_val = False\n",
    "\n",
    "# Uncomment the stats that have to be plotted\n",
    "accuracies = [\n",
    "    #'s_acc',\n",
    "    #'s_precision',\n",
    "    #'s_recall',\n",
    "    #'s_f1',\n",
    "    'pitch',\n",
    "    'pitch_drums',\n",
    "    'pitch_non_drums',\n",
    "    #'dur',\n",
    "    #'note'\n",
    "]\n",
    "plot_losses(model_dir, losses, plot_val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e9631",
   "metadata": {},
   "source": [
    "Current epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c38c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['tot_batches']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ba9af",
   "metadata": {},
   "source": [
    "Show the parameters of the current model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9232c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.load(os.path.join(models_dir, model, 'configuration'), map_location='cpu')\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f864f",
   "metadata": {},
   "source": [
    "Learning rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = checkpoint['lrs']\n",
    "plt.plot(range(1, len(lrs)+1), lrs, label='Lr')\n",
    "plt.grid()\n",
    "plt.ylim(0)\n",
    "plt.xlim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf8198",
   "metadata": {},
   "source": [
    "Betas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc5d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "betas = checkpoint['betas']\n",
    "plt.plot(range(1, len(betas)+1), betas, label='Beta')\n",
    "plt.grid()\n",
    "#plt.ylim(0, 0.001)\n",
    "plt.xlim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a24a8",
   "metadata": {},
   "source": [
    "Reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b3f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(3)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Current device idx:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710edc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import itertools\n",
    "from torch_geometric.data.collate import collate\n",
    "\n",
    "\n",
    "class MIDIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir, n_bars=2):\n",
    "        self.dir = dir\n",
    "        _, _, files = next(os.walk(self.dir))\n",
    "        self.len = len(files)\n",
    "        self.n_bars = n_bars\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    \n",
    "    def _get_track_edges(self, acts, edge_type_ind=0):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        \n",
    "        # Create node labels\n",
    "        labels = np.zeros(acts.shape)\n",
    "        acts_inds = np.where(acts == 1)\n",
    "        num_nodes = len(acts_inds[0])\n",
    "        labels[acts_inds] = np.arange(num_nodes)\n",
    "        labels = labels.transpose()\n",
    "\n",
    "        track_edges = []\n",
    "\n",
    "        for track in range(a_t.shape[1]):\n",
    "            tr_inds = list(inds[inds[:,1] == track])\n",
    "            e_inds = [(tr_inds[i],\n",
    "                    tr_inds[i+1]) for i in range(len(tr_inds)-1)]\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind+track, e[1][0]-e[0][0]) for e in e_inds]\n",
    "            inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "            track_edges.extend(edges)\n",
    "            track_edges.extend(inv_edges)\n",
    "\n",
    "        return np.array(track_edges, dtype='long')\n",
    "\n",
    "    \n",
    "    def _get_onset_edges(self, acts, edge_type_ind=4):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        # Create node labels\n",
    "        labels = np.zeros(acts.shape)\n",
    "        acts_inds = np.where(acts == 1)\n",
    "        num_nodes = len(acts_inds[0])\n",
    "        labels[acts_inds] = np.arange(num_nodes)\n",
    "        labels = labels.transpose()\n",
    "\n",
    "        onset_edges = []\n",
    "\n",
    "        for i in ts_inds:\n",
    "            ts_acts_inds = list(inds[inds[:,0] == i])\n",
    "            if len(ts_acts_inds) < 2:\n",
    "                continue\n",
    "            e_inds = list(itertools.combinations(ts_acts_inds, 2))\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, 0) for e in e_inds]\n",
    "            inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "            onset_edges.extend(edges)\n",
    "            onset_edges.extend(inv_edges)\n",
    "\n",
    "        return np.array(onset_edges, dtype='long')\n",
    "\n",
    "\n",
    "    def _get_next_edges(self, acts, edge_type_ind=5):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        # Create node labels\n",
    "        labels = np.zeros(acts.shape)\n",
    "        acts_inds = np.where(acts == 1)\n",
    "        num_nodes = len(acts_inds[0])\n",
    "        labels[acts_inds] = np.arange(num_nodes)\n",
    "        labels = labels.transpose()\n",
    "\n",
    "        next_edges = []\n",
    "\n",
    "        for i in range(len(ts_inds)-1):\n",
    "\n",
    "            ind_s = ts_inds[i]\n",
    "            ind_e = ts_inds[i+1]\n",
    "            s = inds[inds[:,0] == ind_s]\n",
    "            e = inds[inds[:,0] == ind_e]\n",
    "\n",
    "            e_inds = [t for t in list(itertools.product(s, e)) if t[0][1] != t[1][1]]\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, ind_e-ind_s) for e in e_inds]\n",
    "            inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "            \n",
    "            next_edges.extend(edges)\n",
    "            next_edges.extend(inv_edges)\n",
    "            \n",
    "        return np.array(next_edges, dtype='long')\n",
    "    \n",
    "    def _get_super_edges(self, num_nodes, edge_type_ind=6):\n",
    "    \n",
    "        super_edges = [(num_nodes, i, edge_type_ind, 0) for i in range(num_nodes)]\n",
    "        inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "        \n",
    "        super_edges.extend(inv_edges)\n",
    "        \n",
    "        return np.array(super_edges, dtype='long')\n",
    "        \n",
    "    \n",
    "    def _get_node_features(self, acts, num_nodes):\n",
    "        \n",
    "        num_tracks = acts.shape[0]\n",
    "        features = torch.zeros((num_nodes, num_tracks), dtype=torch.float)\n",
    "        features[np.arange(num_nodes), np.stack(np.where(acts))[0]] = 1.\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Load tensors\n",
    "        sample_path = os.path.join(self.dir, str(idx) + \".npz\")\n",
    "        data = np.load(sample_path)\n",
    "        seq_tensor = data[\"seq_tensor\"]\n",
    "        seq_acts = data[\"seq_acts\"]\n",
    "        \n",
    "        # From (#tracks x #timesteps x ...) to (#bars x #tracks x #timesteps x ...)\n",
    "        seq_tensor = seq_tensor.reshape(seq_tensor.shape[0], self.n_bars, -1,\n",
    "                                        seq_tensor.shape[2], seq_tensor.shape[3])\n",
    "        seq_tensor = seq_tensor.transpose(1, 0, 2, 3, 4)\n",
    "        seq_acts = seq_acts.reshape(seq_acts.shape[0], self.n_bars, -1)\n",
    "        seq_acts = seq_acts.transpose(1, 0, 2)\n",
    "        \n",
    "        # Construct src_key_padding_mask (PAD = 130)\n",
    "        src_mask = torch.from_numpy((seq_tensor[..., 0] == 130))\n",
    "\n",
    "        # From decimals to one-hot (pitch)\n",
    "        pitches = seq_tensor[..., 0]\n",
    "        onehot_p = np.zeros(\n",
    "            (pitches.shape[0]*pitches.shape[1]*pitches.shape[2]*pitches.shape[3],\n",
    "             131), \n",
    "            dtype=float\n",
    "        )\n",
    "        onehot_p[np.arange(0, onehot_p.shape[0]), pitches.reshape(-1)] = 1.\n",
    "        onehot_p = onehot_p.reshape(pitches.shape[0], pitches.shape[1], \n",
    "                                    pitches.shape[2], pitches.shape[3], 131)\n",
    "        \n",
    "        # From decimals to one-hot (dur)\n",
    "        durs = seq_tensor[..., 1]\n",
    "        onehot_d = np.zeros(\n",
    "            (durs.shape[0]*durs.shape[1]*durs.shape[2]*durs.shape[3],\n",
    "             99),\n",
    "            dtype=float\n",
    "        )\n",
    "        onehot_d[np.arange(0, onehot_d.shape[0]), durs.reshape(-1)] = 1.\n",
    "        onehot_d = onehot_d.reshape(durs.shape[0], durs.shape[1], \n",
    "                                    durs.shape[2], durs.shape[3], 99)\n",
    "        \n",
    "        # Concatenate pitches and durations\n",
    "        new_seq_tensor = np.concatenate((onehot_p, onehot_d),\n",
    "                                        axis=-1)\n",
    "        \n",
    "        graphs = []\n",
    "        \n",
    "        # Iterate over bars and construct a graph for each bar\n",
    "        for i in range(self.n_bars):\n",
    "            \n",
    "            # Number of nodes\n",
    "            n = torch.sum(torch.Tensor(seq_acts[i]), dtype=torch.long)\n",
    "            \n",
    "            # Get edges from boolean activations\n",
    "            # Todo: optimize and refactor\n",
    "            track_edges = self._get_track_edges(seq_acts[i])\n",
    "            onset_edges = self._get_onset_edges(seq_acts[i])\n",
    "            next_edges = self._get_next_edges(seq_acts[i])\n",
    "            #super_edges = self._get_super_edges(n)\n",
    "            edges = [track_edges, onset_edges, next_edges]\n",
    "            \n",
    "            # Concatenate edge tensors (N x 4) (if any)\n",
    "            # First two columns -> source and dest nodes\n",
    "            # Third column -> edge_type, Fourth column -> timestep distance\n",
    "            no_edges = (len(track_edges) == 0 and \n",
    "                        len(onset_edges) == 0 and len(next_edges) == 0)\n",
    "            if not no_edges:\n",
    "                edge_list = np.concatenate([x for x in edges\n",
    "                                              if x.size > 0])\n",
    "                edge_list = torch.from_numpy(edge_list)\n",
    "                \n",
    "            # Adapt tensor to torch_geometric's Data\n",
    "            # No edges: add fictitious self-edge\n",
    "            edge_index = (torch.LongTensor([[0], [0]]) if no_edges else\n",
    "                                   edge_list[:, :2].t().contiguous())\n",
    "            attrs = (torch.Tensor([[0, 0]]) if no_edges else\n",
    "                                           edge_list[:, 2:])\n",
    "\n",
    "            # One hot timestep distance concatenated to edge type\n",
    "            edge_attrs = torch.zeros(attrs.size(0), 1+seq_acts.shape[-1])\n",
    "            edge_attrs[:, 0] = attrs[:, 0]\n",
    "            edge_attrs[np.arange(edge_attrs.size(0)), attrs.long()[:, 1]+1] = 1\n",
    "            #edge_attrs = torch.Tensor(attrs.float())\n",
    "            \n",
    "            node_features = self._get_node_features(seq_acts[i], n)\n",
    "            is_drum = node_features[:, 0].bool()\n",
    "            \n",
    "            graphs.append(Data(edge_index=edge_index, edge_attrs=edge_attrs,\n",
    "                               num_nodes=n, node_features=node_features,\n",
    "                               is_drum=is_drum))\n",
    "            \n",
    "            \n",
    "        # Merge the graphs corresponding to different bars into a single big graph\n",
    "        graphs, _, inc_dict = collate(\n",
    "            Data,\n",
    "            data_list=graphs,\n",
    "            increment=True,\n",
    "            add_batch=True\n",
    "        )\n",
    "        \n",
    "        # Change bars assignment vector name (otherwise, Dataloader's collate\n",
    "        # would overwrite graphs.batch)\n",
    "        graphs.bars = graphs.batch\n",
    "        \n",
    "        # Filter silences in order to get a sparse representation\n",
    "        new_seq_tensor = new_seq_tensor.reshape(-1, new_seq_tensor.shape[-2],\n",
    "                                                new_seq_tensor.shape[-1])\n",
    "        src_mask = src_mask.reshape(-1, src_mask.shape[-1])\n",
    "        new_seq_tensor = new_seq_tensor[seq_acts.reshape(-1).astype(bool)]\n",
    "        src_mask = src_mask[seq_acts.reshape(-1).astype(bool)]\n",
    "        \n",
    "        new_seq_tensor = torch.Tensor(new_seq_tensor)\n",
    "        seq_acts = torch.Tensor(seq_acts)\n",
    "        graphs.x_seq = new_seq_tensor\n",
    "        graphs.x_acts = seq_acts\n",
    "        graphs.src_mask = src_mask\n",
    "        \n",
    "        # Todo: start with torch at mount\n",
    "        #return torch.Tensor(new_seq_tensor), torch.Tensor(seq_acts), graphs, src_mask\n",
    "        return graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d35dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "from typing import Callable\n",
    "from torch_geometric.nn.inits import reset\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter as Param\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter\n",
    "from torch_sparse import SparseTensor, matmul, masked_select_nnz\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (Tensor, Tensor) -> Tensor\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (SparseTensor, Tensor) -> SparseTensor\n",
    "    pass\n",
    "\n",
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return masked_select_nnz(edge_index, edge_mask, layout='coo')\n",
    "\n",
    "def masked_edge_attrs(edge_attrs, edge_mask):\n",
    "    return edge_attrs[edge_mask, :]\n",
    "\n",
    "\n",
    "class RGCNConv(MessagePassing):\n",
    "    r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "    Relational Data with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}_{\\textrm{root}} \\cdot\n",
    "        \\mathbf{x}_i + \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
    "        \\frac{1}{|\\mathcal{N}_r(i)|} \\mathbf{\\Theta}_r \\cdot \\mathbf{x}_j,\n",
    "\n",
    "    where :math:`\\mathcal{R}` denotes the set of relations, *i.e.* edge types.\n",
    "    Edge type needs to be a one-dimensional :obj:`torch.long` tensor which\n",
    "    stores a relation identifier\n",
    "    :math:`\\in \\{ 0, \\ldots, |\\mathcal{R}| - 1\\}` for each edge.\n",
    "\n",
    "    .. note::\n",
    "        This implementation is as memory-efficient as possible by iterating\n",
    "        over each individual relation type.\n",
    "        Therefore, it may result in low GPU utilization in case the graph has a\n",
    "        large number of relations.\n",
    "        As an alternative approach, :class:`FastRGCNConv` does not iterate over\n",
    "        each individual type, but may consume a large amount of memory to\n",
    "        compensate.\n",
    "        We advise to check out both implementations to see which one fits your\n",
    "        needs.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\n",
    "            corresponds to the sizes of source and target dimensionalities.\n",
    "            In case no input features are given, this argument should\n",
    "            correspond to the number of nodes in your graph.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_relations (int): Number of relations.\n",
    "        nn (torch.nn.Module): A neural network :math:`h_{\\mathbf{\\Theta}}` that\n",
    "            maps edge features :obj:`edge_attr` of shape :obj:`[-1,\n",
    "            num_edge_features]` to shape\n",
    "            :obj:`[-1, in_channels * out_channels]`, *e.g.*, defined by\n",
    "            :class:`torch.nn.Sequential`.\n",
    "        num_bases (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the basis-decomposition regularization scheme where\n",
    "            :obj:`num_bases` denotes the number of bases to use.\n",
    "            (default: :obj:`None`)\n",
    "        num_blocks (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the block-diagonal-decomposition regularization scheme where\n",
    "            :obj:`num_blocks` denotes the number of blocks to use.\n",
    "            (default: :obj:`None`)\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
    "            (default: :obj:`\"mean\"`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        num_relations: int,\n",
    "        #num_dists: int,\n",
    "        nn: Callable,\n",
    "        num_bases: Optional[int] = None,\n",
    "        num_blocks: Optional[int] = None,\n",
    "        dropout: Optional[float] = 0.1,\n",
    "        aggr: str = 'mean',\n",
    "        root_weight: bool = True,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(aggr=aggr, node_dim=0, **kwargs)\n",
    "\n",
    "        if num_bases is not None and num_blocks is not None:\n",
    "            raise ValueError('Can not apply both basis-decomposition and '\n",
    "                             'block-diagonal-decomposition at the same time.')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.nn = nn\n",
    "        #self.num_dists = num_dists\n",
    "        self.dropout = dropout\n",
    "        self.num_relations = num_relations\n",
    "        self.num_bases = num_bases\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "        self.in_channels_l = in_channels[0]\n",
    "\n",
    "        if num_bases is not None:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_bases, in_channels[0], out_channels))\n",
    "            self.comp = Parameter(torch.Tensor(num_relations, num_bases))\n",
    "        \n",
    "        elif num_blocks is not None:\n",
    "            assert (in_channels[0] % num_blocks == 0\n",
    "                    and out_channels % num_blocks == 0)\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, num_blocks,\n",
    "                             in_channels[0] // num_blocks,\n",
    "                             out_channels // num_blocks))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        else:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, in_channels[0], out_channels))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        if root_weight:\n",
    "            self.root = Param(torch.Tensor(in_channels[1], out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Param(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        #self.dist_weights = Parameter(torch.Tensor(self.num_dists))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        reset(self.nn)\n",
    "        glorot(self.comp)\n",
    "        glorot(self.root)\n",
    "        #zeros(self.dist_weights)\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
    "                edge_index: Adj, edge_type: OptTensor = None,\n",
    "                edge_attr: OptTensor = None):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x: The input node features. Can be either a :obj:`[num_nodes,\n",
    "                in_channels]` node feature matrix, or an optional\n",
    "                one-dimensional node index tensor (in which case input features\n",
    "                are treated as trainable node embeddings).\n",
    "                Furthermore, :obj:`x` can be of type :obj:`tuple` denoting\n",
    "                source and destination node features.\n",
    "            edge_type: The one-dimensional relation type/index for each edge in\n",
    "                :obj:`edge_index`.\n",
    "                Should be only :obj:`None` in case :obj:`edge_index` is of type\n",
    "                :class:`torch_sparse.tensor.SparseTensor`.\n",
    "                (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert input features to a pair of node features or node indices.\n",
    "        x_l: OptTensor = None\n",
    "        if isinstance(x, tuple):\n",
    "            x_l = x[0]\n",
    "        else:\n",
    "            x_l = x\n",
    "        if x_l is None:\n",
    "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
    "\n",
    "        x_r: Tensor = x_l\n",
    "        if isinstance(x, tuple):\n",
    "            x_r = x[1]\n",
    "\n",
    "        size = (x_l.size(0), x_r.size(0))\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_type = edge_index.storage.value()\n",
    "        assert edge_type is not None\n",
    "\n",
    "        # propagate_type: (x: Tensor)\n",
    "        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)\n",
    "\n",
    "        weight = self.weight\n",
    "        if self.num_bases is not None:  # Basis-decomposition =================\n",
    "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
    "                self.num_relations, self.in_channels_l, self.out_channels)\n",
    "\n",
    "        if self.num_blocks is not None:  # Block-diagonal-decomposition =====\n",
    "\n",
    "            if x_l.dtype == torch.long and self.num_blocks is not None:\n",
    "                raise ValueError('Block-diagonal decomposition not supported '\n",
    "                                 'for non-continuous input features.')\n",
    "\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                h = self.propagate(tmp, x=x_l, size=size)\n",
    "                h = h.view(-1, weight.size(1), weight.size(2))\n",
    "                h = torch.einsum('abc,bcd->abd', h, weight[i])\n",
    "                out += h.contiguous().view(-1, self.out_channels)\n",
    "\n",
    "        else:  # No regularization/Basis-decomposition ========================\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                attr = masked_edge_attrs(edge_attr, edge_type == i)\n",
    "\n",
    "                if x_l.dtype == torch.long:\n",
    "                    out += self.propagate(tmp, x=weight[i, x_l], size=size)\n",
    "                else:\n",
    "                    h = self.propagate(tmp, x=x_l, size=size,\n",
    "                                       edge_attr=attr)\n",
    "                    out = out + (h @ weight[i])\n",
    "\n",
    "        root = self.root\n",
    "        if root is not None:\n",
    "            out += root[x_r] if x_r.dtype == torch.long else x_r @ root\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        #weights = self.dist_weights[edge_attr.view(-1).long()]\n",
    "        #weights = torch.diag(weights)\n",
    "        #return torch.matmul(weights, x_j)\n",
    "        weights = self.nn(edge_attr)\n",
    "        weights = weights[..., :self.in_channels_l]\n",
    "        weights = weights.view(-1, self.in_channels_l)\n",
    "        ret = x_j * weights\n",
    "        ret = F.relu(ret)\n",
    "        ret = F.dropout(ret, p=self.dropout, training=self.training)\n",
    "        return ret\n",
    "    \n",
    "        \n",
    "        \n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        adj_t = adj_t.set_value(None, layout=None)\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, num_relations={self.num_relations})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f1aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch_geometric.nn.conv import GCNConv#, RGCNConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.nn.glob import GlobalAttention\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch_scatter import scatter_mean\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "\n",
    "# Todo: check and think about max_len\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 256):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *                     \\\n",
    "                             (-math.log(10000.0)/d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position*div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=256, hidden_dim=256, output_dim=256, num_layers=2,\n",
    "                 act=True, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert num_layers >= 1\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if num_layers == 1:\n",
    "            self.layers.append(nn.Linear(input_dim, output_dim))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        \n",
    "            for i in range(num_layers-2):\n",
    "                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "            self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.act = act\n",
    "        self.p = dropout\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "            x = layer(x)\n",
    "            if self.act:\n",
    "                x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=240, hidden_size=256, num_layers=2, \n",
    "                 dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True,\n",
    "                            num_layers=num_layers, bidirectional=True)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size=256, output_size=240, num_layers=2, \n",
    "                 dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Linear(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True,\n",
    "                            num_layers=num_layers, bidirectional=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output = self.embedding(x).view(1, 1, -1)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=256, hidden_dim=256, n_layers=3, \n",
    "                 num_relations=3, num_dists=32, batch_norm=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "        edge_nn = nn.Linear(num_dists, input_dim)\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        self.layers.append(RGCNConv(input_dim, hidden_dim,\n",
    "                                    num_relations, edge_nn))\n",
    "        if self.batch_norm:\n",
    "            self.norm_layers.append(BatchNorm(hidden_dim))\n",
    "        \n",
    "        for i in range(n_layers-1):\n",
    "            #edge_nn = nn.Linear(num_dists, input_dim)\n",
    "            self.layers.append(RGCNConv(hidden_dim, hidden_dim,\n",
    "                                        num_relations, edge_nn))\n",
    "            if self.batch_norm:\n",
    "                self.norm_layers.append(BatchNorm(hidden_dim))\n",
    "            \n",
    "        self.p = dropout\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attrs = data.x, data.edge_index, data.edge_attrs\n",
    "        #batch = data.distinct_bars\n",
    "        edge_type = edge_attrs[:, 0]\n",
    "        edge_attr = edge_attrs[:, 1:]\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            residual = x\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "            x = self.layers[i](x, edge_index, edge_type, edge_attr)\n",
    "            if self.batch_norm:\n",
    "                x = self.norm_layers[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = residual + x\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CNNEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_dim=256, dense_dim=256, batch_norm=False,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.conv = nn.Sequential(\n",
    "                # 4*32 --> 8*4*32\n",
    "                nn.Conv2d(1, 8, 3, padding=1),\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(True),\n",
    "                # 8*4*32 --> 8*4*8\n",
    "                nn.MaxPool2d((1, 4), stride=(1, 4)),\n",
    "                # 8*4*8 --> 16*4*8\n",
    "                nn.Conv2d(8, 16, 3, padding=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(True)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # 4*32 --> 8*4*32\n",
    "                nn.Conv2d(1, 8, 3, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                # 8*4*32 --> 8*4*8\n",
    "                nn.MaxPool2d((1, 4), stride=(1, 4)),\n",
    "                # 8*4*8 --> 16*4*8\n",
    "                nn.Conv2d(8, 16, 3, padding=1),\n",
    "                nn.ReLU(True)\n",
    "            )\n",
    "        \n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16*4*8, dense_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dense_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=256, dense_dim=256, batch_norm=False,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim, dense_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dense_dim, 16*4*8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(16, 4, 8))\n",
    "\n",
    "        if batch_norm:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=(1, 4), mode='nearest'),\n",
    "                nn.Conv2d(16, 8, 3, padding=1),\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(8, 1, 3, padding=1)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=(1, 4), mode='nearest'),\n",
    "                nn.Conv2d(16, 8, 3, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(8, 1, 3, padding=1)\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        return x.unsqueeze(1)\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "        \n",
    "        \n",
    "        self.notes_pitch_emb = nn.Linear(self.d_token_pitches, \n",
    "                                               self.d//2)\n",
    "        \n",
    "        self.bn_npe = nn.BatchNorm1d(num_features=self.d//2)\n",
    "        \n",
    "        self.drums_pitch_emb = nn.Linear(self.d_token_pitches, \n",
    "                                               self.d//2)\n",
    "        \n",
    "        self.bn_dpe = nn.BatchNorm1d(num_features=self.d//2)\n",
    "        \n",
    "        self.dur_emb = nn.Linear(self.d_token_dur, self.d//2)\n",
    "        \n",
    "        self.bn_de = nn.BatchNorm1d(num_features=self.d//2)\n",
    "        \n",
    "        self.chord_encoder = nn.Linear(self.d * (self.max_simu_notes-1),\n",
    "                                       self.d)\n",
    "\n",
    "        # Graph encoder\n",
    "        self.graph_encoder = GraphEncoder(dropout=self.dropout, \n",
    "                                          input_dim=self.d,\n",
    "                                          hidden_dim=self.d,\n",
    "                                          n_layers=self.gnn_n_layers,\n",
    "                                          num_relations=self.n_relations,\n",
    "                                          batch_norm=self.batch_norm)\n",
    "        \n",
    "        gate_nn = nn.Sequential(\n",
    "            MLP(input_dim=self.d, output_dim=1, num_layers=1, act=False,\n",
    "                      dropout=self.dropout),\n",
    "            nn.BatchNorm1d(1)\n",
    "        )\n",
    "        self.graph_attention = GlobalAttention(gate_nn)\n",
    "        \n",
    "        #self.context_bar_rnn = nn.GRU(input_size=self.d,\n",
    "        #                              hidden_size=self.d//2,\n",
    "        #                              num_layers=1,\n",
    "        #                              bidirectional=True,\n",
    "        #                              batch_first=True, \n",
    "        #                              dropout=self.dropout)\n",
    "        \n",
    "        self.bars_encoder_attr = nn.Linear(self.n_bars*self.d,\n",
    "                                           self.d)\n",
    "        \n",
    "        \n",
    "        self.cnn_encoder = CNNEncoder(dense_dim=self.d,\n",
    "                                      output_dim=self.d,\n",
    "                                      dropout=0,\n",
    "                                      batch_norm=self.batch_norm)\n",
    "        \n",
    "        self.bars_encoder_struct = nn.Linear(self.n_bars*self.d,\n",
    "                                             self.d)\n",
    "        #self.struct_bar_rnn = nn.GRU(input_size=self.d,\n",
    "        #                              hidden_size=self.d//2,\n",
    "        #                              num_layers=1,\n",
    "        #                              batch_first=True, \n",
    "        #                              dropout=self.dropout)\n",
    "        \n",
    "        self.linear_merge = nn.Linear(2*self.d, self.d)\n",
    "        \n",
    "        self.bn_lm = nn.BatchNorm1d(num_features=self.d)\n",
    "        \n",
    "        # Linear layers that compute the final mu and log_var\n",
    "        # Todo: as parameters\n",
    "        self.linear_mu = nn.Linear(self.d, self.d)\n",
    "        self.linear_log_var = nn.Linear(self.d, self.d)\n",
    "\n",
    "        \n",
    "    def forward(self, x_seq, x_acts, x_graph, src_mask):\n",
    "        \n",
    "        # No start of seq token\n",
    "        x_seq = x_seq[:, 1:, :]\n",
    "        \n",
    "        # Get drums and non drums tensors\n",
    "        drums = x_seq[x_graph.is_drum]\n",
    "        src_mask_drums = src_mask[x_graph.is_drum]\n",
    "        non_drums = x_seq[torch.logical_not(x_graph.is_drum)]\n",
    "        src_mask_non_drums = src_mask[torch.logical_not(x_graph.is_drum)]\n",
    "        \n",
    "        # Permute dimensions to batch_first = False\n",
    "        #drums = drums.permute(1, 0, 2)\n",
    "        #non_drums = non_drums.permute(1, 0, 2)\n",
    "        \n",
    "        # Compute note/drums embeddings\n",
    "        s = drums.size()\n",
    "        drums_pitch = self.drums_pitch_emb(drums[..., :self.d_token_pitches])\n",
    "        drums_pitch = self.bn_dpe(drums_pitch.view(-1, self.d//2))\n",
    "        drums_pitch = drums_pitch.view(s[0], s[1], self.d//2)\n",
    "        drums_dur = self.dur_emb(drums[..., self.d_token_pitches:])\n",
    "        drums_dur = self.bn_de(drums_dur.view(-1, self.d//2))\n",
    "        drums_dur = drums_dur.view(s[0], s[1], self.d//2)\n",
    "        drums = torch.cat((drums_pitch, drums_dur), dim=-1)\n",
    "        #drums = self.dropout_layer(drums)\n",
    "        # [n_nodes x max_simu_notes x d]\n",
    "        \n",
    "        s = non_drums.size()\n",
    "        non_drums_pitch = self.notes_pitch_emb(non_drums[..., :self.d_token_pitches])\n",
    "        non_drums_pitch = self.bn_npe(non_drums_pitch.view(-1, self.d//2))\n",
    "        non_drums_pitch = non_drums_pitch.view(s[0], s[1], self.d//2)\n",
    "        non_drums_dur = self.dur_emb(non_drums[..., self.d_token_pitches:])\n",
    "        non_drums_dur = self.bn_de(non_drums_dur.view(-1, self.d//2))\n",
    "        non_drums_dur = non_drums_dur.view(s[0], s[1], self.d//2)\n",
    "        non_drums = torch.cat((non_drums_pitch, non_drums_dur), dim=-1)\n",
    "        #non_drums = self.dropout_layer(non_drums)\n",
    "        # [n_nodes x max_simu_notes x d]\n",
    "        \n",
    "        #len_drums = self.max_simu_notes - torch.sum(src_mask_drums, dim=1)\n",
    "        #len_non_drums = self.max_simu_notes - torch.sum(src_mask_non_drums, dim=1)\n",
    "        \n",
    "        #drums = pack_padded_sequence(drums, len_drums.cpu().view(-1),\n",
    "        #                             enforce_sorted=False)\n",
    "        #non_drums = pack_padded_sequence(non_drums, len_non_drums.cpu().view(-1),\n",
    "        #                                 enforce_sorted=False)\n",
    "\n",
    "        # Compute chord embeddings both for drums and non drums\n",
    "        drums = self.chord_encoder(drums.view(-1, self.d*(self.max_simu_notes-1)))\n",
    "        non_drums = self.chord_encoder(non_drums.view(-1, self.d*(self.max_simu_notes-1)))\n",
    "        drums = F.relu(drums)\n",
    "        non_drums = F.relu(non_drums)\n",
    "        drums = self.dropout_layer(drums)\n",
    "        non_drums = self.dropout_layer(non_drums)\n",
    "        # [n_nodes x d]\n",
    "        \n",
    "        #hidden = torch.zeros(drums.size(1), )\n",
    "        #drums = self.chord_encoder_drums(drums)[-1]\n",
    "        #non_drums = self.chord_encoder(non_drums)[-1]\n",
    "        #drums = torch.mean(drums, dim=0)\n",
    "        #non_drums = torch.mean(non_drums, dim=0)\n",
    "        \n",
    "        #drums = self.dropout_layer(drums)\n",
    "        #non_drums = self.dropout_layer(non_drums)\n",
    "        \n",
    "        # Merge drums and non-drums\n",
    "        #out = torch.zeros((x_seq.size(0), self.d_model), \n",
    "        #                  device=self.device)\n",
    "        out = torch.zeros((x_seq.size(0), self.d), \n",
    "                          device=self.device, dtype=torch.half)\n",
    "        out[x_graph.is_drum] = drums\n",
    "        out[torch.logical_not(x_graph.is_drum)] = non_drums\n",
    "        # [n_nodes x d]\n",
    "        \n",
    "        #x_graph.x = torch.cat((x_graph.node_features, out), 1)\n",
    "        x_graph.x = out\n",
    "        x_graph.distinct_bars = x_graph.bars + self.n_bars*x_graph.batch\n",
    "        out = self.graph_encoder(x_graph)\n",
    "        # [n_nodes x d]\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            out = self.graph_attention(out,\n",
    "                                       batch=x_graph.distinct_bars)\n",
    "            # [bs x n_bars x d]\n",
    "            \n",
    "        out = out.view(-1, self.n_bars * self.d)\n",
    "        # [bs x n_bars * d]\n",
    "        out_attr = self.bars_encoder_attr(out)\n",
    "        # [bs x d]\n",
    "        \n",
    "        # Process structure\n",
    "        out = self.cnn_encoder(x_acts.view(-1, self.n_tracks,\n",
    "                                                self.resolution*4))\n",
    "        # [bs * n_bars x d]\n",
    "        out = out.view(-1, self.n_bars * self.d)\n",
    "        # [bs x n_bars * d]\n",
    "        out_struct = self.bars_encoder_struct(out)\n",
    "        # [bs x d]\n",
    "        \n",
    "        # Merge attr state and struct state\n",
    "        out = torch.cat((out_attr, out_struct), dim=1)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.linear_merge(out)\n",
    "        out = self.bn_lm(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        # Compute mu and log(std^2)\n",
    "        out = self.dropout_layer(out)\n",
    "        mu = self.linear_mu(out)\n",
    "        log_var = self.linear_log_var(out)\n",
    "        \n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        self.lin_divide = nn.Linear(self.d, 2 * self.d)\n",
    "        \n",
    "        self.bn_ld = nn.BatchNorm1d(num_features=2*self.d)\n",
    "        \n",
    "        self.bars_decoder_attr = nn.Linear(self.d, self.d * self.n_bars)\n",
    "        #self.context_bar_rnn = nn.GRU(input_size=self.d,\n",
    "        #                              hidden_size=self.d//2,\n",
    "        #                              num_layers=1,\n",
    "        #                              bidirectional=True,\n",
    "        #                              batch_first=True,\n",
    "        #                              dropout=self.dropout)\n",
    "        \n",
    "        self.bars_decoder_struct = nn.Linear(self.d, self.d * self.n_bars)\n",
    "        #self.struct_bar_rnn = nn.GRU(input_size=self.d//2,\n",
    "        #                              hidden_size=self.d//2,\n",
    "        #                              num_layers=1,\n",
    "        #                              batch_first=True,\n",
    "        #                              dropout=self.dropout)\n",
    "        \n",
    "        self.cnn_decoder = CNNDecoder(input_dim=self.d,\n",
    "                                      dense_dim=self.d,\n",
    "                                      dropout=0,\n",
    "                                      batch_norm=self.batch_norm)\n",
    "        \n",
    "        self.graph_decoder = GraphEncoder(dropout=self.dropout,\n",
    "                                          input_dim=self.d,\n",
    "                                          hidden_dim=self.d,\n",
    "                                          n_layers=self.gnn_n_layers,\n",
    "                                          num_relations=self.n_relations,\n",
    "                                          batch_norm=self.batch_norm)\n",
    "        \n",
    "        #gate_nn = nn.Sequential(\n",
    "        #    MLP(input_dim=self.d, output_dim=1, num_layers=1, act=False,\n",
    "        #              dropout=self.dropout),\n",
    "        #    nn.BatchNorm1d(1)\n",
    "        #)\n",
    "        #feat_nn = nn.Sequential(\n",
    "        #    MLP(input_dim=self.d, output_dim=self.d//2, num_layers=1,\n",
    "        #              dropout=self.dropout),\n",
    "        #    nn.BatchNorm1d(self.d//2)\n",
    "        #)\n",
    "        #self.graph_attention = GlobalAttention(gate_nn, feat_nn)\n",
    "        \n",
    "        self.chord_decoder = nn.Linear(self.d, self.d*(self.max_simu_notes-1))\n",
    "        #self.chord_decoder = nn.GRU(input_size=self.d,\n",
    "        #                            hidden_size=self.d,\n",
    "        #                            num_layers=1,\n",
    "        #                            dropout=self.dropout)\n",
    "        #self.chord_decoder_drums = nn.GRU(input_size=self.d,\n",
    "        #                                  hidden_size=self.d,\n",
    "        #                                  num_layers=1,\n",
    "        #                                  dropout=self.dropout)\n",
    "        \n",
    "        # Pitch and dur linear layers\n",
    "        self.drums_pitch_emb = nn.Linear(self.d//2, self.d_token_pitches)\n",
    "        self.notes_pitch_emb = nn.Linear(self.d//2, self.d_token_pitches)\n",
    "        self.dur_emb = nn.Linear(self.d//2, self.d_token_dur)\n",
    "        \n",
    "        \n",
    "    def forward_struct(self, z):\n",
    "        # z: [bs x d]\n",
    "        \n",
    "        # Obtain z_structure and z_attributes from z\n",
    "        #z = self.dropout_layer(z)\n",
    "        z = self.lin_divide(z)\n",
    "        z = self.bn_ld(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.dropout_layer(z)\n",
    "        # [bs x 2*d]\n",
    "        \n",
    "        out_struct = z[:, :self.d]\n",
    "        # [bs x d] \n",
    "        out_struct = self.bars_decoder_struct(out_struct)\n",
    "        # [bs x n_bars * d]\n",
    "        \n",
    "        out_struct = self.cnn_decoder(out_struct.reshape(-1, self.d))\n",
    "        \n",
    "        return out_struct\n",
    "\n",
    "\n",
    "    def forward(self, z, x_seq, x_acts, x_graph, src_mask, tgt_mask,\n",
    "                inference=False):\n",
    "        # z: [bs x d]\n",
    "        \n",
    "        # Obtain z_structure and z_attributes from z\n",
    "        #z = self.dropout_layer(z)\n",
    "        z = self.lin_divide(z)\n",
    "        z = self.bn_ld(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.dropout_layer(z)\n",
    "        # [bs x 2*d]\n",
    "        \n",
    "        out_struct = z[:, :self.d]\n",
    "        # [bs x d] \n",
    "        out_struct = self.bars_decoder_struct(out_struct)\n",
    "        # [bs x n_bars * d]\n",
    "        \n",
    "        out_struct = self.cnn_decoder(out_struct.reshape(-1, self.d))\n",
    "        out_struct = out_struct.view(x_acts.size())\n",
    "        \n",
    "        # Decode attributes\n",
    "        out = z[:, self.d:]\n",
    "        # [bs x d]\n",
    "        out = self.bars_decoder_attr(out)\n",
    "        # [bs x n_bars * d]\n",
    "        \n",
    "        # Initialize node features with corresponding z_bar\n",
    "        # and propagate with GNN\n",
    "        _, counts = torch.unique(x_graph.distinct_bars, return_counts=True)\n",
    "        out = out.view(-1, self.d)\n",
    "        out = torch.repeat_interleave(out, counts, axis=0)\n",
    "        # [n_nodes x d]\n",
    "        \n",
    "        # Add one-hot encoding of tracks\n",
    "        # Todo: use also edge info\n",
    "        #x_graph.x = torch.cat((x_graph.node_features, out), 1)\n",
    "        x_graph.x = out\n",
    "        out = self.graph_decoder(x_graph)\n",
    "        # [n_nodes x d]\n",
    "        #print(\"Node decodings:\", node_decs.size())\n",
    "        \n",
    "        \n",
    "        #out = torch.matmul(out, self.chord_decoder.weight)\n",
    "        out = self.chord_decoder(out)\n",
    "        # [n_nodes x max_simu_notes * d]\n",
    "        out = out.view(-1, self.max_simu_notes-1, self.d)\n",
    "        \n",
    "        drums = out[x_graph.is_drum]\n",
    "        non_drums = out[torch.logical_not(x_graph.is_drum)]\n",
    "        # [n_nodes(dr/non_dr) x max_simu_notes x d]\n",
    "        \n",
    "        # Obtain final pitch and dur decodings\n",
    "        # (softmax to be applied outside the model)\n",
    "        non_drums = self.dropout_layer(non_drums)\n",
    "        drums = self.dropout_layer(drums)\n",
    "        \n",
    "        drums_pitch = self.drums_pitch_emb(drums[..., :self.d//2])\n",
    "        drums_dur = self.dur_emb(drums[..., self.d//2:])\n",
    "        drums = torch.cat((drums_pitch, drums_dur), dim=-1)\n",
    "        #drums_pitch = torch.matmul(drums[..., :self.d//2], self.drums_pitch_emb.weight)\n",
    "        #drums_dur = torch.matmul(drums[..., self.d//2:], self.dur_emb.weight)\n",
    "        #drums = torch.cat((drums_pitch, drums_dur), dim=-1)\n",
    "        # [n_nodes(dr) x max_simu_notes x d_token]\n",
    "        non_drums_pitch = self.notes_pitch_emb(non_drums[..., :self.d//2])\n",
    "        non_drums_dur = self.dur_emb(non_drums[..., self.d//2:])\n",
    "        non_drums = torch.cat((non_drums_pitch, non_drums_dur), dim=-1)\n",
    "        #non_drums_pitch = torch.matmul(non_drums[..., :self.d//2], self.notes_pitch_emb.weight)\n",
    "        #non_drums_dur = torch.matmul(non_drums[..., self.d//2:], self.dur_emb.weight)\n",
    "        #non_drums = torch.cat((non_drums_pitch, non_drums_dur), dim=-1)\n",
    "        # [n_nodes(non_dr) x max_simu_notes x d_token]\n",
    "        \n",
    "        # Merge drums and non-drums\n",
    "        out = torch.zeros((x_seq.size(0), x_seq.size(1), x_seq.size(2)),\n",
    "                          device=self.device, dtype=torch.half)\n",
    "        out[x_graph.is_drum] = drums\n",
    "        out[torch.logical_not(x_graph.is_drum)] = non_drums\n",
    "        \n",
    "        out = out.view(x_seq.size())\n",
    "\n",
    "        return out, out_struct\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(**kwargs)\n",
    "        self.decoder = Decoder(\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x_seq, x_acts, x_graph, src_mask, tgt_mask,\n",
    "                inference=False):\n",
    "        \n",
    "        #src_mask = src_mask.view(-1, src_mask.size(-1))\n",
    "        \n",
    "        # Encoder pass\n",
    "        mu, log_var = self.encoder(x_seq, x_acts, x_graph, src_mask)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = torch.exp(0.5*log_var)\n",
    "        z = z * torch.randn_like(z)\n",
    "        #print(\"eps:\", eps.size())\n",
    "        z = z + mu\n",
    "        \n",
    "        # Shifting target sequence and mask for transformer decoder\n",
    "        tgt = x_seq[..., :-1, :]\n",
    "        src_mask = src_mask[:, :-1]\n",
    "        \n",
    "        # Decoder pass\n",
    "        out = self.decoder(z, tgt, x_acts, x_graph, src_mask, tgt_mask,\n",
    "                           inference=inference)\n",
    "        \n",
    "        return out, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e439a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "def print_params(model):\n",
    "    \n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, parameter in model.named_parameters():\n",
    "        \n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "            \n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params += param\n",
    "        \n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4846b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'models/2barsGNNDEF/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(model, 'checkpoint'), map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e258528",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = checkpoint['model_state_dict']\n",
    "params = torch.load(os.path.join(model, 'params'), map_location='cpu')\n",
    "vae = VAE(**params['model'], device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7315f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_state_dict(state_dict)\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b96c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "n_bars = 2\n",
    "bs = 256\n",
    "nw = 4\n",
    "\n",
    "ds_dir = \"/data/cosenza/datasets/preprocessed_2bars/\"\n",
    "dataset = MIDIDataset(ds_dir, n_bars=n_bars)\n",
    "print('Dataset len:', len(dataset))\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=bs, shuffle=False, num_workers=nw)\n",
    "#print(len(subset))\n",
    "\n",
    "\n",
    "train_len = int(0.7 * len(dataset)) \n",
    "valid_len = int(0.01 * len(dataset))\n",
    "test_len = len(dataset) - train_len - valid_len\n",
    "#train_dataset, valid_dataset, test_dataset = random_split(model_dataset, (train_count, valid_count, test_count))\n",
    "tr_set, vl_set, ts_set = random_split(dataset, (train_len, valid_len, test_len))\n",
    "\n",
    "trainloader = DataLoader(tr_set, batch_size=bs, shuffle=True, num_workers=nw)\n",
    "validloader = DataLoader(vl_set, batch_size=bs, shuffle=False, num_workers=nw)\n",
    "#testloader = DataLoader(ts_set, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "tr_len = len(tr_set)\n",
    "vl_len = len(vl_set)\n",
    "ts_len = len(ts_set)\n",
    "\n",
    "print('TR set len:', len(tr_set))\n",
    "print('VL set len:', len(vl_set))\n",
    "print('TS set len:', len(ts_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee356a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, inputs in enumerate(trainloader):\n",
    "\n",
    "        x_graph = inputs.to(device)\n",
    "        x_seq, x_acts, src_mask = x_graph.x_seq, x_graph.x_acts, x_graph.src_mask\n",
    "        tgt_mask = generate_square_subsequent_mask(x_seq.size(-2)-1).to(device)\n",
    "\n",
    "        # Forward pass, get the reconstructions\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs, mu, log_var = vae(x_seq, x_acts, x_graph, src_mask, tgt_mask)\n",
    "\n",
    "        break\n",
    "    \n",
    "\n",
    "seq_rec, acts_rec  = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ee4d2",
   "metadata": {},
   "source": [
    "Reconstruct activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pitches_accuracy(seq_rec, x_seq, is_drum=None, drums=None):\n",
    "        \n",
    "    if drums is not None:\n",
    "        if drums:\n",
    "            seq_rec = seq_rec[is_drum]\n",
    "            x_seq = x_seq[is_drum]\n",
    "        else:\n",
    "            seq_rec = seq_rec[torch.logical_not(is_drum)]\n",
    "            x_seq = x_seq[torch.logical_not(is_drum)]\n",
    "\n",
    "    pitches_rec = F.softmax(seq_rec[..., :131], dim=-1)\n",
    "    pitches_rec = torch.argmax(pitches_rec, dim=-1)\n",
    "    pitches_true = torch.argmax(x_seq[..., :131], dim=-1)\n",
    "\n",
    "    #print(\"All EOS pitches?\", torch.all(pitches_rec == 129))\n",
    "\n",
    "    mask = (pitches_true != 130)\n",
    "    #mask = torch.logical_and(pitches_true != 128,\n",
    "     #                        pitches_true != 129)\n",
    "    #mask = torch.logical_and(mask,\n",
    "     #                        pitches_true != 130)\n",
    "\n",
    "    preds_pitches = (pitches_rec == pitches_true)\n",
    "    preds_pitches = torch.logical_and(preds_pitches, mask)\n",
    "\n",
    "    return torch.sum(preds_pitches) / torch.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ee032",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seq = x_seq[..., 1:, :]\n",
    "_pitches_accuracy(seq_rec, new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "print(acts_rec.size())\n",
    "print(x_acts.size())\n",
    "print(x_seq.size())\n",
    "print(seq_rec.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39579d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_struct(s):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.pcolormesh(s.detach().cpu().numpy(), edgecolors='k', linewidth=1)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    plt.xticks(range(0, s.size(1), 8), range(1, 5))\n",
    "    plt.yticks(range(0, 4), range(1, 5))\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcd9c9",
   "metadata": {},
   "source": [
    "Reconstructed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c225d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.sigmoid(acts_rec[3])\n",
    "s[s > 0.5] = 1\n",
    "s[s <= 0.5] = 0\n",
    "\n",
    "plot_struct(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c2e43",
   "metadata": {},
   "source": [
    "Real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02410420",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = x_acts[3]\n",
    "plot_struct(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d20d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dense reconstruction from sparse representation\n",
    "\n",
    "def dense_from_sparse(seq, acts, bars):\n",
    "    \n",
    "    dtype = seq.dtype\n",
    "    \n",
    "    acts = acts.view(-1, bars, acts.size(-2), acts.size(-1))\n",
    "    dtype = seq.dtype\n",
    "    seq_dense = torch.zeros((acts.size(0), acts.size(1), acts.size(2),\n",
    "                             acts.size(3), seq.size(-2), seq.size(-1)),\n",
    "                            dtype=dtype).to(device)\n",
    "\n",
    "    size = seq_dense.size()\n",
    "\n",
    "    seq_dense = seq_dense.view(-1, seq_dense.size(-2), seq_dense.size(-1))\n",
    "\n",
    "    silence = torch.zeros((seq_dense.size(-2), seq_dense.size(-1)),\n",
    "                            dtype=dtype).to(device)\n",
    "    silence[:, 129] = 1. # eos token\n",
    "\n",
    "    seq_dense[acts.bool().view(-1)] = seq\n",
    "    seq_dense[torch.logical_not(acts.bool().view(-1))] = silence\n",
    "\n",
    "    seq_dense = seq_dense.view(size)\n",
    "    \n",
    "    return seq_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_rec_dense = dense_from_sparse(seq_rec, x_acts, bars=n_bars)\n",
    "x_seq_dense = dense_from_sparse(x_seq, x_acts, bars=n_bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ccb213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse bars dimension\n",
    "seq_rec_dense = seq_rec_dense.permute(0, 2, 1, 3, 4, 5)\n",
    "x_seq_dense = x_seq_dense.permute(0, 2, 1, 3, 4, 5)\n",
    "\n",
    "print(seq_rec_dense.size())\n",
    "\n",
    "size = [\n",
    "    seq_rec_dense.size(0),\n",
    "    seq_rec_dense.size(1),\n",
    "    -1,\n",
    "    seq_rec_dense.size(4),\n",
    "    seq_rec_dense.size(5)\n",
    "]\n",
    "\n",
    "seq_rec_dense = seq_rec_dense.reshape(size)\n",
    "size[-2] = x_seq_dense.size(-2)\n",
    "x_seq_dense = x_seq_dense.reshape(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_real = x_seq_dense[12]\n",
    "music_rec = seq_rec_dense[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 8\n",
    "\n",
    "#tracks = [drum_track, bass_track, guitar_track, strings_track]\n",
    "import copy\n",
    "\n",
    "def from_tensor_to_muspy(music_tensor, track_data):\n",
    "    \n",
    "    tracks = []\n",
    "    \n",
    "    for tr in range(music_tensor.size(0)):\n",
    "        \n",
    "        notes = []\n",
    "        \n",
    "        for ts in range(music_tensor.size(1)):\n",
    "            for note in range(music_tensor.size(2)):\n",
    "                \n",
    "                pitch = music_tensor[tr, ts, note, :131]\n",
    "                pitch = torch.argmax(pitch)\n",
    "\n",
    "                if pitch == 129:\n",
    "                    break\n",
    "                \n",
    "                if pitch != 128:\n",
    "                    dur = music_tensor[tr, ts, note, 131:]\n",
    "                    dur = torch.argmax(dur) + 1\n",
    "                    \n",
    "                    if dur == 97 or dur == 98 or dur == 99:\n",
    "                        dur = 4\n",
    "                        continue\n",
    "                    \n",
    "                    notes.append(muspy.Note(ts, pitch.item(), dur.item(), 64))\n",
    "                    #notes.append(muspy.Note(ts, pitch.item(), dur, 64))\n",
    "        \n",
    "        if track_data[tr][0] == 'Drums':\n",
    "            track = muspy.Track(name='Drums', is_drum=True, notes=copy.deepcopy(notes))\n",
    "        else:\n",
    "            track = muspy.Track(name=track_data[tr][0], \n",
    "                                program=track_data[tr][1],\n",
    "                                notes=copy.deepcopy(notes))\n",
    "        tracks.append(track)\n",
    "    \n",
    "    meta = muspy.Metadata(title='prova')\n",
    "    music = muspy.Music(tracks=tracks, metadata=meta, resolution=RESOLUTION)\n",
    "    \n",
    "    return music\n",
    "\n",
    "\n",
    "track_data = [('Drums', -1), ('Bass', 34), ('Guitar', 1), ('Strings', 41)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae906df2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import muspy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "prefix = \"data/music/\"\n",
    "\n",
    "real = from_tensor_to_muspy(music_real, track_data)\n",
    "\n",
    "fig, axs_ = plt.subplots(4, sharex=True, figsize=(10,10))\n",
    "fig.subplots_adjust(hspace=0)\n",
    "axs = axs_.tolist()\n",
    "muspy.show_pianoroll(music=real, yticklabel='off', grid_axis='off', axs=axs)\n",
    "plt.savefig(prefix + \"real\" + \".png\", dpi=200)\n",
    "muspy.write_midi(prefix + \"real\" + \".mid\", real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02ceb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rec = from_tensor_to_muspy(music_rec, track_data)\n",
    "\n",
    "fig, axs_ = plt.subplots(4, sharex=True, figsize=(10,10))\n",
    "fig.subplots_adjust(hspace=0)\n",
    "axs = axs_.tolist()\n",
    "\n",
    "muspy.show_pianoroll(rec, yticklabel='off', grid_axis='off', axs=axs)\n",
    "plt.savefig(prefix + \"rec\" + \".png\", dpi=200)\n",
    "muspy.write_midi(prefix + \"rec\" + \".mid\", rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee6b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"data/music/file\"\n",
    "\n",
    "for i in range(10):\n",
    "    music_tensor = dataset[20+i][0]\n",
    "    music = from_tensor_to_muspy(music_tensor, track_data)\n",
    "    muspy.show_pianoroll(music, yticklabel='off', grid_axis='off')\n",
    "    plt.savefig(prefix + str(i) + \".png\")\n",
    "    muspy.write_midi(prefix + str(i) + \".mid\", music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_tracks': 4,\n",
    "    'dropout': 0.1,\n",
    "    'd_token': 230,\n",
    "    'd_model': 256,\n",
    "    'n_head_transf': 2,\n",
    "    'n_layers_transf': 2,\n",
    "    'gnn_input_dim': 256 + 4,\n",
    "    'gnn_n_layers': 3,\n",
    "    'd_latent': 256,\n",
    "    'resolution': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a095f",
   "metadata": {},
   "source": [
    "Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c878bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.normal(torch.zeros((256, 512),\n",
    "                             device=vae.decoder.device),\n",
    "                 torch.ones((256, 512),\n",
    "                            device=vae.decoder.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c372d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain z_structure and z_attributes from z\n",
    "#z = self.dropout_layer(z)\n",
    "z = vae.decoder.lin_divide(z)\n",
    "z = vae.decoder.bn_ld(z)\n",
    "z = F.relu(z)\n",
    "z = vae.decoder.dropout_layer(z)\n",
    "# [bs x 2*d]\n",
    "\n",
    "out_struct = z[:, :vae.decoder.d]\n",
    "# [bs x d] \n",
    "out_struct = vae.decoder.bars_decoder_struct(out_struct)\n",
    "# [bs x n_bars * d]\n",
    "\n",
    "out_struct = vae.decoder.cnn_decoder(out_struct.reshape(-1, vae.decoder.d))\n",
    "out_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_struct.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28bcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acts_rec = torch.sigmoid(out_struct.view(256, 2, 4, 32))\n",
    "acts_rec[acts_rec < 0.5] = 0\n",
    "acts_rec[acts_rec >= 0.5] = 1\n",
    "acts_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dfd7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.pcolormesh(acts_rec[1, 0].detach().cpu().numpy(), edgecolors='k', linewidth=1)\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal')\n",
    "plt.xticks(range(0, acts_rec[1, 0].size(1), 8), range(1, 5))\n",
    "plt.yticks(range(0, 4), range(1, 5))\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af688cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "acts_rec[0].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
