{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmanueleCosenza/Polyphemus/blob/main/midi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "50lpUn9bO0ug"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cosenza/thesis/Polyphemus\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  main\r\n",
      "* sparse\r\n"
     ]
    }
   ],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -C data -xvzf data/lmd_matched.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "She0QbN5Kopo",
    "outputId": "0f3fb4c7-bd7d-4ee4-b2cd-d567d8e490db"
   },
   "outputs": [],
   "source": [
    "# Install the required music libraries\n",
    "#!pip3 install muspy\n",
    "#!pip3 install pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uveQkY7O0CF",
    "outputId": "12e1f638-ee78-4617-844a-10e9a26c298e"
   },
   "outputs": [],
   "source": [
    "# Install torch_geometric\n",
    "#!v=$(python3 -c \"import torch; print(torch.__version__)\"); \\\n",
    "#pip3 install torch-scatter -f https://data.pyg.org/whl/torch-${v}.html; \\\n",
    "#pip3 install torch-sparse -f https://data.pyg.org/whl/torch-${v}.html; \\\n",
    "#pip3 install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "B45l1513wJ1Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import muspy\n",
    "from itertools import product\n",
    "import pypianoroll as pproll\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class MIDIPreprocessor():\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def preprocess_dataset(self, dir, early_exit=None):\n",
    "        pass\n",
    "    \n",
    "    def preprocess_file(self, f):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Todo: to config file (or separate files)\n",
    "MAX_SIMU_NOTES = 16 # 14 + SOS and EOS\n",
    "\n",
    "PITCH_SOS = 128\n",
    "PITCH_EOS = 129\n",
    "PITCH_PAD = 130\n",
    "DUR_SOS = 96\n",
    "DUR_EOS = 97\n",
    "DUR_PAD = 98\n",
    "\n",
    "MAX_DUR = 96\n",
    "\n",
    "# Number of time steps per quarter note\n",
    "# To get bar resolution -> RESOLUTION*4\n",
    "RESOLUTION = 8 \n",
    "NUM_BARS = 2\n",
    "\n",
    "\n",
    "def preprocess_file(filepath, dest_dir, num_samples):\n",
    "\n",
    "    saved_samples = 0\n",
    "\n",
    "    print(\"Preprocessing file \" + filepath)\n",
    "\n",
    "    # Load the file both as a pypianoroll song and a muspy song\n",
    "    # (Need to load both since muspy.to_pypianoroll() is expensive)\n",
    "    try:\n",
    "        pproll_song = pproll.read(filepath, resolution=RESOLUTION)\n",
    "        muspy_song = muspy.read(filepath)\n",
    "    except Exception as e:\n",
    "        print(\"Song skipped (Invalid song format)\")\n",
    "        return 0\n",
    "    \n",
    "    # Only accept songs that have a time signature of 4/4 and no time changes\n",
    "    for t in muspy_song.time_signatures:\n",
    "        if t.numerator != 4 or t.denominator != 4:\n",
    "            print(\"Song skipped ({}/{} time signature)\".\n",
    "                            format(t.numerator, t.denominator))\n",
    "            return 0\n",
    "\n",
    "    # Gather tracks of pypianoroll song based on MIDI program number\n",
    "    drum_tracks = []\n",
    "    bass_tracks = []\n",
    "    guitar_tracks = []\n",
    "    strings_tracks = []\n",
    "\n",
    "    for track in pproll_song.tracks:\n",
    "        if track.is_drum:\n",
    "            #continue\n",
    "            track.name = 'Drums'\n",
    "            drum_tracks.append(track)\n",
    "        elif 0 <= track.program <= 31:\n",
    "            track.name = 'Guitar'\n",
    "            guitar_tracks.append(track)\n",
    "        elif 32 <= track.program <= 39:\n",
    "            track.name = 'Bass'\n",
    "            bass_tracks.append(track)\n",
    "        else:\n",
    "            # Tracks with program > 39 are all considered as strings tracks\n",
    "            # and will be merged into a single track later on\n",
    "            strings_tracks.append(track)\n",
    "\n",
    "    # Filter song if it does not contain drum, guitar, bass or strings tracks\n",
    "    #if not guitar_tracks \\\n",
    "    if not drum_tracks or not guitar_tracks \\\n",
    "            or not bass_tracks or not strings_tracks:\n",
    "        print(\"Song skipped (does not contain drum or \"\n",
    "                \"guitar or bass or strings tracks)\")\n",
    "        return 0\n",
    "    \n",
    "    # Merge strings tracks into a single pypianoroll track\n",
    "    strings = pproll.Multitrack(tracks=strings_tracks)\n",
    "    strings_track = pproll.Track(pianoroll=strings.blend(mode='max'),\n",
    "                                 program=48, name='Strings')\n",
    "\n",
    "    combinations = list(product(drum_tracks, bass_tracks, guitar_tracks))\n",
    "    #combinations = list(product(bass_tracks, guitar_tracks))\n",
    "\n",
    "    # Single instruments can have multiple tracks.\n",
    "    # Consider all possible combinations of drum, bass, and guitar tracks\n",
    "    for i, combination in enumerate(combinations):\n",
    "\n",
    "        print(\"Processing combination\", i+1, \"of\", len(combinations))\n",
    "        \n",
    "        # Process combination (called 'subsong' from now on)\n",
    "        drum_track, bass_track, guitar_track = combination\n",
    "        tracks = [drum_track, bass_track, guitar_track, strings_track]\n",
    "        #bass_track, guitar_track = combination\n",
    "        #tracks = [bass_track, guitar_track, strings_track]\n",
    "        \n",
    "        pproll_subsong = pproll.Multitrack(\n",
    "            tracks=tracks,\n",
    "            tempo=pproll_song.tempo,\n",
    "            resolution=RESOLUTION\n",
    "        )\n",
    "        muspy_subsong = muspy.from_pypianoroll(pproll_subsong)\n",
    "        \n",
    "        tracks_notes = [track.notes for track in muspy_subsong.tracks]\n",
    "        \n",
    "        # Obtain length of subsong (maximum of each track's length)\n",
    "        length = 0\n",
    "        for notes in tracks_notes:\n",
    "            track_length = max(note.end for note in notes) if notes else 0\n",
    "            length = max(length, track_length)\n",
    "        length += 1\n",
    "\n",
    "        # Add timesteps until length is a multiple of RESOLUTION\n",
    "        length = length if length%(RESOLUTION*4) == 0 \\\n",
    "                            else length + (RESOLUTION*4-(length%(RESOLUTION*4)))\n",
    "\n",
    "\n",
    "        tracks_tensors = []\n",
    "        tracks_activations = []\n",
    "\n",
    "        # Todo: adapt to velocity\n",
    "        for notes in tracks_notes:\n",
    "\n",
    "            # Initialize encoder-ready track tensor\n",
    "            # track_tensor: (length x max_simu_notes x 2 (or 3 if velocity))\n",
    "            # The last dimension contains pitches and durations (and velocities)\n",
    "            # int16 is enough for small to medium duration values\n",
    "            track_tensor = np.zeros((length, MAX_SIMU_NOTES, 2), np.int16)\n",
    "\n",
    "            track_tensor[:, :, 0] = PITCH_PAD\n",
    "            track_tensor[:, 0, 0] = PITCH_SOS\n",
    "            track_tensor[:, :, 1] = DUR_PAD\n",
    "            track_tensor[:, 0, 1] = DUR_SOS\n",
    "\n",
    "            # Keeps track of how many notes have been stored in each timestep\n",
    "            # (int8 imposes that MAX_SIMU_NOTES < 256)\n",
    "            notes_counter = np.ones(length, dtype=np.int8)\n",
    "\n",
    "            # Todo: np.put_along_axis?\n",
    "            for note in notes:\n",
    "                # Insert note in the lowest position available in the timestep\n",
    "                \n",
    "                t = note.time\n",
    "                \n",
    "                if notes_counter[t] >= MAX_SIMU_NOTES-1:\n",
    "                    # Skip note if there is no more space\n",
    "                    continue\n",
    "                \n",
    "                pitch = max(min(note.pitch, 127), 0)\n",
    "                track_tensor[t, notes_counter[t], 0] = pitch\n",
    "                dur = max(min(MAX_DUR, note.duration), 1)\n",
    "                track_tensor[t, notes_counter[t], 1] = dur-1\n",
    "                notes_counter[t] += 1\n",
    "            \n",
    "            # Add end of sequence token\n",
    "            track_tensor[np.arange(0, length), notes_counter, 0] = PITCH_EOS\n",
    "            track_tensor[np.arange(0, length), notes_counter, 1] = DUR_EOS\n",
    "            \n",
    "            # Get track activations, a boolean tensor indicating whether notes\n",
    "            # are being played in a timestep (sustain does not count)\n",
    "            # (needed for graph rep.)\n",
    "            activations = np.array(notes_counter-1, dtype=bool)\n",
    "            \n",
    "            tracks_tensors.append(track_tensor)\n",
    "            tracks_activations.append(activations)\n",
    "        \n",
    "        # (#tracks x length x max_simu_notes x 2 (or 3))\n",
    "        subsong_tensor = np.stack(tracks_tensors, axis=0)\n",
    "\n",
    "        # (#tracks x length)\n",
    "        subsong_activations = np.stack(tracks_activations, axis=0)\n",
    "\n",
    "\n",
    "        # Slide window over 'subsong_tensor' and 'subsong_activations' along the\n",
    "        # time axis (2nd dimension) with the stride of a bar\n",
    "        # Todo: np.lib.stride_tricks.as_strided(song_proll)\n",
    "        for i in range(0, length-NUM_BARS*RESOLUTION*4+1, RESOLUTION*4):\n",
    "            \n",
    "            # Get the sequence and its activations\n",
    "            seq_tensor = subsong_tensor[:, i:i+NUM_BARS*RESOLUTION*4, :]\n",
    "            seq_acts = subsong_activations[:, i:i+NUM_BARS*RESOLUTION*4]\n",
    "            seq_tensor = np.copy(seq_tensor)\n",
    "            seq_acts = np.copy(seq_acts)\n",
    "\n",
    "            if NUM_BARS > 1:\n",
    "                # Skip sequence if it contains more than one bar of consecutive\n",
    "                # silence in at least one track\n",
    "                bars = seq_acts.reshape(seq_acts.shape[0], NUM_BARS, -1)\n",
    "                bars_acts = np.any(bars, axis=2)\n",
    "\n",
    "                if 1 in np.diff(np.where(bars_acts == 0)[1]):\n",
    "                    continue\n",
    "                    \n",
    "                # Skip sequence if it contains one bar of complete silence\n",
    "                # (in terms of note activations)\n",
    "                silences = np.logical_not(np.any(bars_acts, axis=0))\n",
    "                if np.any(silences):\n",
    "                    continue\n",
    "                \n",
    "            else:\n",
    "                # In the case of just 1 bar, skip it if all tracks are silenced\n",
    "                bar_acts = np.any(seq_acts, axis=1)\n",
    "                if not np.any(bar_acts):\n",
    "                    continue\n",
    "            \n",
    "            # Randomly transpose the pitches of the sequence (-5 to 6 semitones)\n",
    "            # Not considering pad, sos, eos tokens\n",
    "            # Not transposing drums/percussions\n",
    "            shift = np.random.choice(np.arange(-5, 7), 1)\n",
    "            cond = (seq_tensor[1:, :, :, 0] != PITCH_PAD) &                     \\\n",
    "                   (seq_tensor[1:, :, :, 0] != PITCH_SOS) &                     \\\n",
    "                   (seq_tensor[1:, :, :, 0] != PITCH_EOS)\n",
    "            #cond = (seq_tensor[:, :, :, 0] != PITCH_PAD) &                     \\\n",
    "            #       (seq_tensor[:, :, :, 0] != PITCH_SOS) &                     \\\n",
    "            #       (seq_tensor[:, :, :, 0] != PITCH_EOS)\n",
    "            non_perc = seq_tensor[1:, ...]\n",
    "            #non_perc = seq_tensor\n",
    "            non_perc[cond, 0] += shift\n",
    "            non_perc[cond, 0] = np.clip(non_perc[cond, 0], a_min=0, a_max=127)\n",
    "\n",
    "            # Save sample (seq_tensor and seq_acts) to file\n",
    "            curr_sample = str(num_samples + saved_samples)\n",
    "            sample_filepath = os.path.join(dest_dir, curr_sample)\n",
    "            np.savez(sample_filepath, seq_tensor=seq_tensor, seq_acts=seq_acts)\n",
    "\n",
    "            saved_samples += 1\n",
    "\n",
    "\n",
    "    print(\"File preprocessing finished. Saved samples:\", saved_samples)\n",
    "    print()\n",
    "\n",
    "    return saved_samples\n",
    "\n",
    "\n",
    "\n",
    "# Total number of files: 116189\n",
    "# Number of unique files: 45129\n",
    "def preprocess_dataset(dataset_dir, dest_dir, num_files=45129, early_exit=None):\n",
    "\n",
    "    files_dict = {}\n",
    "    seen = 0\n",
    "    tot_samples = 0\n",
    "    not_filtered = 0\n",
    "    finished = False\n",
    "    \n",
    "    print(\"Starting preprocessing\")\n",
    "    \n",
    "    progress_bar = tqdm(range(early_exit)) if early_exit is not None else tqdm(range(num_files))\n",
    "    start = time.time()\n",
    "\n",
    "    # Visit recursively the directories inside the dataset directory\n",
    "    for dirpath, dirs, files in os.walk(dataset_dir):\n",
    "\n",
    "        # Sort alphabetically the found directories\n",
    "        # (to help guess the remaining time) \n",
    "        dirs.sort()\n",
    "        \n",
    "        print(\"Current path:\", dirpath)\n",
    "        print()\n",
    "\n",
    "        for f in files:\n",
    "            \n",
    "            seen += 1\n",
    "\n",
    "            if f in files_dict:\n",
    "                # Skip already seen file\n",
    "                files_dict[f] += 1\n",
    "                continue\n",
    "\n",
    "            # File never seen before, add to dictionary of files\n",
    "            # (from filename to # of occurrences)\n",
    "            files_dict[f] = 1\n",
    "\n",
    "            # Preprocess file\n",
    "            filepath = os.path.join(dirpath, f)\n",
    "            n_saved = preprocess_file(filepath, dest_dir, tot_samples)\n",
    "\n",
    "            tot_samples += n_saved\n",
    "            if n_saved > 0:\n",
    "                not_filtered += 1\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            # Todo: also print # of processed (not filtered) files\n",
    "            #       and # of produced sequences (samples)\n",
    "            print(\"Total number of seen files:\", seen)\n",
    "            print(\"Number of unique files:\", len(files_dict))\n",
    "            print(\"Total number of non filtered songs:\", not_filtered)\n",
    "            print(\"Total number of saved samples:\", tot_samples)\n",
    "            print()\n",
    "\n",
    "            # Exit when a maximum number of files has been processed (if set)\n",
    "            if early_exit != None and len(files_dict) >= early_exit:\n",
    "                finished = True\n",
    "                break\n",
    "\n",
    "        if finished:\n",
    "            break\n",
    "    \n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"Preprocessing completed in (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "              .format(int(hours),int(minutes),seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYc5y-CYyetK"
   },
   "outputs": [],
   "source": [
    "!rm -rf data/preprocessed/\n",
    "!mkdir data/preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqnubg3oP4ES",
    "outputId": "40cc38a2-1f7d-4f6f-e6c9-9e14dfc7f683"
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'data/lmd_matched/Y/G/'\n",
    "dest_dir = 'data/preprocessed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG88mekfrrcp"
   },
   "source": [
    "Check preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dataset(dataset_dir, dest_dir, early_exit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JlP6iUNugNtP"
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(dest_dir, \"5.npz\")\n",
    "data = np.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VUpOEObhwYQ",
    "outputId": "aac6e029-93b1-485f-f13a-2a00abedbc7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 64, 16, 2)\n",
      "(4, 64)\n"
     ]
    }
   ],
   "source": [
    "print(data[\"seq_tensor\"].shape)\n",
    "print(data[\"seq_acts\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6NA5IAAmtK8",
    "outputId": "6e661b3a-05a1-4e2d-9a3d-e1c037b4d04f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128,  96],\n",
       "       [129,  97],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98],\n",
       "       [130,  98]], dtype=int16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"seq_tensor\"][0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C19X9m-3iMlm"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zymqD-UqR8wq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "import itertools\n",
    "from torch_geometric.data.collate import collate\n",
    "\n",
    "\n",
    "def unpackbits(x, num_bits):\n",
    "\n",
    "    if np.issubdtype(x.dtype, np.floating):\n",
    "        raise ValueError(\"numpy data type needs to be int-like\")\n",
    "\n",
    "    xshape = list(x.shape)\n",
    "    x = x.reshape([-1, 1])\n",
    "    mask = 2**np.arange(num_bits, dtype=x.dtype).reshape([1, num_bits])\n",
    "\n",
    "    return (x & mask).astype(bool).astype(int).reshape(xshape + [num_bits])\n",
    "\n",
    "\n",
    "class MIDIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir, n_bars=2):\n",
    "        self.dir = dir\n",
    "        _, _, files = next(os.walk(self.dir))\n",
    "        self.len = len(files)\n",
    "        self.n_bars = n_bars\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    \n",
    "    def __get_track_edges(self, acts, edge_type_ind=0):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        \n",
    "        # Create node labels\n",
    "        labels = np.zeros(acts.shape)\n",
    "        acts_inds = np.where(acts == 1)\n",
    "        num_nodes = len(acts_inds[0])\n",
    "        labels[acts_inds] = np.arange(num_nodes)\n",
    "        labels = labels.transpose()\n",
    "\n",
    "        track_edges = []\n",
    "\n",
    "        for track in range(a_t.shape[1]):\n",
    "            tr_inds = list(inds[inds[:,1] == track])\n",
    "            e_inds = [(tr_inds[i],\n",
    "                    tr_inds[i+1]) for i in range(len(tr_inds)-1)]\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, e[1][0]-e[0][0]) for e in e_inds]\n",
    "            track_edges.extend(edges)\n",
    "\n",
    "        return np.array(track_edges, dtype='long')\n",
    "\n",
    "    \n",
    "    def __get_onset_edges(self, acts, edge_type_ind=1):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        # Create node labels\n",
    "        labels = np.zeros(acts.shape)\n",
    "        acts_inds = np.where(acts == 1)\n",
    "        num_nodes = len(acts_inds[0])\n",
    "        labels[acts_inds] = np.arange(num_nodes)\n",
    "        labels = labels.transpose()\n",
    "\n",
    "        onset_edges = []\n",
    "\n",
    "        for i in ts_inds:\n",
    "            ts_acts_inds = list(inds[inds[:,0] == i])\n",
    "            if len(ts_acts_inds) < 2:\n",
    "                continue\n",
    "            e_inds = list(itertools.combinations(ts_acts_inds, 2))\n",
    "            edges = [(labels[tuple(e[0])], labels[tuple(e[1])], edge_type_ind, 0) for e in e_inds]\n",
    "            inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "            onset_edges.extend(edges)\n",
    "            onset_edges.extend(inv_edges)\n",
    "\n",
    "        return np.array(onset_edges, dtype='long')\n",
    "\n",
    "\n",
    "    def __get_next_edges(self, acts, edge_type_ind=2):\n",
    "\n",
    "        a_t = acts.transpose()\n",
    "        inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "        ts_acts = np.any(a_t, axis=1)\n",
    "        ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "        # Create node labels\n",
    "        labels = np.zeros(acts.shape)\n",
    "        acts_inds = np.where(acts == 1)\n",
    "        num_nodes = len(acts_inds[0])\n",
    "        labels[acts_inds] = np.arange(num_nodes)\n",
    "        labels = labels.transpose()\n",
    "\n",
    "        next_edges = []\n",
    "\n",
    "        for i in range(len(ts_inds)-1):\n",
    "\n",
    "            ind_s = ts_inds[i]\n",
    "            ind_e = ts_inds[i+1]\n",
    "            s = inds[inds[:,0] == ind_s]\n",
    "            e = inds[inds[:,0] == ind_e]\n",
    "\n",
    "            e_inds = [t for t in list(itertools.product(s, e)) if t[0][1] != t[1][1]]\n",
    "            edges = [(labels[tuple(e[0])],labels[tuple(e[1])], edge_type_ind, ind_e-ind_s) for e in e_inds]\n",
    "\n",
    "            next_edges.extend(edges)\n",
    "\n",
    "        return np.array(next_edges, dtype='long')\n",
    "    \n",
    "    \n",
    "    def _get_node_features(self, acts, num_nodes):\n",
    "        \n",
    "        num_tracks = acts.shape[0]\n",
    "        features = torch.zeros((num_nodes, num_tracks), dtype=torch.float)\n",
    "        features[np.arange(num_nodes), np.stack(np.where(acts))[0]] = 1.\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Load tensors\n",
    "        sample_path = os.path.join(self.dir, str(idx) + \".npz\")\n",
    "        data = np.load(sample_path)\n",
    "        seq_tensor = data[\"seq_tensor\"]\n",
    "        seq_acts = data[\"seq_acts\"]\n",
    "        \n",
    "        # From (#tracks x #timesteps x ...) to (#bars x #tracks x #timesteps x ...)\n",
    "        seq_tensor = seq_tensor.reshape(seq_tensor.shape[0], self.n_bars, -1,\n",
    "                                        seq_tensor.shape[2], seq_tensor.shape[3])\n",
    "        seq_tensor = seq_tensor.transpose(1, 0, 2, 3, 4)\n",
    "        seq_acts = seq_acts.reshape(seq_acts.shape[0], self.n_bars, -1)\n",
    "        seq_acts = seq_acts.transpose(1, 0, 2)\n",
    "        \n",
    "        # Construct src_key_padding_mask (PAD = 130)\n",
    "        src_mask = torch.from_numpy((seq_tensor[..., 0] == 130))\n",
    "\n",
    "        # From decimals to one-hot (pitch)\n",
    "        pitches = seq_tensor[..., 0]\n",
    "        onehot_p = np.zeros(\n",
    "            (pitches.shape[0]*pitches.shape[1]*pitches.shape[2]*pitches.shape[3],\n",
    "             131), \n",
    "            dtype=float\n",
    "        )\n",
    "        onehot_p[np.arange(0, onehot_p.shape[0]), pitches.reshape(-1)] = 1.\n",
    "        onehot_p = onehot_p.reshape(pitches.shape[0], pitches.shape[1], \n",
    "                                    pitches.shape[2], pitches.shape[3], 131)\n",
    "        \n",
    "        # From decimals to one-hot (dur)\n",
    "        durs = seq_tensor[..., 1]\n",
    "        onehot_d = np.zeros(\n",
    "            (durs.shape[0]*durs.shape[1]*durs.shape[2]*durs.shape[3],\n",
    "             99),\n",
    "            dtype=float\n",
    "        )\n",
    "        onehot_d[np.arange(0, onehot_d.shape[0]), durs.reshape(-1)] = 1.\n",
    "        onehot_d = onehot_d.reshape(durs.shape[0], durs.shape[1], \n",
    "                                    durs.shape[2], durs.shape[3], 99)\n",
    "        \n",
    "        # Concatenate pitches and durations\n",
    "        new_seq_tensor = np.concatenate((onehot_p, onehot_d),\n",
    "                                        axis=-1)\n",
    "        \n",
    "        graphs = []\n",
    "        \n",
    "        # Iterate over bars and construct a graph for each bar\n",
    "        for i in range(self.n_bars):\n",
    "            \n",
    "            # Get edges from boolean activations\n",
    "            # Todo: optimize and refactor\n",
    "            track_edges = self.__get_track_edges(seq_acts[i])\n",
    "            onset_edges = self.__get_onset_edges(seq_acts[i])\n",
    "            next_edges = self.__get_next_edges(seq_acts[i])\n",
    "            edges = [track_edges, onset_edges, next_edges]\n",
    "            \n",
    "            # Concatenate edge tensors (N x 4) (if any)\n",
    "            # First two columns -> source and dest nodes\n",
    "            # Third column -> edge_type, Fourth column -> timestep distance\n",
    "            no_edges = (len(track_edges) == 0 and \n",
    "                        len(onset_edges) == 0 and len(next_edges) == 0)\n",
    "            if not no_edges:\n",
    "                edge_list = np.concatenate([x for x in edges\n",
    "                                              if x.size > 0])\n",
    "                edge_list = torch.from_numpy(edge_list)\n",
    "                \n",
    "            # Adapt tensor to torch_geometric's Data\n",
    "            # No edges: add fictitious self-edge\n",
    "            edge_index = (torch.LongTensor([[0], [0]]) if no_edges else\n",
    "                                   edge_list[:, :2].t().contiguous())\n",
    "            attrs = (torch.Tensor([[0, 0]]) if no_edges else\n",
    "                                           edge_list[:, 2:])\n",
    "\n",
    "            # One hot timestep distance concatenated to edge type\n",
    "            #edge_attrs = torch.zeros(attrs.size(0), 1+seq_acts.shape[-1])\n",
    "            #edge_attrs[:, 0] = attrs[:, 0]\n",
    "            #edge_attrs[np.arange(edge_attrs.size(0)), attrs.long()[:, 1]+1] = 1\n",
    "            edge_attrs = torch.Tensor(attrs.float())\n",
    "            \n",
    "            n = torch.sum(torch.Tensor(seq_acts[i]), dtype=torch.long) # sparse\n",
    "            node_features = self._get_node_features(seq_acts[i], n)\n",
    "            \n",
    "            graphs.append(Data(edge_index=edge_index, edge_attrs=edge_attrs,\n",
    "                               num_nodes=n, node_features=node_features))\n",
    "            \n",
    "            \n",
    "        # Merge the graphs corresponding to different bars into a single big graph\n",
    "        graphs, _, inc_dict = collate(\n",
    "            Data,\n",
    "            data_list=graphs,\n",
    "            increment=True,\n",
    "            add_batch=True\n",
    "        )\n",
    "        \n",
    "        # Change bars assignment vector name (in order not to be overwritten\n",
    "        # by Dataloader's collate)\n",
    "        graphs.bars = graphs.batch\n",
    "        \n",
    "        # Filter silences in order to have a sparse representation\n",
    "        new_seq_tensor = new_seq_tensor.reshape(-1, new_seq_tensor.shape[-2],\n",
    "                                                new_seq_tensor.shape[-1])\n",
    "        src_mask = src_mask.reshape(-1, src_mask.shape[-1])\n",
    "        new_seq_tensor = new_seq_tensor[seq_acts.reshape(-1).astype(bool)]\n",
    "        src_mask = src_mask[seq_acts.reshape(-1).astype(bool)]\n",
    "        \n",
    "        new_seq_tensor = torch.Tensor(new_seq_tensor)\n",
    "        seq_acts = torch.Tensor(seq_acts)\n",
    "        graphs.x_seq = new_seq_tensor\n",
    "        graphs.x_acts = seq_acts\n",
    "        graphs.src_mask = src_mask\n",
    "        \n",
    "        # Todo: start with torch at mount\n",
    "        #return torch.Tensor(new_seq_tensor), torch.Tensor(seq_acts), graphs, src_mask\n",
    "        return graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "from typing import Callable\n",
    "from torch_geometric.nn.inits import reset\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter as Param\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter\n",
    "from torch_sparse import SparseTensor, matmul, masked_select_nnz\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (Tensor, Tensor) -> Tensor\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.jit._overload\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    # type: (SparseTensor, Tensor) -> SparseTensor\n",
    "    pass\n",
    "\n",
    "\n",
    "def masked_edge_index(edge_index, edge_mask):\n",
    "    if isinstance(edge_index, Tensor):\n",
    "        return edge_index[:, edge_mask]\n",
    "    else:\n",
    "        return masked_select_nnz(edge_index, edge_mask, layout='coo')\n",
    "\n",
    "def masked_edge_attrs(edge_attrs, edge_mask):\n",
    "    return edge_attrs[edge_mask, :]\n",
    "\n",
    "\n",
    "class RGCNConv(MessagePassing):\n",
    "    r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "    Relational Data with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}_{\\textrm{root}} \\cdot\n",
    "        \\mathbf{x}_i + \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
    "        \\frac{1}{|\\mathcal{N}_r(i)|} \\mathbf{\\Theta}_r \\cdot \\mathbf{x}_j,\n",
    "\n",
    "    where :math:`\\mathcal{R}` denotes the set of relations, *i.e.* edge types.\n",
    "    Edge type needs to be a one-dimensional :obj:`torch.long` tensor which\n",
    "    stores a relation identifier\n",
    "    :math:`\\in \\{ 0, \\ldots, |\\mathcal{R}| - 1\\}` for each edge.\n",
    "\n",
    "    .. note::\n",
    "        This implementation is as memory-efficient as possible by iterating\n",
    "        over each individual relation type.\n",
    "        Therefore, it may result in low GPU utilization in case the graph has a\n",
    "        large number of relations.\n",
    "        As an alternative approach, :class:`FastRGCNConv` does not iterate over\n",
    "        each individual type, but may consume a large amount of memory to\n",
    "        compensate.\n",
    "        We advise to check out both implementations to see which one fits your\n",
    "        needs.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\n",
    "            corresponds to the sizes of source and target dimensionalities.\n",
    "            In case no input features are given, this argument should\n",
    "            correspond to the number of nodes in your graph.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_relations (int): Number of relations.\n",
    "        nn (torch.nn.Module): A neural network :math:`h_{\\mathbf{\\Theta}}` that\n",
    "            maps edge features :obj:`edge_attr` of shape :obj:`[-1,\n",
    "            num_edge_features]` to shape\n",
    "            :obj:`[-1, in_channels * out_channels]`, *e.g.*, defined by\n",
    "            :class:`torch.nn.Sequential`.\n",
    "        num_bases (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the basis-decomposition regularization scheme where\n",
    "            :obj:`num_bases` denotes the number of bases to use.\n",
    "            (default: :obj:`None`)\n",
    "        num_blocks (int, optional): If set to not :obj:`None`, this layer will\n",
    "            use the block-diagonal-decomposition regularization scheme where\n",
    "            :obj:`num_blocks` denotes the number of blocks to use.\n",
    "            (default: :obj:`None`)\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
    "            (default: :obj:`\"mean\"`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        num_relations: int,\n",
    "        num_dists: int,\n",
    "        #nn: Callable,\n",
    "        num_bases: Optional[int] = None,\n",
    "        num_blocks: Optional[int] = None,\n",
    "        aggr: str = 'mean',\n",
    "        root_weight: bool = True,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(aggr=aggr, node_dim=0, **kwargs)\n",
    "\n",
    "        if num_bases is not None and num_blocks is not None:\n",
    "            raise ValueError('Can not apply both basis-decomposition and '\n",
    "                             'block-diagonal-decomposition at the same time.')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        #self.nn = nn\n",
    "        self.num_dists = num_dists\n",
    "        self.num_relations = num_relations\n",
    "        self.num_bases = num_bases\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "        self.in_channels_l = in_channels[0]\n",
    "\n",
    "        if num_bases is not None:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_bases, in_channels[0], out_channels))\n",
    "            self.comp = Parameter(torch.Tensor(num_relations, num_bases))\n",
    "        \n",
    "        elif num_blocks is not None:\n",
    "            assert (in_channels[0] % num_blocks == 0\n",
    "                    and out_channels % num_blocks == 0)\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, num_blocks,\n",
    "                             in_channels[0] // num_blocks,\n",
    "                             out_channels // num_blocks))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        else:\n",
    "            self.weight = Parameter(\n",
    "                torch.Tensor(num_relations, in_channels[0], out_channels))\n",
    "            self.register_parameter('comp', None)\n",
    "\n",
    "        if root_weight:\n",
    "            self.root = Param(torch.Tensor(in_channels[1], out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Param(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.dist_weights = Parameter(torch.Tensor(self.num_dists))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        #reset(self.nn)\n",
    "        glorot(self.comp)\n",
    "        glorot(self.root)\n",
    "        zeros(self.dist_weights)\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[OptTensor, Tuple[OptTensor, Tensor]],\n",
    "                edge_index: Adj, edge_type: OptTensor = None,\n",
    "                edge_attr: OptTensor = None):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x: The input node features. Can be either a :obj:`[num_nodes,\n",
    "                in_channels]` node feature matrix, or an optional\n",
    "                one-dimensional node index tensor (in which case input features\n",
    "                are treated as trainable node embeddings).\n",
    "                Furthermore, :obj:`x` can be of type :obj:`tuple` denoting\n",
    "                source and destination node features.\n",
    "            edge_type: The one-dimensional relation type/index for each edge in\n",
    "                :obj:`edge_index`.\n",
    "                Should be only :obj:`None` in case :obj:`edge_index` is of type\n",
    "                :class:`torch_sparse.tensor.SparseTensor`.\n",
    "                (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert input features to a pair of node features or node indices.\n",
    "        x_l: OptTensor = None\n",
    "        if isinstance(x, tuple):\n",
    "            x_l = x[0]\n",
    "        else:\n",
    "            x_l = x\n",
    "        if x_l is None:\n",
    "            x_l = torch.arange(self.in_channels_l, device=self.weight.device)\n",
    "\n",
    "        x_r: Tensor = x_l\n",
    "        if isinstance(x, tuple):\n",
    "            x_r = x[1]\n",
    "\n",
    "        size = (x_l.size(0), x_r.size(0))\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_type = edge_index.storage.value()\n",
    "        assert edge_type is not None\n",
    "\n",
    "        # propagate_type: (x: Tensor)\n",
    "        out = torch.zeros(x_r.size(0), self.out_channels, device=x_r.device)\n",
    "\n",
    "        weight = self.weight\n",
    "        if self.num_bases is not None:  # Basis-decomposition =================\n",
    "            weight = (self.comp @ weight.view(self.num_bases, -1)).view(\n",
    "                self.num_relations, self.in_channels_l, self.out_channels)\n",
    "\n",
    "        if self.num_blocks is not None:  # Block-diagonal-decomposition =====\n",
    "\n",
    "            if x_l.dtype == torch.long and self.num_blocks is not None:\n",
    "                raise ValueError('Block-diagonal decomposition not supported '\n",
    "                                 'for non-continuous input features.')\n",
    "\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                h = self.propagate(tmp, x=x_l, size=size)\n",
    "                h = h.view(-1, weight.size(1), weight.size(2))\n",
    "                h = torch.einsum('abc,bcd->abd', h, weight[i])\n",
    "                out += h.contiguous().view(-1, self.out_channels)\n",
    "\n",
    "        else:  # No regularization/Basis-decomposition ========================\n",
    "            for i in range(self.num_relations):\n",
    "                tmp = masked_edge_index(edge_index, edge_type == i)\n",
    "                attr = masked_edge_attrs(edge_attr, edge_type == i)\n",
    "\n",
    "                if x_l.dtype == torch.long:\n",
    "                    out += self.propagate(tmp, x=weight[i, x_l], size=size)\n",
    "                else:\n",
    "                    h = self.propagate(tmp, x=x_l, size=size,\n",
    "                                       edge_attr=attr)\n",
    "                    out = out + (h @ weight[i])\n",
    "\n",
    "        root = self.root\n",
    "        if root is not None:\n",
    "            out += root[x_r] if x_r.dtype == torch.long else x_r @ root\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        weights = self.dist_weights[edge_attr.view(-1).long()]\n",
    "        weights = torch.diag(weights)\n",
    "        #weight = weight[..., :self.in_channels_l*self.in_channels_l]\n",
    "        #weight = weight.view(-1, self.in_channels_l, self.in_channels_l)\n",
    "        #return torch.matmul(x_j.unsqueeze(1), weight).squeeze(1)\n",
    "        return torch.matmul(weights, x_j)\n",
    "        \n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        adj_t = adj_t.set_value(None, layout=None)\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, num_relations={self.num_relations})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hSwcnlq4g50O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch_geometric.nn.conv import GCNConv#, RGCNConv\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "\n",
    "# Todo: check and think about max_len\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 256):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *                     \\\n",
    "                             (-math.log(10000.0)/d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position*div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=256, hidden_dim=256, output_dim=256, n_layers=2,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        \n",
    "        for i in range(n_layers-2):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.p = dropout\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "            x = self.layers[i](x)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        x = F.dropout(x, p=self.p, training=self.training)\n",
    "        \n",
    "        return self.layers[-1](x)\n",
    "\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=256, hidden_dim=256, n_layers=3, \n",
    "                 num_relations=3, num_dists=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(RGCNConv(input_dim, hidden_dim,\n",
    "                                    num_relations, num_dists))\n",
    "        \n",
    "        for i in range(n_layers-1):\n",
    "            self.layers.append(RGCNConv(hidden_dim, hidden_dim,\n",
    "                                        num_relations, num_dists))\n",
    "            \n",
    "        self.p = dropout\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attrs = data.x, data.edge_index, data.edge_attrs\n",
    "        edge_type = edge_attrs[:, 0]\n",
    "        edge_attr = edge_attrs[:, 1:]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.dropout(x, p=self.p, training=self.training)\n",
    "            x = layer(x, edge_index, edge_type, edge_attr)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "        self.acts_encoder = MLP(input_dim=self.n_tracks*self.resolution*4,\n",
    "                                hidden_dim=self.d_model,\n",
    "                                output_dim=self.d_model,\n",
    "                                n_layers=self.actsnn_n_layers,\n",
    "                                dropout=self.dropout)\n",
    "\n",
    "        # Todo: one separate encoder for drums\n",
    "        # Transformer Encoder\n",
    "        self.embedding = nn.Linear(self.d_token, self.d_model)\n",
    "        self.pos_encoder = PositionalEncoding(self.d_model, dropout=self.dropout)\n",
    "        transf_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.n_head_transf,\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            transf_layer,\n",
    "            num_layers=self.n_layers_transf\n",
    "        )\n",
    "\n",
    "        # Graph encoder\n",
    "        self.graph_encoder = GraphEncoder(dropout=self.dropout, \n",
    "                                          input_dim=self.gnn_input_dim,\n",
    "                                          hidden_dim=self.d_model,\n",
    "                                          n_layers=self.gnn_n_layers)\n",
    "        \n",
    "        self.linear_merge = nn.Linear(2*self.d_model, self.d_model)\n",
    "\n",
    "        # (LSTM)\n",
    "        self.bars_encoder = nn.Linear(self.d_model*self.n_bars, \n",
    "                                      self.d_model)\n",
    "        \n",
    "        # Linear layers that compute the final mu and log_var\n",
    "        # Todo: as parameters\n",
    "        self.linear_mu = nn.Linear(self.d_model, self.d_latent)\n",
    "        self.linear_log_var = nn.Linear(self.d_model, self.d_latent)\n",
    "\n",
    "        \n",
    "    def forward(self, x_seq, x_acts, x_graph, src_mask):\n",
    "\n",
    "        # Collapse track (and optionally batch and bar) dimension.\n",
    "        # From: (d1 x ... x d_simu_notes x d_token)\n",
    "        # To:   ((d1 * ... * d_timestep) x d_simu_notes x d_token)\n",
    "        #print(\"Init input:\", x_seq.size())\n",
    "        #x_seq = x_seq.view(-1, x_seq.size(-2), x_seq.size(-1))\n",
    "        #print(\"Reshaped x_seq:\", x_seq.size())\n",
    "        \n",
    "        # Filter silences\n",
    "        #x_acts = x_acts.view(-1)\n",
    "        #x_seq = x_seq[x_acts.bool()]\n",
    "        #src_mask = src_mask[x_acts.bool()]\n",
    "        #print(\"Filtered x_seq:\", x_seq.size())\n",
    "        #print(\"Filtered src_mask:\", x_seq.size())\n",
    "        \n",
    "        acts_encs = self.acts_encoder(x_acts.view(-1,\n",
    "                                                  self.n_tracks*self.resolution*4))\n",
    "        print(acts_encs.size())\n",
    "        # batch * bars x 256\n",
    "\n",
    "        # Compute embeddings\n",
    "        out = self.embedding(x_seq)\n",
    "        #print(\"Embs:\", embs.size())\n",
    "\n",
    "        # Permute dimensions to batch_first = False\n",
    "        out = out.permute(1, 0, 2)\n",
    "        #print(\"Seq len first input:\", embs.size())\n",
    "\n",
    "        out = self.pos_encoder(out)\n",
    "        #print(\"Pos encodings:\", pos_encs.size())\n",
    "        \n",
    "        # Transformer forward pass\n",
    "        out = self.transformer_encoder(out, \n",
    "                                       src_key_padding_mask=src_mask)\n",
    "        #print(\"Transf encodings:\", transformer_encs.size())\n",
    "\n",
    "        out = torch.mean(out, 0)\n",
    "        # After mean, pooled_encs: (N x d_model)\n",
    "        #print(\"Pooled encodings:\", pooled_encs.size())\n",
    "\n",
    "        # Concatenate track one hot features with chord encodings\n",
    "        # and compute final node encodings\n",
    "        x_graph.x = torch.cat((x_graph.node_features, out), 1)\n",
    "        out = self.graph_encoder(x_graph)\n",
    "        #print(\"Node encodings:\", node_encs.size())\n",
    "        \n",
    "        \n",
    "        # Take avg of node_encs in each bar.\n",
    "        # Take avgs, concat them and pass them through a linear layer that\n",
    "        # outputs a [bs, d_model] tensor\n",
    "        distinct_bars = x_graph.bars + self.n_bars*x_graph.batch\n",
    "        out = scatter_mean(out, distinct_bars, dim=0)\n",
    "        #print(\"bar_latents:\", bar_latents.size())\n",
    "        \n",
    "        out = torch.cat((out, acts_encs), dim=1)\n",
    "        out = self.linear_merge(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = out.view(out.size(0)//self.n_bars, \n",
    "                                   out.size(1)*self.n_bars)\n",
    "        #print(\"cat_bar_latents:\", cat_bar_latents.size())\n",
    "        out = self.bars_encoder(out)\n",
    "        out = F.relu(out)\n",
    "        #print(\"Bars encoding:\", bars_encoding.size())\n",
    "        \n",
    "        \n",
    "        # DROPOUT?\n",
    "\n",
    "        # Compute mu and log(std^2)\n",
    "        mu = self.linear_mu(out)\n",
    "        log_var = self.linear_log_var(out)\n",
    "        #print(\"Mu:\", mu.size())\n",
    "        #print(\"Log var:\", log_var.size())\n",
    "        \n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        # (LSTM)\n",
    "        self.z_decompressor = nn.Linear(self.d_latent,\n",
    "                                        self.d_model*self.n_bars)\n",
    "\n",
    "        # Boolean activations decoder (MLP)\n",
    "        self.acts_decoder = MLP(input_dim=self.d_model,\n",
    "                                hidden_dim=self.d_model,\n",
    "                                output_dim=self.n_tracks*self.resolution*4,\n",
    "                                n_layers=self.actsnn_n_layers,\n",
    "                                dropout=self.dropout)\n",
    "\n",
    "        # GNN\n",
    "        self.graph_decoder = GraphEncoder(dropout=self.dropout,\n",
    "                                          input_dim=self.gnn_input_dim,\n",
    "                                          hidden_dim=self.d_model,\n",
    "                                          n_layers=self.gnn_n_layers)\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        self.embedding = nn.Linear(self.d_token, self.d_model)\n",
    "        self.pos_encoder = PositionalEncoding(self.d_model, dropout=self.dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.n_head_transf,\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "        self.transf_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=self.n_layers_transf\n",
    "        )\n",
    "        \n",
    "        # Last linear layer\n",
    "        self.lin = nn.Linear(self.d_model, self.d_token)\n",
    "\n",
    "\n",
    "    def forward(self, z, x_seq, x_acts, x_graph, src_mask, tgt_mask,\n",
    "                inference=False):\n",
    "        \n",
    "        #print(\"z:\", z.size())\n",
    "        \n",
    "        # Obtain z_bar from z\n",
    "        out = self.z_decompressor(z)\n",
    "        out = F.relu(out)\n",
    "        # RELU? DROPOUT?\n",
    "        out = out.view(-1, self.n_bars, \n",
    "                         self.d_model)\n",
    "        #print(\"z_bar:\", z_bar.size())\n",
    "\n",
    "        # Compute activations for each z_bar\n",
    "        acts_out = self.acts_decoder(out)\n",
    "        acts_out = acts_out.view(x_acts.size())\n",
    "        #print(\"Acts out:\", acts_out.size())\n",
    "\n",
    "        # Initialize node features with corresponding z_bar\n",
    "        # and propagate with GNN\n",
    "        distinct_bars = x_graph.bars + self.n_bars*x_graph.batch\n",
    "        _, counts = torch.unique(distinct_bars, return_counts=True)\n",
    "        out = out.view(-1, self.d_model)\n",
    "        out = torch.repeat_interleave(out, counts, axis=0)\n",
    "        #print(\"Node features:\", z_node_features.size())\n",
    "        \n",
    "        # Add one-hot encoding of tracks\n",
    "        # Todo: use also edge info\n",
    "        x_graph.x = torch.cat((x_graph.node_features, out), 1)\n",
    "        out = self.graph_decoder(x_graph)\n",
    "        #print(\"Node decodings:\", node_decs.size())\n",
    "        \n",
    "        # Prepare transformer memory\n",
    "        out = out.repeat(16, 1, 1)\n",
    "        #print(\"Tiled node decodings:\", node_decs.size())\n",
    "        \n",
    "        # Filter silences\n",
    "        #x_seq = x_seq.view(-1, x_seq.size(-2), x_seq.size(-1))\n",
    "        #x_acts = x_acts.view(-1)\n",
    "        #x_seq = x_seq[x_acts.bool()]\n",
    "        #src_mask = src_mask[x_acts.bool()]\n",
    "        #print(\"Filtered src_mask:\", src_mask.size())\n",
    "        #print(\"Filtered x_seq:\", x_seq.size())\n",
    "        \n",
    "        # Todo: same embeddings as encoder?\n",
    "        tgt = self.embedding(x_seq)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        out = self.transf_decoder(tgt, out,\n",
    "                                  tgt_key_padding_mask=src_mask,\n",
    "                                  tgt_mask=tgt_mask)\n",
    "        #print(\"Seq out:\", seq_out.size())\n",
    "        \n",
    "        out = self.lin(out)\n",
    "        #print(\"Seq out after lin:\", seq_out.size())\n",
    "        \n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = out.view(x_seq.size())\n",
    "        #print(\"(Final) seq out after transpose\", seq_out.size())\n",
    "\n",
    "        return out, acts_out\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(**kwargs)\n",
    "        self.decoder = Decoder(**kwargs)\n",
    "    \n",
    "    \n",
    "    def forward(self, x_seq, x_acts, x_graph, src_mask, tgt_mask,\n",
    "                inference=False):\n",
    "        \n",
    "        #src_mask = src_mask.view(-1, src_mask.size(-1))\n",
    "        \n",
    "        # Encoder pass\n",
    "        mu, log_var = self.encoder(x_seq, x_acts, x_graph, src_mask)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = torch.exp(0.5*log_var)\n",
    "        z = z * torch.randn_like(z)\n",
    "        #print(\"eps:\", eps.size())\n",
    "        z = z + mu\n",
    "        \n",
    "        # Shifting target sequence and mask for transformer decoder\n",
    "        tgt = x_seq[..., :-1, :]\n",
    "        src_mask = src_mask[:, :-1]\n",
    "        \n",
    "        # Decoder pass\n",
    "        out = self.decoder(z, tgt, x_acts, x_graph, src_mask, tgt_mask,\n",
    "                           inference=inference)\n",
    "        \n",
    "        return out, mu, log_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import copy\n",
    "import time\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "\n",
    "def append_dict(dest_d, source_d):\n",
    "        \n",
    "    for k, v in source_d.items():\n",
    "        dest_d[k].append(v)\n",
    "\n",
    "\n",
    "class VAETrainer():\n",
    "    \n",
    "    def __init__(self, model_dir, checkpoint=False, model=None, optimizer=None,\n",
    "                 init_lr=1e-4, lr_scheduler=None, device=torch.device(\"cuda\"), \n",
    "                 print_every=1, save_every=1, eval_every=100, iters_to_accumulate=1):\n",
    "        \n",
    "        self.model_dir = model_dir\n",
    "        self.device = device\n",
    "        self.print_every = print_every\n",
    "        self.save_every = save_every\n",
    "        self.eval_every = eval_every\n",
    "        self.iters_to_accumulate = iters_to_accumulate\n",
    "        \n",
    "        # Criteria with ignored padding\n",
    "        self.bce_unreduced = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.ce_p = nn.CrossEntropyLoss(ignore_index=130)\n",
    "        self.ce_d = nn.CrossEntropyLoss(ignore_index=98)\n",
    "        \n",
    "        # Training stats\n",
    "        self.tr_losses = defaultdict(list)\n",
    "        self.tr_accuracies = defaultdict(list)\n",
    "        self.val_losses = defaultdict(list)\n",
    "        self.val_accuracies = defaultdict(list)\n",
    "        self.lrs = []\n",
    "        self.times = []\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        \n",
    "        self.tot_batches = 0\n",
    "        self.beta = 0\n",
    "        self.min_val_loss = np.inf\n",
    "        \n",
    "        if checkpoint:\n",
    "            self.load_checkpoint()\n",
    "        \n",
    "    \n",
    "    def train(self, trainloader, validloader=None, epochs=1,\n",
    "              early_exit=None):\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        print(\"Starting training.\\n\")\n",
    "        \n",
    "        if not self.times:\n",
    "            start = time.time()\n",
    "            self.times.append(start)\n",
    "        \n",
    "        progress_bar = tqdm(range(len(trainloader)))\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "                \n",
    "        # Zero out the gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            self.cur_epoch = epoch\n",
    "            \n",
    "            for batch_idx, inputs in enumerate(trainloader):\n",
    "                \n",
    "                self.cur_batch_idx = batch_idx\n",
    "                \n",
    "                # Get the inputs\n",
    "                x_graph = inputs.to(self.device)\n",
    "                x_seq, x_acts, src_mask = x_graph.x_seq, x_graph.x_acts, x_graph.src_mask\n",
    "                tgt_mask = generate_square_subsequent_mask(x_seq.size(-2)-1).to(self.device)\n",
    "                \n",
    "                inputs = (x_seq, x_acts, x_graph)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # Forward pass, get the reconstructions\n",
    "                    outputs, mu, log_var = self.model(x_seq, x_acts, x_graph, src_mask, tgt_mask)\n",
    "\n",
    "                    # Compute the backprop loss and other required losses\n",
    "                    tot_loss, losses = self._compute_losses(inputs, outputs, mu,\n",
    "                                                             log_var)\n",
    "                    tot_loss = tot_loss / self.iters_to_accumulate\n",
    "                \n",
    "                # Free GPU\n",
    "                del x_seq\n",
    "                del x_acts\n",
    "                del src_mask\n",
    "                del tgt_mask\n",
    "                \n",
    "                # Backprop\n",
    "                scaler.scale(tot_loss).backward()\n",
    "                #tot_loss.backward()\n",
    "                #self.optimizer.step()\n",
    "                    \n",
    "                if (self.tot_batches + 1) % self.iters_to_accumulate == 0:\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    if self.lr_scheduler is not None:\n",
    "                        self.lr_scheduler.step()\n",
    "                \n",
    "                # Compute accuracies\n",
    "                accs = self._compute_accuracies(inputs, outputs)\n",
    "                \n",
    "                # Update the stats\n",
    "                append_dict(self.tr_losses, losses)\n",
    "                last_lr = (self.lr_scheduler.lr \n",
    "                               if self.lr_scheduler is not None else self.init_lr)\n",
    "                self.lrs.append(last_lr)\n",
    "                append_dict(self.tr_accuracies, accs)\n",
    "                now = time.time()\n",
    "                self.times.append(now)\n",
    "                \n",
    "                # Print stats\n",
    "                if (self.tot_batches + 1) % self.print_every == 0:\n",
    "                    print(\"Training on batch {}/{} of epoch {}/{} complete.\"\n",
    "                          .format(batch_idx+1, len(trainloader), epoch+1, epochs))\n",
    "                    self._print_stats()\n",
    "                    #print(\"Tot_loss: {:.4f} acts_loss: {:.4f} \"\n",
    "                          #.format(running_loss/self.print_every, acts_loss), end='')\n",
    "                    #print(\"pitches_loss: {:.4f} dur_loss: {:.4f} kld_loss: {:.4f}\"\n",
    "                          #.format(pitches_loss, dur_loss, kld_loss))\n",
    "                    print(\"\\n----------------------------------------\\n\")\n",
    "                    \n",
    "                # ------------------------------------\n",
    "                # EVAL ON VL SET EVERY N GRADIENT UPDATES\n",
    "                # ------------------------------------\n",
    "                \n",
    "                if validloader is not None and (self.tot_batches + 1) % self.eval_every == 0:\n",
    "                    \n",
    "                    # Evaluate on val set\n",
    "                    print(\"\\nEvaluating on validation set...\\n\")\n",
    "                    val_losses, val_accuracies = self.evaluate(validloader)\n",
    "                    \n",
    "                    # Update stats\n",
    "                    append_dict(self.val_losses, val_losses)\n",
    "                    append_dict(self.val_accuracies, val_accuracies)\n",
    "                    \n",
    "                    print(\"Val losses:\")\n",
    "                    print(val_losses)\n",
    "                    print(\"Val accuracies:\")\n",
    "                    print(val_accuracies)\n",
    "                    \n",
    "                    # Save model if val loss (tot) reached a new minimum\n",
    "                    tot_loss = val_losses['tot']\n",
    "                    if tot_loss < self.min_val_loss:\n",
    "                        print(\"\\nValidation loss improved.\")\n",
    "                        print(\"Saving new best model to disk...\\n\")\n",
    "                        self._save_model('best_model')\n",
    "                        self.min_val_loss = tot_loss\n",
    "                    \n",
    "                    self.model.train()\n",
    "                \n",
    "                progress_bar.update(1)     \n",
    "                    \n",
    "                # When appropriate, save model and stats on disk\n",
    "                if self.save_every > 0 and (self.tot_batches + 1) % self.save_every == 0:\n",
    "                    print(\"\\nSaving model to disk...\\n\")\n",
    "                    self._save_model('checkpoint')\n",
    "                \n",
    "                # Stop prematurely if early_exit is set and reached\n",
    "                if early_exit is not None and (self.tot_batches + 1) > early_exit:\n",
    "                    break\n",
    "                \n",
    "                self.tot_batches += 1\n",
    "            \n",
    "\n",
    "        end = time.time()\n",
    "        # Todo: self.__print_time()\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"Training completed in (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "                  .format(int(hours),int(minutes),seconds))\n",
    "        \n",
    "        print(\"Saving model to disk...\")\n",
    "        self._save_model('checkpoint')\n",
    "        \n",
    "        print(\"Model saved.\")\n",
    "        \n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        \n",
    "        losses = defaultdict(list)\n",
    "        accs = defaultdict(list)\n",
    "        \n",
    "        self.model.eval()\n",
    "        progress_bar = tqdm(range(len(loader)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, inputs in enumerate(loader):\n",
    "\n",
    "                # Get the inputs and move them to device\n",
    "                x_graph = inputs.to(self.device)\n",
    "                x_seq, x_acts, src_mask = x_graph.x_seq, x_graph.x_acts, x_graph.src_mask\n",
    "                #x_seq, x_acts, x_graph, src_mask = inputs\n",
    "                #x_seq = x_seq.float().to(self.device)\n",
    "                #x_acts = x_acts.to(self.device)\n",
    "                #x_graph = x_graph.to(self.device)\n",
    "                #src_mask = src_mask.to(self.device)\n",
    "                tgt_mask = generate_square_subsequent_mask(x_seq.size(-2)-1).to(self.device)\n",
    "                inputs = (x_seq, x_acts, x_graph)\n",
    "\n",
    "                # Forward pass, get the reconstructions\n",
    "                outputs, mu, log_var = self.model(x_seq, x_acts, x_graph, src_mask, tgt_mask)\n",
    "\n",
    "                # Compute losses and accuracies wrt batch\n",
    "                _, losses_b = self._compute_losses(inputs, outputs, mu,\n",
    "                                                         log_var)\n",
    "                accs_b = self._compute_accuracies(inputs, outputs)\n",
    "                \n",
    "                # Save losses and accuracies\n",
    "                append_dict(losses, losses_b)\n",
    "                append_dict(accs, accs_b)\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "        \n",
    "        \n",
    "        # Compute avg losses and accuracies\n",
    "        avg_losses = {}\n",
    "        for k, l in losses.items():\n",
    "            avg_losses[k] = mean(l)\n",
    "            \n",
    "        avg_accs = {}\n",
    "        for k, l in accs.items():\n",
    "            avg_accs[k] = mean(l)\n",
    "            \n",
    "        return avg_losses, avg_accs\n",
    "                \n",
    "        \n",
    "    \n",
    "    def _compute_losses(self, inputs, outputs, mu, log_var):\n",
    "        \n",
    "        x_seq, x_acts, _ = inputs\n",
    "        seq_rec, acts_rec = outputs\n",
    "        \n",
    "        # Shift outputs for transformer decoder loss and filter silences\n",
    "        x_seq = x_seq[..., 1:, :]\n",
    "        #x_seq = x_seq[x_acts.bool()]\n",
    "        #print(x_seq.size())\n",
    "        #print(seq_rec.size())\n",
    "                \n",
    "        # Compute the losses\n",
    "        acts_loss = self.bce_unreduced(acts_rec.view(-1), x_acts.view(-1).float())\n",
    "        #weights = torch.zeros(acts_loss.size()).to(device)\n",
    "        #weights[x_acts.view(-1) == 1] = 0.9\n",
    "        #weights[x_acts.view(-1) == 0] = 0.1\n",
    "        #acts_loss = torch.mean(weights*acts_loss)\n",
    "        acts_loss = torch.mean(acts_loss)\n",
    "        \n",
    "        pitches_loss = self.ce_p(seq_rec.reshape(-1, seq_rec.size(-1))[:, :131],\n",
    "                          x_seq.reshape(-1, x_seq.size(-1))[:, :131].argmax(dim=1))\n",
    "        dur_loss = self.ce_d(seq_rec.reshape(-1, seq_rec.size(-1))[:, 131:],\n",
    "                          x_seq.reshape(-1, x_seq.size(-1))[:, 131:].argmax(dim=1))\n",
    "        kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        rec_loss = pitches_loss + dur_loss + acts_loss\n",
    "        #rec_loss = acts_loss\n",
    "        tot_loss = rec_loss + self.beta*kld_loss\n",
    "        \n",
    "        losses = {\n",
    "            'tot': tot_loss.item(),\n",
    "            'pitches': pitches_loss.item(),\n",
    "            'dur': dur_loss.item(),\n",
    "            'acts': acts_loss.item(),\n",
    "            'rec': rec_loss.item(),\n",
    "            'kld': kld_loss.item(),\n",
    "            'beta*kld': self.beta*kld_loss.item()\n",
    "        }\n",
    "        \n",
    "        return tot_loss, losses\n",
    "            \n",
    "            \n",
    "    def _compute_accuracies(self, inputs, outputs):\n",
    "        \n",
    "        x_seq, x_acts, _ = inputs\n",
    "        seq_rec, acts_rec = outputs\n",
    "        \n",
    "        # Shift outputs and filter silences\n",
    "        x_seq = x_seq[..., 1:, :]\n",
    "        #x_seq = x_seq[x_acts.bool()]\n",
    "        #print(x_seq.size())\n",
    "        #print(seq_rec.size())\n",
    "        \n",
    "        notes_acc = self._note_accuracy(seq_rec, x_seq)\n",
    "        pitches_acc = self._pitches_accuracy(seq_rec, x_seq)\n",
    "        dur_acc = self._dur_accuracy(seq_rec, x_seq)\n",
    "        acts_acc = self._acts_accuracy(acts_rec, x_acts)\n",
    "        acts_precision = self._acts_precision(acts_rec, x_acts)\n",
    "        acts_recall = self._acts_recall(acts_rec, x_acts)\n",
    "        acts_f1 = 2*acts_recall*acts_precision / (acts_recall+acts_precision)\n",
    "        \n",
    "        accs = {\n",
    "            'notes': notes_acc.item(),\n",
    "            'pitches': pitches_acc.item(),\n",
    "            'dur': dur_acc.item(),\n",
    "            'acts_acc': acts_acc.item(),\n",
    "            'acts_precision': acts_precision.item(),\n",
    "            'acts_recall': acts_recall.item(),\n",
    "            'acts_f1': acts_f1.item()\n",
    "        }\n",
    "        \n",
    "        return accs\n",
    "    \n",
    "    \n",
    "    def _note_accuracy(self, seq_rec, x_seq):\n",
    "        \n",
    "        pitches_rec = F.softmax(seq_rec[..., :131], dim=-1)\n",
    "        pitches_rec = torch.argmax(pitches_rec, dim=-1)\n",
    "        pitches_true = torch.argmax(x_seq[..., :131], dim=-1)\n",
    "        \n",
    "        #print(torch.all(pitches_rec == 129))\n",
    "        #print(pitches_rec)\n",
    "        \n",
    "        mask_p = (pitches_true != 130)\n",
    "        #mask = torch.logical_and(pitches_true != 128,\n",
    "         #                        pitches_true != 129)\n",
    "        #mask = torch.logical_and(mask,\n",
    "         #                        pitches_true != 130)\n",
    "        \n",
    "        preds_pitches = (pitches_rec == pitches_true)\n",
    "        preds_pitches = torch.logical_and(preds_pitches, mask_p)\n",
    "        \n",
    "        \n",
    "        dur_rec = F.softmax(seq_rec[..., 131:], dim=-1)\n",
    "        dur_rec = torch.argmax(dur_rec, dim=-1)\n",
    "        dur_true = torch.argmax(x_seq[..., 131:], dim=-1)\n",
    "        \n",
    "        #print(torch.all(dur_rec == 97))\n",
    "        \n",
    "        mask_d = (dur_true != 98)\n",
    "        #mask = torch.logical_and(pitches_true != 128,\n",
    "         #                        pitches_true != 129)\n",
    "        #mask = torch.logical_and(mask,\n",
    "         #                        pitches_true != 130)\n",
    "        \n",
    "        preds_dur = (dur_rec == dur_true)\n",
    "        preds_dur = torch.logical_and(preds_dur, mask_d)\n",
    "        \n",
    "        return torch.sum(torch.logical_and(preds_pitches, \n",
    "                                           preds_dur)) / torch.sum(mask_p)\n",
    "    \n",
    "    \n",
    "    def _acts_precision(self, acts_rec, x_acts):\n",
    "        \n",
    "        acts_rec = torch.sigmoid(acts_rec)\n",
    "        acts_rec[acts_rec < 0.5] = 0\n",
    "        acts_rec[acts_rec >= 0.5] = 1\n",
    "        \n",
    "        tp = torch.sum(x_acts[acts_rec == 1])\n",
    "        \n",
    "        return tp / torch.sum(acts_rec)\n",
    "    \n",
    "    \n",
    "    def _acts_recall(self, acts_rec, x_acts):\n",
    "        \n",
    "        acts_rec = torch.sigmoid(acts_rec)\n",
    "        acts_rec[acts_rec < 0.5] = 0\n",
    "        acts_rec[acts_rec >= 0.5] = 1\n",
    "        \n",
    "        tp = torch.sum(x_acts[acts_rec == 1])\n",
    "        \n",
    "        return tp / torch.sum(x_acts)\n",
    "    \n",
    "    \n",
    "    def _acts_accuracy(self, acts_rec, x_acts):\n",
    "        \n",
    "        acts_rec = torch.sigmoid(acts_rec)\n",
    "        acts_rec[acts_rec < 0.5] = 0\n",
    "        acts_rec[acts_rec >= 0.5] = 1\n",
    "        \n",
    "        #print(\"All zero acts?\", torch.all(acts_rec == 0))\n",
    "        #print(\"All one acts?\", torch.all(acts_rec == 0))\n",
    "        \n",
    "        return torch.sum(acts_rec == x_acts) / x_acts.numel()\n",
    "    \n",
    "    \n",
    "    def _pitches_accuracy(self, seq_rec, x_seq):\n",
    "        \n",
    "        pitches_rec = F.softmax(seq_rec[..., :131], dim=-1)\n",
    "        pitches_rec = torch.argmax(pitches_rec, dim=-1)\n",
    "        pitches_true = torch.argmax(x_seq[..., :131], dim=-1)\n",
    "        \n",
    "        #print(\"All EOS pitches?\", torch.all(pitches_rec == 129))\n",
    "        \n",
    "        mask = (pitches_true != 130)\n",
    "        #mask = torch.logical_and(pitches_true != 128,\n",
    "         #                        pitches_true != 129)\n",
    "        #mask = torch.logical_and(mask,\n",
    "         #                        pitches_true != 130)\n",
    "        \n",
    "        preds_pitches = (pitches_rec == pitches_true)\n",
    "        preds_pitches = torch.logical_and(preds_pitches, mask)\n",
    "        \n",
    "        return torch.sum(preds_pitches) / torch.sum(mask)\n",
    "    \n",
    "    \n",
    "    def _dur_accuracy(self, seq_rec, x_seq):\n",
    "        \n",
    "        dur_rec = F.softmax(seq_rec[..., 131:], dim=-1)\n",
    "        dur_rec = torch.argmax(dur_rec, dim=-1)\n",
    "        dur_true = torch.argmax(x_seq[..., 131:], dim=-1)\n",
    "        \n",
    "        #print(\"All EOS durs?\", torch.all(dur_rec == 97))\n",
    "        \n",
    "        mask = (dur_true != 98)\n",
    "        #mask = torch.logical_and(pitches_true != 128,\n",
    "         #                        pitches_true != 129)\n",
    "        #mask = torch.logical_and(mask,\n",
    "         #                        pitches_true != 130)\n",
    "        \n",
    "        preds_dur = (dur_rec == dur_true)\n",
    "        preds_dur = torch.logical_and(preds_dur, mask)\n",
    "        \n",
    "        return torch.sum(preds_dur) / torch.sum(mask)\n",
    "    \n",
    "    \n",
    "    def _save_model(self, filename):\n",
    "        path = os.path.join(self.model_dir, filename)\n",
    "        torch.save({\n",
    "            'epoch': self.cur_epoch,\n",
    "            'batch': self.cur_batch_idx,\n",
    "            'tot_batches': self.tot_batches,\n",
    "            'beta': self.beta,\n",
    "            'min_val_loss': self.min_val_loss,\n",
    "            'print_every': self.print_every,\n",
    "            'save_every': self.save_every,\n",
    "            'eval_every': self.eval_every,\n",
    "            'lrs': self.lrs,\n",
    "            'tr_losses': self.tr_losses,\n",
    "            'tr_accuracies': self.tr_accuracies,\n",
    "            'val_losses': self.val_losses,\n",
    "            'val_accuracies': self.val_accuracies,\n",
    "            #'scheduler_state_dict': self.lr_scheduler.state_dict(), # Todo: fix\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }, path)\n",
    "        \n",
    "    \n",
    "    def load(self):\n",
    "        \n",
    "        checkpoint = torch.load(os.path.join(self.model_dir, 'checkpoint'))\n",
    "        \n",
    "        self.cur_epoch = checkpoint['epoch']\n",
    "        self.cur_batch_idx = checkpoint['batch']\n",
    "        self.save_every = checkpoint['save_every']\n",
    "        self.eval_every = checkpoint['eval_every']\n",
    "        self.lrs = checkpoint['lrs']\n",
    "        self.tr_losses = checkpoint['tr_losses']\n",
    "        self.tr_accuracies = checkpoint['tr_accuracies']\n",
    "        self.val_losses = checkpoint['val_losses']\n",
    "        self.val_accuracies = checkpoint['val_accuracies']\n",
    "        self.times = checkpoint['times']\n",
    "        self.min_val_loss = checkpoint['min_val_loss']\n",
    "        self.beta = checkpoint['beta']\n",
    "        self.tot_batches = checkpoint['tot_batches']\n",
    "        \n",
    "        \n",
    "    def _print_stats(self):\n",
    "        \n",
    "        hours, rem = divmod(self.times[-1]-self.times[0], 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"Elapsed time from start (h:m:s): {:0>2}:{:0>2}:{:05.2f}\"\n",
    "                  .format(int(hours), int(minutes), seconds))\n",
    "        \n",
    "        avg_lr = mean(self.lrs[-self.print_every:])\n",
    "        \n",
    "        # Take mean of the last non-printed batches for each stat\n",
    "        \n",
    "        avg_losses = {}\n",
    "        for k, l in self.tr_losses.items():\n",
    "            avg_losses[k] = mean(l[-self.print_every:])\n",
    "        \n",
    "        avg_accs = {}\n",
    "        for k, l in self.tr_accuracies.items():\n",
    "            avg_accs[k] = mean(l[-self.print_every:])\n",
    "        \n",
    "        print(\"Losses:\")\n",
    "        print(avg_losses)\n",
    "        print(\"Accuracies:\")\n",
    "        print(avg_accs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset len: 6813946\n",
      "TR set len: 4769762\n",
      "VL set len: 1362789\n",
      "TS set len: 681395\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "bs = 64\n",
    "nw = 4\n",
    "n_bars = 2\n",
    "\n",
    "ds_dir = \"/data/cosenza/datasets/preprocessed_2bars/\"\n",
    "#ds_dir = 'data/preprocessed'\n",
    "\n",
    "dataset = MIDIDataset(ds_dir, n_bars=n_bars)\n",
    "\n",
    "print('Dataset len:', len(dataset))\n",
    "\n",
    "train_len = int(0.7 * len(dataset)) \n",
    "valid_len = int(0.2 * len(dataset))\n",
    "test_len = len(dataset) - train_len - valid_len\n",
    "#train_dataset, valid_dataset, test_dataset = random_split(model_dataset, (train_count, valid_count, test_count))\n",
    "tr_set, vl_set, ts_set = random_split(dataset, (train_len, valid_len, test_len))\n",
    "\n",
    "trainloader = DataLoader(tr_set, batch_size=bs, shuffle=True, num_workers=nw)\n",
    "validloader = DataLoader(vl_set, batch_size=bs, shuffle=False, num_workers=nw)\n",
    "#testloader = DataLoader(ts_set, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "print('TR set len:', len(tr_set))\n",
    "print('VL set len:', len(vl_set))\n",
    "print('TS set len:', len(ts_set))\n",
    "\n",
    "#n_samples = 128\n",
    "#subset = Subset(dataset, np.arange(n_samples))\n",
    "#loader = DataLoader(subset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 369], edge_attrs=[369, 2], num_nodes=82, node_features=[82, 4], batch=[82], ptr=[3], bars=[82], x_seq=[82, 16, 230], x_acts=[2, 4, 32], src_mask=[82, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 9463], edge_attrs=[9463, 2], num_nodes=2755, node_features=[2755, 4], batch=[2755], bars=[2755], x_seq=[2755, 16, 230], x_acts=[128, 4, 32], src_mask=[2755, 16], ptr=[65])\n"
     ]
    }
   ],
   "source": [
    "for i, g in enumerate(trainloader):\n",
    "    print(g)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current device idx: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Current device idx:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm models/vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "def print_params(model):\n",
    "    \n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, parameter in model.named_parameters():\n",
    "        \n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "            \n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params += param\n",
    "        \n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from typing import Optional\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from lr_scheduler.lr_scheduler import LearningRateScheduler\n",
    "\n",
    "class TransformerLRScheduler(LearningRateScheduler):\n",
    "    r\"\"\"\n",
    "    Transformer Learning Rate Scheduler proposed in \"Attention Is All You Need\"\n",
    "    Args:\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        init_lr (float): Initial learning rate.\n",
    "        peak_lr (float): Maximum learning rate.\n",
    "        final_lr (float): Final learning rate.\n",
    "        final_lr_scale (float): Final learning rate scale\n",
    "        warmup_steps (int): Warmup the learning rate linearly for the first N updates\n",
    "        decay_steps (int): Steps in decay stages\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer: Optimizer,\n",
    "            init_lr: float,\n",
    "            peak_lr: float,\n",
    "            final_lr: float,\n",
    "            final_lr_scale: float,\n",
    "            warmup_steps: int,\n",
    "            decay_steps: int,\n",
    "    ) -> None:\n",
    "        assert isinstance(warmup_steps, int), \"warmup_steps should be inteager type\"\n",
    "        assert isinstance(decay_steps, int), \"total_steps should be inteager type\"\n",
    "\n",
    "        super(TransformerLRScheduler, self).__init__(optimizer, init_lr)\n",
    "        self.final_lr = final_lr\n",
    "        self.peak_lr = peak_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "\n",
    "        self.warmup_rate = self.peak_lr / self.warmup_steps\n",
    "        self.decay_factor = -math.log(final_lr_scale) / self.decay_steps\n",
    "\n",
    "        self.init_lr = init_lr\n",
    "        self.update_steps = 0\n",
    "\n",
    "    def _decide_stage(self):\n",
    "        if self.update_steps < self.warmup_steps:\n",
    "            return 0, self.update_steps\n",
    "\n",
    "        if self.warmup_steps <= self.update_steps:\n",
    "            return 1, self.update_steps - self.warmup_steps\n",
    "\n",
    "        return 2, None\n",
    "\n",
    "    def step(self, val_loss: Optional[torch.FloatTensor] = None):\n",
    "        self.update_steps += 1\n",
    "        stage, steps_in_stage = self._decide_stage()\n",
    "\n",
    "        if stage == 0:\n",
    "            self.lr = self.update_steps * self.warmup_rate\n",
    "        elif stage == 1:\n",
    "            self.lr = self.peak_lr * math.exp(-self.decay_factor * steps_in_stage)\n",
    "        else:\n",
    "            raise ValueError(\"Undefined stage\")\n",
    "\n",
    "        self.set_lr(self.optimizer, self.lr)\n",
    "\n",
    "        return self.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'model': {\n",
    "        'dropout': 0, # was 0.1\n",
    "        'd_token': 230,\n",
    "        'd_model': 256,\n",
    "        'n_head_transf': 4, # was 2\n",
    "        'n_layers_transf': 3, # was 2\n",
    "        'd_latent': 256,\n",
    "        'gnn_input_dim': 256 + 4,\n",
    "        'gnn_n_layers': 3,\n",
    "        'actsnn_n_layers': 2,\n",
    "\n",
    "        'n_bars': 2,\n",
    "        'n_tracks': 4,\n",
    "        'resolution': 8\n",
    "    },\n",
    "    \n",
    "    'scheduler': {\n",
    "        'init_lr':1e-10, \n",
    "        'peak_lr': 1e-5,\n",
    "        'final_lr': 1e-7, \n",
    "        'final_lr_scale': 0.1,\n",
    "        'warmup_steps': 8000, \n",
    "        'decay_steps': 800000\n",
    "    },\n",
    "    \n",
    "    'optimizer': {\n",
    "        'betas': (0.9, 0.98),\n",
    "        'eps': 1e-09,\n",
    "        'lr': 5e-6\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model and moving it to the specified device...\n",
      "+----------------------------------------------------------------+------------+\n",
      "|                            Modules                             | Parameters |\n",
      "+----------------------------------------------------------------+------------+\n",
      "|              encoder.acts_encoder.layers.0.weight              |   32768    |\n",
      "|               encoder.acts_encoder.layers.0.bias               |    256     |\n",
      "|              encoder.acts_encoder.layers.1.weight              |   65536    |\n",
      "|               encoder.acts_encoder.layers.1.bias               |    256     |\n",
      "|                    encoder.embedding.weight                    |   58880    |\n",
      "|                     encoder.embedding.bias                     |    256     |\n",
      "| encoder.transformer_encoder.layers.0.self_attn.in_proj_weight  |   196608   |\n",
      "|  encoder.transformer_encoder.layers.0.self_attn.in_proj_bias   |    768     |\n",
      "| encoder.transformer_encoder.layers.0.self_attn.out_proj.weight |   65536    |\n",
      "|  encoder.transformer_encoder.layers.0.self_attn.out_proj.bias  |    256     |\n",
      "|      encoder.transformer_encoder.layers.0.linear1.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.0.linear1.bias        |    2048    |\n",
      "|      encoder.transformer_encoder.layers.0.linear2.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.0.linear2.bias        |    256     |\n",
      "|       encoder.transformer_encoder.layers.0.norm1.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.0.norm1.bias         |    256     |\n",
      "|       encoder.transformer_encoder.layers.0.norm2.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.0.norm2.bias         |    256     |\n",
      "| encoder.transformer_encoder.layers.1.self_attn.in_proj_weight  |   196608   |\n",
      "|  encoder.transformer_encoder.layers.1.self_attn.in_proj_bias   |    768     |\n",
      "| encoder.transformer_encoder.layers.1.self_attn.out_proj.weight |   65536    |\n",
      "|  encoder.transformer_encoder.layers.1.self_attn.out_proj.bias  |    256     |\n",
      "|      encoder.transformer_encoder.layers.1.linear1.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.1.linear1.bias        |    2048    |\n",
      "|      encoder.transformer_encoder.layers.1.linear2.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.1.linear2.bias        |    256     |\n",
      "|       encoder.transformer_encoder.layers.1.norm1.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.1.norm1.bias         |    256     |\n",
      "|       encoder.transformer_encoder.layers.1.norm2.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.1.norm2.bias         |    256     |\n",
      "| encoder.transformer_encoder.layers.2.self_attn.in_proj_weight  |   196608   |\n",
      "|  encoder.transformer_encoder.layers.2.self_attn.in_proj_bias   |    768     |\n",
      "| encoder.transformer_encoder.layers.2.self_attn.out_proj.weight |   65536    |\n",
      "|  encoder.transformer_encoder.layers.2.self_attn.out_proj.bias  |    256     |\n",
      "|      encoder.transformer_encoder.layers.2.linear1.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.2.linear1.bias        |    2048    |\n",
      "|      encoder.transformer_encoder.layers.2.linear2.weight       |   524288   |\n",
      "|       encoder.transformer_encoder.layers.2.linear2.bias        |    256     |\n",
      "|       encoder.transformer_encoder.layers.2.norm1.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.2.norm1.bias         |    256     |\n",
      "|       encoder.transformer_encoder.layers.2.norm2.weight        |    256     |\n",
      "|        encoder.transformer_encoder.layers.2.norm2.bias         |    256     |\n",
      "|             encoder.graph_encoder.layers.0.weight              |   199680   |\n",
      "|              encoder.graph_encoder.layers.0.root               |   66560    |\n",
      "|              encoder.graph_encoder.layers.0.bias               |    256     |\n",
      "|          encoder.graph_encoder.layers.0.dist_weights           |     32     |\n",
      "|             encoder.graph_encoder.layers.1.weight              |   196608   |\n",
      "|              encoder.graph_encoder.layers.1.root               |   65536    |\n",
      "|              encoder.graph_encoder.layers.1.bias               |    256     |\n",
      "|          encoder.graph_encoder.layers.1.dist_weights           |     32     |\n",
      "|             encoder.graph_encoder.layers.2.weight              |   196608   |\n",
      "|              encoder.graph_encoder.layers.2.root               |   65536    |\n",
      "|              encoder.graph_encoder.layers.2.bias               |    256     |\n",
      "|          encoder.graph_encoder.layers.2.dist_weights           |     32     |\n",
      "|                  encoder.linear_merge.weight                   |   131072   |\n",
      "|                   encoder.linear_merge.bias                    |    256     |\n",
      "|                  encoder.bars_encoder.weight                   |   131072   |\n",
      "|                   encoder.bars_encoder.bias                    |    256     |\n",
      "|                    encoder.linear_mu.weight                    |   65536    |\n",
      "|                     encoder.linear_mu.bias                     |    256     |\n",
      "|                 encoder.linear_log_var.weight                  |   65536    |\n",
      "|                  encoder.linear_log_var.bias                   |    256     |\n",
      "|                 decoder.z_decompressor.weight                  |   131072   |\n",
      "|                  decoder.z_decompressor.bias                   |    512     |\n",
      "|              decoder.acts_decoder.layers.0.weight              |   65536    |\n",
      "|               decoder.acts_decoder.layers.0.bias               |    256     |\n",
      "|              decoder.acts_decoder.layers.1.weight              |   32768    |\n",
      "|               decoder.acts_decoder.layers.1.bias               |    128     |\n",
      "|             decoder.graph_decoder.layers.0.weight              |   199680   |\n",
      "|              decoder.graph_decoder.layers.0.root               |   66560    |\n",
      "|              decoder.graph_decoder.layers.0.bias               |    256     |\n",
      "|          decoder.graph_decoder.layers.0.dist_weights           |     32     |\n",
      "|             decoder.graph_decoder.layers.1.weight              |   196608   |\n",
      "|              decoder.graph_decoder.layers.1.root               |   65536    |\n",
      "|              decoder.graph_decoder.layers.1.bias               |    256     |\n",
      "|          decoder.graph_decoder.layers.1.dist_weights           |     32     |\n",
      "|             decoder.graph_decoder.layers.2.weight              |   196608   |\n",
      "|              decoder.graph_decoder.layers.2.root               |   65536    |\n",
      "|              decoder.graph_decoder.layers.2.bias               |    256     |\n",
      "|          decoder.graph_decoder.layers.2.dist_weights           |     32     |\n",
      "|                    decoder.embedding.weight                    |   58880    |\n",
      "|                     decoder.embedding.bias                     |    256     |\n",
      "|    decoder.transf_decoder.layers.0.self_attn.in_proj_weight    |   196608   |\n",
      "|     decoder.transf_decoder.layers.0.self_attn.in_proj_bias     |    768     |\n",
      "|   decoder.transf_decoder.layers.0.self_attn.out_proj.weight    |   65536    |\n",
      "|    decoder.transf_decoder.layers.0.self_attn.out_proj.bias     |    256     |\n",
      "| decoder.transf_decoder.layers.0.multihead_attn.in_proj_weight  |   196608   |\n",
      "|  decoder.transf_decoder.layers.0.multihead_attn.in_proj_bias   |    768     |\n",
      "| decoder.transf_decoder.layers.0.multihead_attn.out_proj.weight |   65536    |\n",
      "|  decoder.transf_decoder.layers.0.multihead_attn.out_proj.bias  |    256     |\n",
      "|         decoder.transf_decoder.layers.0.linear1.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.0.linear1.bias          |    2048    |\n",
      "|         decoder.transf_decoder.layers.0.linear2.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.0.linear2.bias          |    256     |\n",
      "|          decoder.transf_decoder.layers.0.norm1.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.0.norm1.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.0.norm2.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.0.norm2.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.0.norm3.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.0.norm3.bias           |    256     |\n",
      "|    decoder.transf_decoder.layers.1.self_attn.in_proj_weight    |   196608   |\n",
      "|     decoder.transf_decoder.layers.1.self_attn.in_proj_bias     |    768     |\n",
      "|   decoder.transf_decoder.layers.1.self_attn.out_proj.weight    |   65536    |\n",
      "|    decoder.transf_decoder.layers.1.self_attn.out_proj.bias     |    256     |\n",
      "| decoder.transf_decoder.layers.1.multihead_attn.in_proj_weight  |   196608   |\n",
      "|  decoder.transf_decoder.layers.1.multihead_attn.in_proj_bias   |    768     |\n",
      "| decoder.transf_decoder.layers.1.multihead_attn.out_proj.weight |   65536    |\n",
      "|  decoder.transf_decoder.layers.1.multihead_attn.out_proj.bias  |    256     |\n",
      "|         decoder.transf_decoder.layers.1.linear1.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.1.linear1.bias          |    2048    |\n",
      "|         decoder.transf_decoder.layers.1.linear2.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.1.linear2.bias          |    256     |\n",
      "|          decoder.transf_decoder.layers.1.norm1.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.1.norm1.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.1.norm2.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.1.norm2.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.1.norm3.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.1.norm3.bias           |    256     |\n",
      "|    decoder.transf_decoder.layers.2.self_attn.in_proj_weight    |   196608   |\n",
      "|     decoder.transf_decoder.layers.2.self_attn.in_proj_bias     |    768     |\n",
      "|   decoder.transf_decoder.layers.2.self_attn.out_proj.weight    |   65536    |\n",
      "|    decoder.transf_decoder.layers.2.self_attn.out_proj.bias     |    256     |\n",
      "| decoder.transf_decoder.layers.2.multihead_attn.in_proj_weight  |   196608   |\n",
      "|  decoder.transf_decoder.layers.2.multihead_attn.in_proj_bias   |    768     |\n",
      "| decoder.transf_decoder.layers.2.multihead_attn.out_proj.weight |   65536    |\n",
      "|  decoder.transf_decoder.layers.2.multihead_attn.out_proj.bias  |    256     |\n",
      "|         decoder.transf_decoder.layers.2.linear1.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.2.linear1.bias          |    2048    |\n",
      "|         decoder.transf_decoder.layers.2.linear2.weight         |   524288   |\n",
      "|          decoder.transf_decoder.layers.2.linear2.bias          |    256     |\n",
      "|          decoder.transf_decoder.layers.2.norm1.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.2.norm1.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.2.norm2.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.2.norm2.bias           |    256     |\n",
      "|          decoder.transf_decoder.layers.2.norm3.weight          |    256     |\n",
      "|           decoder.transf_decoder.layers.2.norm3.bias           |    256     |\n",
      "|                       decoder.lin.weight                       |   58880    |\n",
      "|                        decoder.lin.bias                        |    230     |\n",
      "+----------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 11164966\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Starting training.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4468a0636f411e9ee06460a192e37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 256])\n",
      "Training on batch 1/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:02.60\n",
      "Losses:\n",
      "{'tot': 10.536662101745605, 'pitches': 5.071052074432373, 'dur': 4.768734455108643, 'acts': 0.6968751549720764, 'rec': 10.536662101745605, 'kld': 27.162864685058594, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.011963102035224438, 'dur': 0.0008648025104776025, 'acts_acc': 0.48248291015625, 'acts_precision': 0.16931818425655365, 'acts_recall': 0.5599398612976074, 'acts_f1': 0.2600122094154358}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 2/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:03.05\n",
      "Losses:\n",
      "{'tot': 10.431967735290527, 'pitches': 5.046939849853516, 'dur': 4.688830852508545, 'acts': 0.6961966156959534, 'rec': 10.431967735290527, 'kld': 27.391124725341797, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004825737327337265, 'dur': 0.0012064343318343163, 'acts_acc': 0.48974609375, 'acts_precision': 0.18563348054885864, 'acts_recall': 0.5856531262397766, 'acts_f1': 0.28191033005714417}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 3/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:03.32\n",
      "Losses:\n",
      "{'tot': 10.442028999328613, 'pitches': 5.055559158325195, 'dur': 4.68984842300415, 'acts': 0.6966218948364258, 'rec': 10.442028999328613, 'kld': 27.428627014160156, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004770164843648672, 'dur': 0.002601908054202795, 'acts_acc': 0.48797607421875, 'acts_precision': 0.1677691638469696, 'acts_recall': 0.5739368200302124, 'acts_f1': 0.2596416771411896}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 4/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:03.60\n",
      "Losses:\n",
      "{'tot': 10.439925193786621, 'pitches': 5.063718795776367, 'dur': 4.679832458496094, 'acts': 0.6963735818862915, 'rec': 10.439925193786621, 'kld': 27.396818161010742, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.009915784001350403, 'dur': 0.0006791632622480392, 'acts_acc': 0.48944091796875, 'acts_precision': 0.1771213561296463, 'acts_recall': 0.5745468139648438, 'acts_f1': 0.27076977491378784}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 5/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:03.90\n",
      "Losses:\n",
      "{'tot': 10.458085060119629, 'pitches': 5.068157196044922, 'dur': 4.69337797164917, 'acts': 0.6965494751930237, 'rec': 10.458085060119629, 'kld': 27.395999908447266, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004191963002085686, 'dur': 0.0017346054082736373, 'acts_acc': 0.484130859375, 'acts_precision': 0.17200180888175964, 'acts_recall': 0.5791271328926086, 'acts_f1': 0.26522988080978394}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 6/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:04.25\n",
      "Losses:\n",
      "{'tot': 10.449049949645996, 'pitches': 5.061397552490234, 'dur': 4.691060543060303, 'acts': 0.6965916752815247, 'rec': 10.449049949645996, 'kld': 27.367839813232422, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.0037052284460514784, 'dur': 0.0002744613739196211, 'acts_acc': 0.48614501953125, 'acts_precision': 0.17527297139167786, 'acts_recall': 0.5686346888542175, 'acts_f1': 0.26795336604118347}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 7/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:04.58\n",
      "Losses:\n",
      "{'tot': 10.418500900268555, 'pitches': 5.037787437438965, 'dur': 4.684592247009277, 'acts': 0.6961215734481812, 'rec': 10.418500900268555, 'kld': 27.300018310546875, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004586602095514536, 'dur': 0.00048280024202540517, 'acts_acc': 0.49249267578125, 'acts_precision': 0.19856294989585876, 'acts_recall': 0.5751569271087646, 'acts_f1': 0.2952098250389099}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 8/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:04.88\n",
      "Losses:\n",
      "{'tot': 10.443169593811035, 'pitches': 5.053457736968994, 'dur': 4.693373203277588, 'acts': 0.6963390707969666, 'rec': 10.443169593811035, 'kld': 27.29997444152832, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.00510594854131341, 'dur': 0.0008935409714467824, 'acts_acc': 0.48785400390625, 'acts_precision': 0.19022232294082642, 'acts_recall': 0.5731374025344849, 'acts_f1': 0.28564128279685974}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 9/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:05.17\n",
      "Losses:\n",
      "{'tot': 10.467361450195312, 'pitches': 5.065693378448486, 'dur': 4.705244541168213, 'acts': 0.6964240074157715, 'rec': 10.467361450195312, 'kld': 27.388273239135742, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.005398494191467762, 'dur': 0.0008523938013240695, 'acts_acc': 0.48779296875, 'acts_precision': 0.1773282140493393, 'acts_recall': 0.5843575596809387, 'acts_f1': 0.27208879590034485}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 10/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:05.48\n",
      "Losses:\n",
      "{'tot': 10.439447402954102, 'pitches': 5.062025547027588, 'dur': 4.68131160736084, 'acts': 0.696110188961029, 'rec': 10.439447402954102, 'kld': 27.342233657836914, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004425371997058392, 'dur': 0.0009387152967974544, 'acts_acc': 0.48944091796875, 'acts_precision': 0.17866972088813782, 'acts_recall': 0.5646973252296448, 'acts_f1': 0.2714522182941437}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 11/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:05.78\n",
      "Losses:\n",
      "{'tot': 10.422269821166992, 'pitches': 5.0329694747924805, 'dur': 4.692875385284424, 'acts': 0.6964246034622192, 'rec': 10.422269821166992, 'kld': 27.367464065551758, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.008868583478033543, 'dur': 0.0010749798966571689, 'acts_acc': 0.48577880859375, 'acts_precision': 0.17744122445583344, 'acts_recall': 0.5772058963775635, 'acts_f1': 0.27143844962120056}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 12/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:06.10\n",
      "Losses:\n",
      "{'tot': 10.460841178894043, 'pitches': 5.058715343475342, 'dur': 4.705420017242432, 'acts': 0.6967054605484009, 'rec': 10.460841178894043, 'kld': 27.41131591796875, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.005932671017944813, 'dur': 0.00013796909479424357, 'acts_acc': 0.48907470703125, 'acts_precision': 0.17889699339866638, 'acts_recall': 0.5736207365989685, 'acts_f1': 0.2727351784706116}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 13/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:06.41\n",
      "Losses:\n",
      "{'tot': 10.438328742980957, 'pitches': 5.056475639343262, 'dur': 4.685664176940918, 'acts': 0.6961886882781982, 'rec': 10.438328742980957, 'kld': 27.402111053466797, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.009367343969643116, 'dur': 0.0010087909176945686, 'acts_acc': 0.488037109375, 'acts_precision': 0.17161791026592255, 'acts_recall': 0.5685086846351624, 'acts_f1': 0.26364749670028687}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 14/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:06.74\n",
      "Losses:\n",
      "{'tot': 10.417113304138184, 'pitches': 5.0329999923706055, 'dur': 4.688061237335205, 'acts': 0.6960512399673462, 'rec': 10.417113304138184, 'kld': 27.39189910888672, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.006141248624771833, 'dur': 0.0007676560780964792, 'acts_acc': 0.4884033203125, 'acts_precision': 0.19165906310081482, 'acts_recall': 0.5669025778770447, 'acts_f1': 0.2864685356616974}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 15/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:07.05\n",
      "Losses:\n",
      "{'tot': 10.42260456085205, 'pitches': 5.032965660095215, 'dur': 4.6934590339660645, 'acts': 0.6961795091629028, 'rec': 10.42260456085205, 'kld': 27.300241470336914, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.013290876522660255, 'dur': 0.0011075730435550213, 'acts_acc': 0.49072265625, 'acts_precision': 0.17994505167007446, 'acts_recall': 0.570806086063385, 'acts_f1': 0.27362924814224243}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 16/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:07.36\n",
      "Losses:\n",
      "{'tot': 10.416597366333008, 'pitches': 5.017157077789307, 'dur': 4.7029876708984375, 'acts': 0.6964532732963562, 'rec': 10.416597366333008, 'kld': 27.392131805419922, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.012328383512794971, 'dur': 0.0007004763465374708, 'acts_acc': 0.48944091796875, 'acts_precision': 0.17584940791130066, 'acts_recall': 0.5642725825309753, 'acts_f1': 0.2681368887424469}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 17/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:07.63\n",
      "Losses:\n",
      "{'tot': 10.456161499023438, 'pitches': 5.049880504608154, 'dur': 4.709965229034424, 'acts': 0.696315348148346, 'rec': 10.456161499023438, 'kld': 27.427242279052734, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.006372326053678989, 'dur': 0.0007586102001369, 'acts_acc': 0.49041748046875, 'acts_precision': 0.17113636434078217, 'acts_recall': 0.5882812738418579, 'acts_f1': 0.2651408612728119}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 18/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:07.94\n",
      "Losses:\n",
      "{'tot': 10.429948806762695, 'pitches': 5.0335211753845215, 'dur': 4.700214385986328, 'acts': 0.6962130069732666, 'rec': 10.429948806762695, 'kld': 27.379838943481445, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.006646867375820875, 'dur': 0.0001414227153873071, 'acts_acc': 0.489013671875, 'acts_precision': 0.17615698277950287, 'acts_recall': 0.5827392339706421, 'acts_f1': 0.2705339193344116}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 19/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:08.23\n",
      "Losses:\n",
      "{'tot': 10.437499046325684, 'pitches': 5.052554607391357, 'dur': 4.6884002685546875, 'acts': 0.6965440511703491, 'rec': 10.437499046325684, 'kld': 27.31707000732422, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004400762729346752, 'dur': 0.0004400762845762074, 'acts_acc': 0.48504638671875, 'acts_precision': 0.16986890137195587, 'acts_recall': 0.5789676308631897, 'acts_f1': 0.2626703977584839}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 20/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:08.53\n",
      "Losses:\n",
      "{'tot': 10.426069259643555, 'pitches': 5.024156093597412, 'dur': 4.706001281738281, 'acts': 0.6959109902381897, 'rec': 10.426069259643555, 'kld': 27.33871078491211, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.011940298601984978, 'dur': 0.000852878438308835, 'acts_acc': 0.4906005859375, 'acts_precision': 0.182796910405159, 'acts_recall': 0.5871860384941101, 'acts_f1': 0.27880042791366577}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 21/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:08.84\n",
      "Losses:\n",
      "{'tot': 10.454779624938965, 'pitches': 5.064834117889404, 'dur': 4.69315767288208, 'acts': 0.69678795337677, 'rec': 10.454779624938965, 'kld': 27.46270179748535, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.006097560748457909, 'dur': 0.00044616300147026777, 'acts_acc': 0.4854736328125, 'acts_precision': 0.16890966892242432, 'acts_recall': 0.5650514960289001, 'acts_f1': 0.26007550954818726}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 22/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:09.13\n",
      "Losses:\n",
      "{'tot': 10.4338960647583, 'pitches': 5.034380912780762, 'dur': 4.702843189239502, 'acts': 0.6966716051101685, 'rec': 10.4338960647583, 'kld': 27.228290557861328, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.016517549753189087, 'dur': 0.002339986152946949, 'acts_acc': 0.4857177734375, 'acts_precision': 0.18081974983215332, 'acts_recall': 0.5719913840293884, 'acts_f1': 0.27477630972862244}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 23/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:09.43\n",
      "Losses:\n",
      "{'tot': 10.426298141479492, 'pitches': 5.032351970672607, 'dur': 4.697099685668945, 'acts': 0.6968458294868469, 'rec': 10.426298141479492, 'kld': 27.354352951049805, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.009940067306160927, 'dur': 0.0016079520573839545, 'acts_acc': 0.48736572265625, 'acts_precision': 0.1679292917251587, 'acts_recall': 0.5603216886520386, 'acts_f1': 0.25841209292411804}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 24/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:09.73\n",
      "Losses:\n",
      "{'tot': 10.442903518676758, 'pitches': 5.063947677612305, 'dur': 4.682642936706543, 'acts': 0.6963127255439758, 'rec': 10.442903518676758, 'kld': 27.39173126220703, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.008523880504071712, 'dur': 0.0017588959308341146, 'acts_acc': 0.49273681640625, 'acts_precision': 0.18498168885707855, 'acts_recall': 0.5752937197685242, 'acts_f1': 0.27994802594184875}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 25/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:10.06\n",
      "Losses:\n",
      "{'tot': 10.419866561889648, 'pitches': 5.041270732879639, 'dur': 4.682893753051758, 'acts': 0.6957012414932251, 'rec': 10.419866561889648, 'kld': 27.443622589111328, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.007422841619700193, 'dur': 0.0014324781950563192, 'acts_acc': 0.4932861328125, 'acts_precision': 0.18932481110095978, 'acts_recall': 0.58163982629776, 'acts_f1': 0.2856651246547699}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 26/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:10.37\n",
      "Losses:\n",
      "{'tot': 10.42070484161377, 'pitches': 5.03766393661499, 'dur': 4.686664581298828, 'acts': 0.6963756680488586, 'rec': 10.42070484161377, 'kld': 27.398738861083984, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.00013812154065817595, 'pitches': 0.006629834417253733, 'dur': 0.0015193370636552572, 'acts_acc': 0.4852294921875, 'acts_precision': 0.17797186970710754, 'acts_recall': 0.5686842799186707, 'acts_f1': 0.2711015045642853}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 27/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:10.68\n",
      "Losses:\n",
      "{'tot': 10.42674446105957, 'pitches': 5.025109767913818, 'dur': 4.705210208892822, 'acts': 0.6964247226715088, 'rec': 10.42674446105957, 'kld': 27.466299057006836, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.008778346702456474, 'dur': 0.0004389173409435898, 'acts_acc': 0.484375, 'acts_precision': 0.17167721688747406, 'acts_recall': 0.5764706134796143, 'acts_f1': 0.26456502079963684}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 28/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:10.99\n",
      "Losses:\n",
      "{'tot': 10.39999771118164, 'pitches': 5.024064540863037, 'dur': 4.679818630218506, 'acts': 0.6961144804954529, 'rec': 10.39999771118164, 'kld': 27.313819885253906, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0001365187781630084, 'pitches': 0.0043686009012162685, 'dur': 0.0008191126398742199, 'acts_acc': 0.48895263671875, 'acts_precision': 0.17622950673103333, 'acts_recall': 0.5771812200546265, 'acts_f1': 0.2700157165527344}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 29/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:11.31\n",
      "Losses:\n",
      "{'tot': 10.446297645568848, 'pitches': 5.037669658660889, 'dur': 4.712330341339111, 'acts': 0.6962976455688477, 'rec': 10.446297645568848, 'kld': 27.18735694885254, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.008720150217413902, 'dur': 0.0004024684603791684, 'acts_acc': 0.48779296875, 'acts_precision': 0.1822822093963623, 'acts_recall': 0.5751610398292542, 'acts_f1': 0.2768303155899048}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 30/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:11.64\n",
      "Losses:\n",
      "{'tot': 10.431792259216309, 'pitches': 5.038189888000488, 'dur': 4.697547435760498, 'acts': 0.6960557699203491, 'rec': 10.431792259216309, 'kld': 27.325511932373047, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.005689383950084448, 'dur': 0.0008473550551570952, 'acts_acc': 0.48785400390625, 'acts_precision': 0.19806654751300812, 'acts_recall': 0.5832505822181702, 'acts_f1': 0.29571202397346497}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 31/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:11.95\n",
      "Losses:\n",
      "{'tot': 10.4163179397583, 'pitches': 5.041917324066162, 'dur': 4.678730010986328, 'acts': 0.6956698298454285, 'rec': 10.4163179397583, 'kld': 27.370441436767578, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0001341741590294987, 'pitches': 0.002817657310515642, 'dur': 0.0018784381682053208, 'acts_acc': 0.49273681640625, 'acts_precision': 0.18607565760612488, 'acts_recall': 0.5827980041503906, 'acts_f1': 0.28208670020103455}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 32/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:12.26\n",
      "Losses:\n",
      "{'tot': 10.433788299560547, 'pitches': 5.064340114593506, 'dur': 4.673251152038574, 'acts': 0.6961979269981384, 'rec': 10.433788299560547, 'kld': 27.36003875732422, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.0029981262050569057, 'dur': 0.001374141196720302, 'acts_acc': 0.4881591796875, 'acts_precision': 0.19170454144477844, 'acts_recall': 0.5691633224487305, 'acts_f1': 0.28680723905563354}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 33/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:12.56\n",
      "Losses:\n",
      "{'tot': 10.464866638183594, 'pitches': 5.052330493927002, 'dur': 4.715407371520996, 'acts': 0.6971282958984375, 'rec': 10.464866638183594, 'kld': 27.416671752929688, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.0027917365077883005, 'dur': 0.0016750418581068516, 'acts_acc': 0.48284912109375, 'acts_precision': 0.17249999940395355, 'acts_recall': 0.5601475834846497, 'acts_f1': 0.26377061009407043}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 34/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:12.87\n",
      "Losses:\n",
      "{'tot': 10.454302787780762, 'pitches': 5.0622100830078125, 'dur': 4.695867538452148, 'acts': 0.6962248086929321, 'rec': 10.454302787780762, 'kld': 27.412334442138672, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.005173587705940008, 'dur': 0.0010891762794926763, 'acts_acc': 0.49005126953125, 'acts_precision': 0.177734375, 'acts_recall': 0.5643925666809082, 'acts_f1': 0.270336389541626}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 35/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:13.18\n",
      "Losses:\n",
      "{'tot': 10.481884956359863, 'pitches': 5.071864604949951, 'dur': 4.713543891906738, 'acts': 0.6964768767356873, 'rec': 10.481884956359863, 'kld': 27.363861083984375, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0001387347438139841, 'pitches': 0.0029134296346455812, 'dur': 0.0008324084337800741, 'acts_acc': 0.48541259765625, 'acts_precision': 0.1786363571882248, 'acts_recall': 0.566895067691803, 'acts_f1': 0.2716667950153351}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 36/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:13.51\n",
      "Losses:\n",
      "{'tot': 10.429959297180176, 'pitches': 5.046710968017578, 'dur': 4.686661243438721, 'acts': 0.6965873837471008, 'rec': 10.429959297180176, 'kld': 27.457765579223633, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.0030491845682263374, 'dur': 0.0007954394677653909, 'acts_acc': 0.48394775390625, 'acts_precision': 0.1823010891675949, 'acts_recall': 0.5685583353042603, 'acts_f1': 0.276080459356308}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 37/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:13.81\n",
      "Losses:\n",
      "{'tot': 10.424436569213867, 'pitches': 5.047515392303467, 'dur': 4.680154800415039, 'acts': 0.6967661380767822, 'rec': 10.424436569213867, 'kld': 27.381561279296875, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.010296540334820747, 'dur': 0.001922020921483636, 'acts_acc': 0.4815673828125, 'acts_precision': 0.1703110933303833, 'acts_recall': 0.5710506439208984, 'acts_f1': 0.2623719274997711}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 38/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:14.09\n",
      "Losses:\n",
      "{'tot': 10.369564056396484, 'pitches': 5.018406867980957, 'dur': 4.654750347137451, 'acts': 0.6964074373245239, 'rec': 10.369564056396484, 'kld': 27.395160675048828, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.011292679235339165, 'dur': 0.0010628404561430216, 'acts_acc': 0.48516845703125, 'acts_precision': 0.17504549026489258, 'acts_recall': 0.5658088326454163, 'acts_f1': 0.2673731744289398}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 39/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:14.40\n",
      "Losses:\n",
      "{'tot': 10.410056114196777, 'pitches': 5.0430169105529785, 'dur': 4.671078681945801, 'acts': 0.6959600448608398, 'rec': 10.410056114196777, 'kld': 27.25400161743164, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.00013099292118567973, 'pitches': 0.006549646146595478, 'dur': 0.0009169504628516734, 'acts_acc': 0.486328125, 'acts_precision': 0.18727272748947144, 'acts_recall': 0.5655456185340881, 'acts_f1': 0.28137272596359253}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 40/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:14.70\n",
      "Losses:\n",
      "{'tot': 10.442978858947754, 'pitches': 5.060070991516113, 'dur': 4.686534404754639, 'acts': 0.6963742971420288, 'rec': 10.442978858947754, 'kld': 27.334312438964844, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.008152549155056477, 'dur': 0.0006908940267749131, 'acts_acc': 0.485107421875, 'acts_precision': 0.18091809749603271, 'acts_recall': 0.5821868181228638, 'acts_f1': 0.27605152130126953}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 41/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:15.01\n",
      "Losses:\n",
      "{'tot': 10.399827003479004, 'pitches': 5.018166542053223, 'dur': 4.684764862060547, 'acts': 0.6968957185745239, 'rec': 10.399827003479004, 'kld': 27.41856575012207, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.00013399437011685222, 'pitches': 0.006163740996271372, 'dur': 0.0016079324996098876, 'acts_acc': 0.4842529296875, 'acts_precision': 0.17456501722335815, 'acts_recall': 0.5519363284111023, 'acts_f1': 0.26524046063423157}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 42/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:15.31\n",
      "Losses:\n",
      "{'tot': 10.40307331085205, 'pitches': 5.043412208557129, 'dur': 4.663500785827637, 'acts': 0.6961603164672852, 'rec': 10.40307331085205, 'kld': 27.30527687072754, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.008750482462346554, 'dur': 0.0005147342453710735, 'acts_acc': 0.48602294921875, 'acts_precision': 0.18307621777057648, 'acts_recall': 0.569915235042572, 'acts_f1': 0.2771291136741638}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 43/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:15.62\n",
      "Losses:\n",
      "{'tot': 10.427599906921387, 'pitches': 5.043358325958252, 'dur': 4.688211441040039, 'acts': 0.6960303783416748, 'rec': 10.427599906921387, 'kld': 27.405704498291016, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004807692486792803, 'dur': 0.0016483516665175557, 'acts_acc': 0.489990234375, 'acts_precision': 0.18095996975898743, 'acts_recall': 0.580021858215332, 'acts_f1': 0.2758561074733734}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 44/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:15.92\n",
      "Losses:\n",
      "{'tot': 10.398409843444824, 'pitches': 5.03677225112915, 'dur': 4.665450572967529, 'acts': 0.696187436580658, 'rec': 10.398409843444824, 'kld': 27.227901458740234, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.009048666805028915, 'dur': 0.00024455858510918915, 'acts_acc': 0.4891357421875, 'acts_precision': 0.19346049427986145, 'acts_recall': 0.5741239786148071, 'acts_f1': 0.28940218687057495}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 45/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:16.22\n",
      "Losses:\n",
      "{'tot': 10.423727989196777, 'pitches': 5.036175727844238, 'dur': 4.691352844238281, 'acts': 0.6961991786956787, 'rec': 10.423727989196777, 'kld': 27.30496597290039, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.010777745395898819, 'dur': 0.0023303234484046698, 'acts_acc': 0.49029541015625, 'acts_precision': 0.1727605164051056, 'acts_recall': 0.5764392018318176, 'acts_f1': 0.2658461630344391}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 46/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:16.51\n",
      "Losses:\n",
      "{'tot': 10.42635440826416, 'pitches': 5.045098781585693, 'dur': 4.684741973876953, 'acts': 0.6965134143829346, 'rec': 10.42635440826416, 'kld': 27.455411911010742, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.00014532770728692412, 'pitches': 0.007411713246256113, 'dur': 0.0010172940092161298, 'acts_acc': 0.4849853515625, 'acts_precision': 0.16384826600551605, 'acts_recall': 0.5614721775054932, 'acts_f1': 0.2536706328392029}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 47/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:16.78\n",
      "Losses:\n",
      "{'tot': 10.428332328796387, 'pitches': 5.0441999435424805, 'dur': 4.687908172607422, 'acts': 0.6962245106697083, 'rec': 10.428332328796387, 'kld': 27.384693145751953, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.005328318569809198, 'dur': 0.0006268610013648868, 'acts_acc': 0.48480224609375, 'acts_precision': 0.15959618985652924, 'acts_recall': 0.5775862336158752, 'acts_f1': 0.2500888705253601}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 48/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:17.10\n",
      "Losses:\n",
      "{'tot': 10.439643859863281, 'pitches': 5.071600437164307, 'dur': 4.6716108322143555, 'acts': 0.6964322328567505, 'rec': 10.439643859863281, 'kld': 27.371448516845703, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.0034919814206659794, 'dur': 0.0007759958389215171, 'acts_acc': 0.48492431640625, 'acts_precision': 0.18060578405857086, 'acts_recall': 0.5742005109786987, 'acts_f1': 0.2747828960418701}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 49/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:17.42\n",
      "Losses:\n",
      "{'tot': 10.438729286193848, 'pitches': 5.040771961212158, 'dur': 4.701661586761475, 'acts': 0.6962960958480835, 'rec': 10.438729286193848, 'kld': 27.396303176879883, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004776436369866133, 'dur': 0.00039803635445423424, 'acts_acc': 0.4920654296875, 'acts_precision': 0.187386155128479, 'acts_recall': 0.5810095071792603, 'acts_f1': 0.2833777964115143}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 50/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:17.74\n",
      "Losses:\n",
      "{'tot': 10.43748950958252, 'pitches': 5.0470099449157715, 'dur': 4.694116115570068, 'acts': 0.6963635683059692, 'rec': 10.43748950958252, 'kld': 27.439708709716797, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.009115646593272686, 'dur': 0.003129251766949892, 'acts_acc': 0.49102783203125, 'acts_precision': 0.18156933784484863, 'acts_recall': 0.5770206451416016, 'acts_f1': 0.27622103691101074}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 51/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:18.02\n",
      "Losses:\n",
      "{'tot': 10.4476900100708, 'pitches': 5.057820796966553, 'dur': 4.693351745605469, 'acts': 0.696516752243042, 'rec': 10.4476900100708, 'kld': 27.389060974121094, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.00846849475055933, 'dur': 0.0008612028323113918, 'acts_acc': 0.48736572265625, 'acts_precision': 0.16901728510856628, 'acts_recall': 0.5768633484840393, 'acts_f1': 0.2614356279373169}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 52/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:18.34\n",
      "Losses:\n",
      "{'tot': 10.434671401977539, 'pitches': 5.050865650177002, 'dur': 4.687387466430664, 'acts': 0.6964184641838074, 'rec': 10.434671401977539, 'kld': 27.355796813964844, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.00954147893935442, 'dur': 0.0006626027170568705, 'acts_acc': 0.48223876953125, 'acts_precision': 0.1831081062555313, 'acts_recall': 0.5697267055511475, 'acts_f1': 0.27714335918426514}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 53/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:18.63\n",
      "Losses:\n",
      "{'tot': 10.408807754516602, 'pitches': 5.040578365325928, 'dur': 4.671449661254883, 'acts': 0.6967789530754089, 'rec': 10.408807754516602, 'kld': 27.34978485107422, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.005721642170101404, 'dur': 0.000286082096863538, 'acts_acc': 0.4892578125, 'acts_precision': 0.17153283953666687, 'acts_recall': 0.5760245323181152, 'acts_f1': 0.2643465995788574}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 54/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:18.93\n",
      "Losses:\n",
      "{'tot': 10.39578628540039, 'pitches': 5.034505367279053, 'dur': 4.665051460266113, 'acts': 0.696230411529541, 'rec': 10.39578628540039, 'kld': 27.29669952392578, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.008809431456029415, 'dur': 0.0009068532381206751, 'acts_acc': 0.48834228515625, 'acts_precision': 0.1855751872062683, 'acts_recall': 0.5787429213523865, 'acts_f1': 0.2810356616973877}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 55/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:19.22\n",
      "Losses:\n",
      "{'tot': 10.407076835632324, 'pitches': 5.034236907958984, 'dur': 4.676131725311279, 'acts': 0.6967073678970337, 'rec': 10.407076835632324, 'kld': 27.460660934448242, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004409045446664095, 'dur': 0.0005689091049134731, 'acts_acc': 0.48394775390625, 'acts_precision': 0.16742493212223053, 'acts_recall': 0.5646336674690247, 'acts_f1': 0.258268266916275}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 56/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:19.51\n",
      "Losses:\n",
      "{'tot': 10.40990924835205, 'pitches': 5.031423568725586, 'dur': 4.682717323303223, 'acts': 0.6957682371139526, 'rec': 10.40990924835205, 'kld': 27.415836334228516, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.005646527279168367, 'dur': 0.0009881423320621252, 'acts_acc': 0.4859619140625, 'acts_precision': 0.17558982968330383, 'acts_recall': 0.5733333230018616, 'acts_f1': 0.26884332299232483}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 57/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:19.81\n",
      "Losses:\n",
      "{'tot': 10.408018112182617, 'pitches': 5.021856307983398, 'dur': 4.6899495124816895, 'acts': 0.696212649345398, 'rec': 10.408018112182617, 'kld': 27.404979705810547, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.00937500037252903, 'dur': 0.00390625, 'acts_acc': 0.4891357421875, 'acts_precision': 0.16357600688934326, 'acts_recall': 0.5738955736160278, 'acts_f1': 0.2545875608921051}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 58/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:20.12\n",
      "Losses:\n",
      "{'tot': 10.446451187133789, 'pitches': 5.037134170532227, 'dur': 4.713003158569336, 'acts': 0.6963136196136475, 'rec': 10.446451187133789, 'kld': 27.4931583404541, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.0038851117715239525, 'dur': 0.0029138338286429644, 'acts_acc': 0.48712158203125, 'acts_precision': 0.17447440326213837, 'acts_recall': 0.56451016664505, 'acts_f1': 0.26656192541122437}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 59/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:20.44\n",
      "Losses:\n",
      "{'tot': 10.406051635742188, 'pitches': 5.0343732833862305, 'dur': 4.675538539886475, 'acts': 0.6961407661437988, 'rec': 10.406051635742188, 'kld': 27.34036636352539, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004547278396785259, 'dur': 0.0006687173736281693, 'acts_acc': 0.4910888671875, 'acts_precision': 0.18150684237480164, 'acts_recall': 0.5767138004302979, 'acts_f1': 0.2761135399341583}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 60/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:20.75\n",
      "Losses:\n",
      "{'tot': 10.403581619262695, 'pitches': 5.016839027404785, 'dur': 4.690251350402832, 'acts': 0.6964907646179199, 'rec': 10.403581619262695, 'kld': 27.41628646850586, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.00013126805424690247, 'pitches': 0.008794959634542465, 'dur': 0.0005250722169876099, 'acts_acc': 0.49029541015625, 'acts_precision': 0.18653497099876404, 'acts_recall': 0.5811814665794373, 'acts_f1': 0.282423734664917}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 61/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:21.05\n",
      "Losses:\n",
      "{'tot': 10.433311462402344, 'pitches': 5.063575267791748, 'dur': 4.673827648162842, 'acts': 0.6959080696105957, 'rec': 10.433311462402344, 'kld': 27.342315673828125, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004225538112223148, 'dur': 0.0007922883960418403, 'acts_acc': 0.4945068359375, 'acts_precision': 0.18899357318878174, 'acts_recall': 0.5743715167045593, 'acts_f1': 0.28440526127815247}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 62/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:21.35\n",
      "Losses:\n",
      "{'tot': 10.396452903747559, 'pitches': 5.030026435852051, 'dur': 4.670023441314697, 'acts': 0.6964024305343628, 'rec': 10.396452903747559, 'kld': 27.45645523071289, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.004505734425038099, 'dur': 0.002048061229288578, 'acts_acc': 0.4891357421875, 'acts_precision': 0.17930327355861664, 'acts_recall': 0.5750274062156677, 'acts_f1': 0.2733663022518158}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 63/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:21.65\n",
      "Losses:\n",
      "{'tot': 10.38548469543457, 'pitches': 5.025500774383545, 'dur': 4.6640119552612305, 'acts': 0.695972740650177, 'rec': 10.38548469543457, 'kld': 27.449705123901367, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.00754561647772789, 'dur': 0.00041157909436151385, 'acts_acc': 0.49395751953125, 'acts_precision': 0.1794373244047165, 'acts_recall': 0.5839225649833679, 'acts_f1': 0.27451664209365845}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 64/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:21.96\n",
      "Losses:\n",
      "{'tot': 10.445903778076172, 'pitches': 5.059909820556641, 'dur': 4.6900506019592285, 'acts': 0.6959435939788818, 'rec': 10.445903778076172, 'kld': 27.35715103149414, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.003135650884360075, 'dur': 0.0014996591489762068, 'acts_acc': 0.48992919921875, 'acts_precision': 0.1812959611415863, 'acts_recall': 0.5621660351753235, 'acts_f1': 0.2741725444793701}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n",
      "Training on batch 65/74528 of epoch 1/100 complete.\n",
      "Elapsed time from start (h:m:s): 00:00:22.26\n",
      "Losses:\n",
      "{'tot': 10.390711784362793, 'pitches': 5.025352954864502, 'dur': 4.66884708404541, 'acts': 0.6965116262435913, 'rec': 10.390711784362793, 'kld': 27.428823471069336, 'beta*kld': 0.0}\n",
      "Accuracies:\n",
      "{'notes': 0.0, 'pitches': 0.0044041103683412075, 'dur': 0.0028026157524436712, 'acts_acc': 0.48748779296875, 'acts_precision': 0.18277980387210846, 'acts_recall': 0.5706676244735718, 'acts_f1': 0.2768780291080475}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "torch.Size([128, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric\n",
    "\n",
    "print(\"Creating the model and moving it to the specified device...\")\n",
    "\n",
    "# Creating the model\n",
    "vae = VAE(**parameters['model']).to(device)\n",
    "#vae = torch_geometric.nn.DataParallel(vae, device_ids=[0, 1, 2])\n",
    "print_params(vae)\n",
    "print()\n",
    "\n",
    "# Creating optimizer and scheduler\n",
    "optimizer = optim.Adam(vae.parameters(), **parameters['optimizer'])\n",
    "scheduler = TransformerLRScheduler(\n",
    "    optimizer=optimizer,\n",
    "    **parameters['scheduler']\n",
    ")\n",
    "\n",
    "# Create model dir\n",
    "models_dir = 'models/'\n",
    "model_name = '2barsACTS'\n",
    "model_dir = os.path.join(models_dir, model_name)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=False)\n",
    "\n",
    "# Save parameters\n",
    "params_path = os.path.join(model_dir, 'params')\n",
    "torch.save(parameters, params_path)\n",
    "\n",
    "print('--------------------------------------------------\\n')\n",
    "\n",
    "trainer = VAETrainer(\n",
    "    model_dir,\n",
    "    model=vae,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=scheduler,\n",
    "    save_every=300,\n",
    "    print_every=1,\n",
    "    eval_every=6000000,\n",
    "    iters_to_accumulate=1,\n",
    "    device=device\n",
    ")\n",
    "trainer.train(trainloader, validloader=validloader, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training from checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is the model? in models/model_name/checkpoint (cause we don't need the best yet)\n",
    "# What were its parameters? model_state_dict\n",
    "# What were the scheduler's parameters? optimizer_state_dict\n",
    "# What were the optimizer's parameters (Adam)? scheduler_state_dict\n",
    "# --> Ok! Now we have a model, a scheduler and an optimizer.\n",
    "\n",
    "# What else do we need? A training checkpoint! This should be gathered inside Trainer!\n",
    "# A training checkpoint is composed of many things such as:\n",
    "#    last epoch, last batch, ...\n",
    "# However, we also need a way to store the dataloaders. Why?\n",
    "# At the beginning we have a full dataset in a directory. Then, we create two (three) subsets,\n",
    "# corresponding to TR, VL (TS). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('models/just_pitches_warmup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = checkpoint['model_state_dict']\n",
    "vae = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, inputs in enumerate(loader):\n",
    "    \n",
    "    x_seq, x_acts, x_graph, src_mask = inputs\n",
    "    x_seq = x_seq.float().to(device)\n",
    "    x_acts = x_acts.to(device)\n",
    "    x_graph = x_graph.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "    tgt_mask = generate_square_subsequent_mask(x_seq.size(-2)-1).to(device)\n",
    "\n",
    "    # Forward pass, get the reconstructions\n",
    "    outputs, mu, log_var = vae(x_seq, x_acts, x_graph, src_mask, tgt_mask)\n",
    "    \n",
    "    break\n",
    "\n",
    "seq_rec, _  = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_rec.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_acts.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dense reconstruction from sparse reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_rec_dense = torch.zeros(x_seq.size(), dtype=torch.float).to(device)\n",
    "seq_rec_dense = seq_rec_dense[..., 1:, :]\n",
    "size = seq_rec_dense.size()\n",
    "\n",
    "seq_rec_dense = seq_rec_dense.view(-1, seq_rec_dense.size(-2), seq_rec_dense.size(-1))\n",
    "\n",
    "silence = torch.zeros(seq_rec_dense.size(-2), seq_rec_dense.size(-1)).to(device)\n",
    "silence[:, 129] = 1. # eos token\n",
    "\n",
    "seq_rec_dense[x_acts.bool().view(-1)] = seq_rec\n",
    "seq_rec_dense[torch.logical_not(x_acts.bool().view(-1))] = silence\n",
    "\n",
    "seq_rec_dense = seq_rec_dense.view(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq_rec_dense.size())\n",
    "print(x_seq.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_real = x_seq[0]\n",
    "music_rec = seq_rec_dense[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_real.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"data/music/\"\n",
    "\n",
    "real = from_tensor_to_muspy(music_real, track_data)\n",
    "muspy.show_pianoroll(real, yticklabel='off', grid_axis='off')\n",
    "plt.savefig(prefix + \"real\" + \".png\")\n",
    "muspy.write_midi(prefix + \"real\" + \".mid\", real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = from_tensor_to_muspy(music_rec, track_data)\n",
    "muspy.show_pianoroll(rec, yticklabel='off', grid_axis='off')\n",
    "plt.savefig(prefix + \"rec\" + \".png\")\n",
    "muspy.write_midi(prefix + \"rec\" + \".mid\", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot music and save it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracks = [drum_track, bass_track, guitar_track, strings_track]\n",
    "import copy\n",
    "\n",
    "def from_tensor_to_muspy(music_tensor, track_data):\n",
    "    \n",
    "    powers = torch.tensor([2**n for n in reversed(range(9))], dtype=torch.float)\n",
    "    tracks = []\n",
    "    \n",
    "    for tr in range(music_tensor.size(0)):\n",
    "        \n",
    "        notes = []\n",
    "        \n",
    "        for ts in range(music_tensor.size(1)):\n",
    "            for note in range(music_tensor.size(2)):\n",
    "                \n",
    "                pitch = music_tensor[tr, ts, note, :131]\n",
    "                pitch = torch.argmax(pitch)\n",
    "\n",
    "                if pitch == 129:\n",
    "                    break\n",
    "                \n",
    "                if pitch != 128:\n",
    "                    #dur = music_tensor[tr, ts, note, 131:]\n",
    "                    #dur = torch.dot(dur, powers).long()\n",
    "                    \n",
    "                    dur = 4\n",
    "                    \n",
    "                    #notes.append(muspy.Note(ts, pitch.item(), dur.item(), 64))\n",
    "                    notes.append(muspy.Note(ts, pitch.item(), dur, 64))\n",
    "        \n",
    "        if track_data[tr][0] == 'Drums':\n",
    "            track = muspy.Track(name='Drums', is_drum=True, notes=copy.deepcopy(notes))\n",
    "        else:\n",
    "            track = muspy.Track(name=track_data[tr][0], \n",
    "                                program=track_data[tr][1],\n",
    "                                notes=copy.deepcopy(notes))\n",
    "        tracks.append(track)\n",
    "    \n",
    "    meta = muspy.Metadata(title='prova')\n",
    "    music = muspy.Music(tracks=tracks, metadata=meta, resolution=RESOLUTION)\n",
    "    \n",
    "    return music\n",
    "\n",
    "\n",
    "track_data = [('Drums', -1), ('Bass', 34), ('Guitar', 1), ('Strings', 41)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"data/music/file\"\n",
    "\n",
    "for i in range(10):\n",
    "    music_tensor = dataset[20+i][0]\n",
    "    music = from_tensor_to_muspy(music_tensor, track_data)\n",
    "    muspy.show_pianoroll(music, yticklabel='off', grid_axis='off')\n",
    "    plt.savefig(prefix + str(i) + \".png\")\n",
    "    muspy.write_midi(prefix + str(i) + \".mid\", music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_path = \"data/music/file2.mid\"\n",
    "muspy.show_pianoroll(music, yticklabel='off', grid_axis='off')\n",
    "plt.savefig('file2.png')\n",
    "muspy.write_midi(music_path, music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][0].size())\n",
    "notes = []\n",
    "notes.append(muspy.Note(1, 48, 20, 64))\n",
    "drums = muspy.Track(is_drum=True)\n",
    "bass = muspy.Track(program=34, notes=notes)\n",
    "guitar = muspy.Track(program=27, notes=[])\n",
    "strings = muspy.Track(program=42, notes=[muspy.Note(0, 100, 4, 64), muspy.Note(4, 91, 20, 64)])\n",
    "\n",
    "tracks = [drums, bass, guitar, strings]\n",
    "\n",
    "meta = muspy.Metadata(title='prova')\n",
    "music = muspy.Music(tracks=tracks, metadata=meta, resolution=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/lmd_matched/M/T/O/TRMTOBP128E07822EF/63edabc86c087f07eca448b0edad53c3.mid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTFN-DCJjZWM"
   },
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfsNpMLrEXLk"
   },
   "source": [
    "next edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DaVTonr_XB8",
    "outputId": "9e5fa8f9-604e-4273-a34d-fad73ad9ab7e"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "a = np.random.randint(2, size=(4,8))\n",
    "a_t = a.transpose()\n",
    "print(a_t)\n",
    "inds = np.stack(np.where(a_t == 1)).transpose()\n",
    "ts_acts = np.any(a_t, axis=1)\n",
    "ts_inds = np.where(ts_acts)[0]\n",
    "\n",
    "labels = np.arange(32).reshape(4, 8).transpose()\n",
    "print(labels)\n",
    "\n",
    "next_edges = []\n",
    "for i in range(len(ts_inds)-1):\n",
    "    ind_s = ts_inds[i]\n",
    "    ind_e = ts_inds[i+1]\n",
    "    s = inds[inds[:,0] == ind_s]\n",
    "    e = inds[inds[:,0] == ind_e]\n",
    "    e_inds = [t for t in list(itertools.product(s, e)) if t[0][1] != t[1][1]]\n",
    "    edges = [(labels[tuple(e[0])],labels[tuple(e[1])], ind_e-ind_s) for e in e_inds]\n",
    "    next_edges.extend(edges)\n",
    "\n",
    "print(next_edges)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ5JQm1aEbmb"
   },
   "source": [
    "onset edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DISmsJB3EatR",
    "outputId": "fc864608-63a6-4ad0-84d9-1478001ce60e"
   },
   "outputs": [],
   "source": [
    "onset_edges = []\n",
    "print(a_t)\n",
    "print(labels)\n",
    "\n",
    "for i in ts_inds:\n",
    "    ts_acts_inds = list(inds[inds[:,0] == i])\n",
    "    if len(ts_acts_inds) < 2:\n",
    "        continue\n",
    "    e_inds = list(itertools.combinations(ts_acts_inds, 2))\n",
    "    edges = [(labels[tuple(e[0])], labels[tuple(e[1])], 0) for e in e_inds]\n",
    "    inv_edges = [(e[1], e[0], *e[2:]) for e in edges]\n",
    "    onset_edges.extend(edges)\n",
    "    onset_edges.extend(inv_edges)\n",
    "\n",
    "print(onset_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujitZCKaa7nu"
   },
   "source": [
    "track edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbVG1vdFa-7e",
    "outputId": "c042449b-eef2-4707-a524-5f66f3ec07c7"
   },
   "outputs": [],
   "source": [
    "print(a_t)\n",
    "print(labels)\n",
    "track_edges = []\n",
    "\n",
    "for track in range(a_t.shape[1]):\n",
    "    tr_inds = list(inds[inds[:,1] == track])\n",
    "    e_inds = [(tr_inds[i],\n",
    "               tr_inds[i+1]) for i in range(len(tr_inds)-1)]\n",
    "    print(e_inds)\n",
    "    edges = [(labels[tuple(e[0])], labels[tuple(e[1])], e[1][0]-e[0][0]) for e in e_inds]\n",
    "    track_edges.extend(edges)\n",
    "\n",
    "print(track_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DzouJ5NqALB",
    "outputId": "20a76e82-6305-4154-d894-6d69a64435a1"
   },
   "outputs": [],
   "source": [
    "track_edges = np.array(track_edges)\n",
    "onset_edges = np.array(onset_edges)\n",
    "np.concatenate((track_edges, onset_edges)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihIYkPWPzyGX"
   },
   "outputs": [],
   "source": [
    "pip install pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie0pU8NWAUNM"
   },
   "outputs": [],
   "source": [
    "import pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTbGBSrdAZGH"
   },
   "outputs": [],
   "source": [
    "multitrack = pypianoroll.read(\"tests_fur-elise.mid\")\n",
    "print(multitrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eVo_BKzAmz4"
   },
   "outputs": [],
   "source": [
    "multitrack.tracks[0].pianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPpWw-rLA7CI"
   },
   "outputs": [],
   "source": [
    "multitrack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-PYbS7FA-Gg"
   },
   "outputs": [],
   "source": [
    "multitrack.trim(0, 12 * multitrack.resolution)\n",
    "multitrack.binarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psxuoTsZBFXY"
   },
   "outputs": [],
   "source": [
    "multitrack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovyixmSvBG3w"
   },
   "outputs": [],
   "source": [
    "multitrack.tracks[0].pianoroll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHlKNufuBzLn"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOhjCOJb34P4bTid7qFDg58",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1NeVldMsPVJd6pXbxZDmuiUP-QJBRhYtj",
   "name": "midi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
